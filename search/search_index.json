{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"LG-ADK: LangGraph Agent Development Kit","text":"<p>LG-ADK is a development kit designed to streamline the creation of LangGraph-based agents. It provides a Python-based framework for building complex agent systems with features similar to Google's Agent Development Kit, but powered by LangGraph.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Modular Agent Architecture: Easily define and customize agents with different capabilities</li> <li>Flexible Graph Construction: Build complex agent workflows using LangGraph's powerful graph-based approach</li> <li>Memory Management: Built-in support for short-term and long-term memory</li> <li>Session Management: Handle conversations and maintain context across interactions</li> <li>Human-in-the-Loop Capabilities: Seamlessly integrate human feedback and intervention</li> <li>Tool Integration: Easily connect agents to external tools and APIs</li> <li>Local Model Support: Run with Ollama or Gemini for enhanced privacy and reduced costs</li> <li>Streaming Responses: Real-time streaming of agent responses</li> <li>Visual Debugging: Inspect and debug agent workflows with langgraph-cli</li> <li>Database Flexibility: Use various databases (local or PostgreSQL) for storage</li> <li>Vector Store Integration: Works with different vector stores for semantic search</li> </ul>"},{"location":"#quick-install","title":"Quick Install","text":"<pre><code>pip install lg-adk\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from lg_adk import Agent, GraphBuilder\nfrom lg_adk.memory import MemoryManager\nfrom lg_adk.tools import WebSearchTool\n\n# Create an agent\nagent = Agent(\n    name=\"assistant\",\n    llm=\"ollama/llama3\",  # Or use \"gemini/gemini-pro\"\n    description=\"A helpful AI assistant\"\n)\n\n# Add a tool\nagent.add_tool(WebSearchTool())\n\n# Create a graph\nbuilder = GraphBuilder()\nbuilder.add_agent(agent)\n\n# Build and run\ngraph = builder.build()\nresponse = graph.invoke({\"input\": \"Hello, how can you help me today?\"})\nprint(response[\"output\"])\n</code></pre>"},{"location":"#license","title":"License","text":"<p>MIT License</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This project is inspired by Google's Agent Development Kit and built on top of LangGraph by LangChain.</p>"},{"location":"examples/model_provider_example/","title":"Using Multiple Model Providers","text":"<p>This example demonstrates how to use LG-ADK's model provider system to create an application that can switch between different language model providers.</p>"},{"location":"examples/model_provider_example/#multi-provider-agent","title":"Multi-Provider Agent","text":"<p>In this example, we'll create a simple agent that can use different model providers (OpenAI, Google, Anthropic, or Ollama) based on user preferences:</p> <pre><code>import os\nfrom typing import Dict, Optional\nfrom lg_adk import Agent, GraphBuilder\nfrom lg_adk.models import get_model, ModelRegistry\nfrom lg_adk.human import HumanInputTool\nfrom lg_adk.tools import WebSearchTool\nfrom lg_adk.memory import MemoryManager\n\n# Set up API keys (in a real application, these would be in environment variables)\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"  # Replace with your actual key\nos.environ[\"GOOGLE_API_KEY\"] = \"your-google-key\"  # Replace with your actual key\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"  # Replace with your actual key\n\nclass ModelSwitchingAgent:\n    \"\"\"An agent that can switch between different model providers.\"\"\"\n\n    def __init__(self):\n        # Available models configuration\n        self.models = {\n            \"openai\": \"openai/gpt-4\",\n            \"google\": \"google/gemini-pro\",\n            \"anthropic\": \"anthropic/claude-3-sonnet\",\n            \"ollama\": \"ollama/llama3\"  # Local model\n        }\n\n        # Start with OpenAI as default\n        self.current_model = \"openai\"\n\n        # Initialize the agent with the default model\n        self.setup_agent()\n\n    def setup_agent(self):\n        \"\"\"Set up the agent with the current model.\"\"\"\n        model = get_model(self.models[self.current_model])\n\n        # Create a new agent with the selected model\n        self.agent = Agent(\n            agent_name=\"model_switcher\",\n            llm=model,\n            system_prompt=(\n                \"You are a helpful assistant powered by a language model. \"\n                \"You can search the web to answer questions and can switch between \"\n                \"different model providers based on user requests.\"\n            )\n        )\n\n        # Add tools\n        self.agent.add_tool(WebSearchTool())\n        self.agent.add_tool(HumanInputTool())\n\n        # Create a graph with the agent\n        self.builder = GraphBuilder()\n        self.builder.add_agent(self.agent)\n        self.builder.add_memory(MemoryManager())\n        self.builder.enable_human_feedback()\n\n        # Build the graph\n        self.graph = self.builder.build()\n\n    def switch_model(self, provider: str) -&gt; str:\n        \"\"\"Switch to a different model provider.\"\"\"\n        if provider not in self.models:\n            return f\"Unknown provider: {provider}. Available providers: {', '.join(self.models.keys())}\"\n\n        # Switch the model\n        self.current_model = provider\n        self.setup_agent()\n\n        return f\"Switched to {provider} model: {self.models[provider]}\"\n\n    def process_message(self, message: str, session_id: Optional[str] = None) -&gt; Dict:\n        \"\"\"Process a user message, handling model switching commands.\"\"\"\n        # Check for model switching command\n        if message.lower().startswith(\"switch to \"):\n            provider = message.lower().replace(\"switch to \", \"\").strip()\n            result = self.switch_model(provider)\n            return {\"output\": result}\n\n        # Regular message processing with the current model\n        return self.graph.invoke({\"input\": message}, {\"session_id\": session_id})\n\n# Usage example\nif __name__ == \"__main__\":\n    agent = ModelSwitchingAgent()\n\n    # Create a unique session ID for this conversation\n    import uuid\n    session_id = str(uuid.uuid4())\n\n    # Interactive chat loop\n    print(\"Multi-Provider Agent Chat (type 'exit' to quit)\")\n    print(\"Current model: \" + agent.current_model)\n    print(\"Available commands: 'switch to openai', 'switch to google', 'switch to anthropic', 'switch to ollama'\")\n    print(\"---------------------------------------------------\")\n\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == \"exit\":\n            break\n\n        response = agent.process_message(user_input, session_id)\n        print(f\"Agent ({agent.current_model}): {response['output']}\")\n</code></pre>"},{"location":"examples/model_provider_example/#comparing-models-example","title":"Comparing Models Example","text":"<p>This example demonstrates how to compare responses from different model providers for the same prompt:</p> <pre><code>import asyncio\nimport os\nfrom typing import Dict, List\nfrom lg_adk.models import get_model, ModelRegistry\n\n# Set up API keys (in a real application, these would be in environment variables)\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"  # Replace with your actual key\nos.environ[\"GOOGLE_API_KEY\"] = \"your-google-key\"  # Replace with your actual key\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"  # Replace with your actual key\n\nasync def compare_models(prompt: str, system_prompt: str = None) -&gt; Dict[str, str]:\n    \"\"\"Compare responses from different model providers for the same prompt.\"\"\"\n    # Configure models to use\n    models = {\n        \"OpenAI GPT-4\": get_model(\"openai/gpt-4\"),\n        \"Google Gemini Pro\": get_model(\"google/gemini-pro\"),\n        \"Anthropic Claude\": get_model(\"anthropic/claude-3-sonnet\"),\n        \"Ollama Llama3\": get_model(\"ollama/llama3\", temperature=0.7)\n    }\n\n    # Create tasks for each model\n    tasks = {}\n    for name, model in models.items():\n        tasks[name] = asyncio.create_task(\n            model.agenerate(prompt, system_prompt=system_prompt)\n        )\n\n    # Wait for all tasks to complete\n    results = {}\n    for name, task in tasks.items():\n        try:\n            results[name] = await task\n        except Exception as e:\n            results[name] = f\"Error: {str(e)}\"\n\n    return results\n\n# Example usage\nasync def main():\n    prompt = \"Explain the concept of quantum entanglement in simple terms.\"\n    system_prompt = \"You are a physics teacher explaining concepts to high school students.\"\n\n    print(f\"Prompt: {prompt}\")\n    print(\"Comparing responses from different models...\")\n    print(\"-\" * 50)\n\n    results = await compare_models(prompt, system_prompt)\n\n    for model_name, response in results.items():\n        print(f\"\\n--- {model_name} ---\")\n        print(response[:300] + \"...\" if len(str(response)) &gt; 300 else response)\n        print(\"-\" * 50)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/model_provider_example/#fallback-chain-example","title":"Fallback Chain Example","text":"<p>This example shows how to use model fallback chains for reliability:</p> <pre><code>from lg_adk.models import ModelRegistry, get_model\nimport os\n\n# Set up API keys (in a real application, these would be in environment variables)\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"  # Replace with your actual key\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"  # Replace with your actual key\n\ndef create_reliable_agent():\n    \"\"\"Create an agent with a fallback chain of models.\"\"\"\n    # Get the model registry\n    registry = ModelRegistry.get_instance()\n\n    # Register a fallback chain named \"reliable-chain\"\n    registry.register_fallback_chain(\n        \"reliable-chain\",\n        [\n            \"openai/gpt-4\",  # Try OpenAI first\n            \"anthropic/claude-3-sonnet\",  # If OpenAI fails, try Anthropic\n            \"ollama/llama3\"  # Local fallback as last resort\n        ]\n    )\n\n    # Use the fallback chain\n    fallback_model = registry.get_model(\"reliable-chain\")\n\n    # Generate a response (will fall back if primary model fails)\n    response = fallback_model.generate(\n        \"Explain why reliability is important in AI systems.\",\n        system_prompt=\"You are a helpful AI expert.\"\n    )\n\n    return response\n\n# Example usage\nif __name__ == \"__main__\":\n    result = create_reliable_agent()\n    print(\"Response from fallback chain:\")\n    print(result)\n</code></pre>"},{"location":"examples/model_provider_example/#running-the-examples","title":"Running the Examples","text":"<p>To run these examples, you'll need to:</p> <ol> <li> <p>Install LG-ADK with all optional dependencies:    <pre><code>pip install \"lg-adk[all]\"\n</code></pre></p> </li> <li> <p>Set up API keys for the model providers you want to use:    <pre><code>export OPENAI_API_KEY=\"your-openai-key\"\nexport GOOGLE_API_KEY=\"your-google-key\"\nexport ANTHROPIC_API_KEY=\"your-anthropic-key\"\n</code></pre></p> </li> <li> <p>For local models, install Ollama from https://ollama.ai/ and run:    <pre><code>ollama pull llama3\n</code></pre></p> </li> <li> <p>Run any of the example scripts:    <pre><code>python model_switching_agent.py\n</code></pre></p> </li> </ol> <p>These examples demonstrate how LG-ADK's model provider system makes it easy to work with different LLM providers, switch between them, or create fallback chains for reliability. </p>"},{"location":"examples/multi_agent_workflow/","title":"Multi-Agent Workflow Examples","text":"<p>This section provides examples of building multi-agent systems using LG-ADK.</p>"},{"location":"examples/multi_agent_workflow/#group-chat-example","title":"Group Chat Example","text":"<p>The following example demonstrates how to create a group chat where multiple agents can collaborate:</p> <pre><code>import os\nfrom typing import Dict, Any, List\n\nfrom lg_adk import Agent, get_model\nfrom lg_adk.tools.group_chat import GroupChatTool\n\n# Create specialized agents\nfinance_agent = Agent(\n    agent_name=\"FinanceExpert\",\n    system_prompt=\"You are a financial expert. Provide accurate financial advice.\",\n    llm=get_model(\"gpt-4\")\n)\n\nlegal_agent = Agent(\n    agent_name=\"LegalExpert\",\n    system_prompt=\"You are a legal expert. Provide accurate legal advice.\",\n    llm=get_model(\"gpt-4\")\n)\n\n# Create a registry of agents\nagents = {\n    \"finance\": finance_agent,\n    \"legal\": legal_agent\n}\n\n# Create the group chat tool\nchat_tool = GroupChatTool(agent_registry=agents)\n\n# Create a new chat\nchat_id = chat_tool.create_chat(\n    name=\"Financial Legal Consultation\",\n    agent_ids=[\"finance\", \"legal\"]\n)\n\n# Run a conversation\nmessages = chat_tool.run_conversation(\n    chat_id=chat_id,\n    initial_prompt=\"What are the tax implications of starting a small business?\",\n    max_turns=4  # Number of turns in the conversation\n)\n\n# Print the conversation\nfor msg in messages:\n    print(f\"{msg.agent_id}: {msg.content}\")\n</code></pre>"},{"location":"examples/multi_agent_workflow/#agent-router-example","title":"Agent Router Example","text":"<p>This example shows how to use the AgentRouter to route tasks to different agents based on their specialties:</p> <pre><code>from lg_adk import Agent, get_model\nfrom lg_adk.tools.agent_router import AgentRouter, RouterType\n\n# Create specialized agents\nresearch_agent = Agent(\n    agent_name=\"Researcher\",\n    system_prompt=\"You are a research specialist. Find and present factual information.\",\n    llm=get_model(\"gpt-4\")\n)\n\nwriter_agent = Agent(\n    agent_name=\"Writer\",\n    system_prompt=\"You are a writing specialist. Create well-structured content.\",\n    llm=get_model(\"gpt-4\")\n)\n\neditor_agent = Agent(\n    agent_name=\"Editor\",\n    system_prompt=\"You are an editor. Improve content for clarity and correctness.\",\n    llm=get_model(\"gpt-4\")\n)\n\n# Create a sequential router (research -&gt; write -&gt; edit)\nsequential_router = AgentRouter(\n    name=\"ContentCreationPipeline\",\n    agents=[research_agent, writer_agent, editor_agent],\n    router_type=RouterType.SEQUENTIAL\n)\n\n# Run a task through the sequential pipeline\nresult = sequential_router.run(\"Explain how blockchain technology works\")\nprint(result[\"output\"])\n\n# Create a selector router\nselector_router = AgentRouter(\n    name=\"ExpertSelector\",\n    agents=[research_agent, writer_agent, editor_agent],\n    router_type=RouterType.SELECTOR\n)\n\n# The router will select the most appropriate agent based on the task\nresult = selector_router.run(\"Research the latest advances in quantum computing\")\nprint(f\"Selected agent: {result.get('agent', 'Unknown')}\")\nprint(f\"Output: {result.get('output', '')}\")\n\n# Create a mixture router\nmixture_router = AgentRouter(\n    name=\"CollaborativeThinking\",\n    agents=[research_agent, writer_agent, editor_agent],\n    router_type=RouterType.MIXTURE\n)\n\n# Get results from all agents\nresult = mixture_router.run(\"What are the best practices for writing technical documentation?\")\nprint(result[\"output\"])\n</code></pre>"},{"location":"examples/multi_agent_workflow/#complete-multi-agent-workflow","title":"Complete Multi-Agent Workflow","text":"<p>This example demonstrates a complete multi-agent workflow that combines different types of collaboration:</p> <pre><code>import os\nfrom typing import Dict, Any, List\n\nfrom lg_adk import Agent, get_model, GraphBuilder\nfrom lg_adk.tools.group_chat import GroupChatTool\nfrom lg_adk.tools.agent_router import AgentRouter, RouterType\n\n# Create a team of specialized agents\nresearch_agent = Agent(\n    agent_name=\"Researcher\",\n    system_prompt=\"You are a research specialist. Find relevant information on any topic.\",\n    llm=get_model(\"gpt-4\")\n)\n\nwriter_agent = Agent(\n    agent_name=\"Writer\",\n    system_prompt=\"You are a writing specialist. Create engaging, well-structured content.\",\n    llm=get_model(\"gpt-4\")\n)\n\neditor_agent = Agent(\n    agent_name=\"Editor\",\n    system_prompt=\"You are an editor. Review and improve content for clarity and correctness.\",\n    llm=get_model(\"gpt-4\")\n)\n\nfact_checker_agent = Agent(\n    agent_name=\"FactChecker\",\n    system_prompt=\"You are a fact checker. Verify the accuracy of information.\",\n    llm=get_model(\"gpt-4\")\n)\n\n# Register all agents\nagents = {\n    \"researcher\": research_agent,\n    \"writer\": writer_agent,\n    \"editor\": editor_agent,\n    \"fact_checker\": fact_checker_agent\n}\n\n# Phase 1: Planning using group chat\ndef planning_phase(topic):\n    \"\"\"Use group chat for collaborative planning.\"\"\"\n    print(\"\\n=== Phase 1: Planning ===\\n\")\n\n    chat_tool = GroupChatTool(agent_registry=agents)\n    chat_id = chat_tool.create_chat(\n        name=\"ContentPlanning\",\n        agent_ids=[\"researcher\", \"writer\", \"editor\"]\n    )\n\n    messages = chat_tool.run_conversation(\n        chat_id=chat_id,\n        initial_prompt=f\"We need to create comprehensive content about {topic}. Let's plan our approach.\",\n        max_turns=6\n    )\n\n    # Extract plan from the last message\n    plan = messages[-1].content\n    print(\"Planning completed:\")\n    for msg in messages:\n        print(f\"{msg.agent_id}: {msg.content}\\n\")\n\n    return plan\n\n# Phase 2: Research and content creation\ndef research_and_create_phase(topic, plan):\n    \"\"\"Use sequential router for research and content creation.\"\"\"\n    print(\"\\n=== Phase 2: Research and Content Creation ===\\n\")\n\n    # Create a sequential router for research and writing\n    creation_router = AgentRouter(\n        name=\"ContentCreation\",\n        agents=[agents[\"researcher\"], agents[\"writer\"]],\n        router_type=RouterType.SEQUENTIAL\n    )\n\n    result = creation_router.run(\n        f\"Based on this plan: {plan}\\nResearch and create content about {topic}\"\n    )\n\n    draft_content = result.get(\"output\", \"\")\n    print(f\"Draft content created:\\n{draft_content}\\n\")\n\n    return draft_content\n\n# Phase 3: Review and improvement\ndef review_phase(draft_content):\n    \"\"\"Use mixture router for review and improvement.\"\"\"\n    print(\"\\n=== Phase 3: Review and Improvement ===\\n\")\n\n    # Create a mixture router for review\n    review_router = AgentRouter(\n        name=\"ContentReview\",\n        agents=[agents[\"editor\"], agents[\"fact_checker\"]],\n        router_type=RouterType.MIXTURE\n    )\n\n    result = review_router.run(\n        f\"Review and improve this content:\\n{draft_content}\"\n    )\n\n    final_content = result.get(\"output\", \"\")\n    print(f\"Final content:\\n{final_content}\\n\")\n\n    return final_content\n\n# Main workflow function\ndef multi_agent_content_workflow(topic):\n    \"\"\"Run the complete multi-agent content creation workflow.\"\"\"\n    # Phase 1: Planning\n    plan = planning_phase(topic)\n\n    # Phase 2: Research and content creation\n    draft_content = research_and_create_phase(topic, plan)\n\n    # Phase 3: Review and improvement\n    final_content = review_phase(draft_content)\n\n    return final_content\n\n# Run the workflow\nif __name__ == \"__main__\":\n    if not os.getenv(\"OPENAI_API_KEY\"):\n        print(\"Please set the OPENAI_API_KEY environment variable\")\n    else:\n        result = multi_agent_content_workflow(\"artificial intelligence ethics\")\n        print(\"\\n=== Workflow Completed ===\\n\")\n        print(result)\n</code></pre>"},{"location":"examples/multi_agent_workflow/#graph-based-multi-agent-workflow","title":"Graph-Based Multi-Agent Workflow","text":"<p>This example demonstrates how to use LG-ADK's GraphBuilder to create a more complex multi-agent workflow:</p> <pre><code>from lg_adk import Agent, GraphBuilder\nfrom lg_adk.tools.agent_router import AgentRouter, RouterType\n\n# Create specialized agents (code omitted for brevity)\n\n# Define node functions\ndef planning_node(state):\n    \"\"\"Plan the content creation approach.\"\"\"\n    topic = state.get(\"input\", \"\")\n\n    planning_agent = Agent(\n        agent_name=\"Planner\",\n        system_prompt=\"You create detailed content plans.\",\n        llm=get_model(\"gpt-4\")\n    )\n\n    result = planning_agent.run({\n        \"input\": f\"Create a detailed plan for content about: {topic}\"\n    })\n\n    plan = result.get(\"output\", \"\")\n    return {\"topic\": topic, \"plan\": plan}\n\ndef research_node(state):\n    \"\"\"Research the topic.\"\"\"\n    topic = state.get(\"topic\", \"\")\n    plan = state.get(\"plan\", \"\")\n\n    research_agent = Agent(\n        agent_name=\"Researcher\",\n        system_prompt=\"You find factual information on any topic.\",\n        llm=get_model(\"gpt-4\")\n    )\n\n    result = research_agent.run({\n        \"input\": f\"Research this topic based on the plan:\\nTopic: {topic}\\nPlan: {plan}\"\n    })\n\n    research = result.get(\"output\", \"\")\n    return {\"topic\": topic, \"plan\": plan, \"research\": research}\n\ndef writing_node(state):\n    \"\"\"Write content based on research.\"\"\"\n    topic = state.get(\"topic\", \"\")\n    plan = state.get(\"plan\", \"\")\n    research = state.get(\"research\", \"\")\n\n    writer_agent = Agent(\n        agent_name=\"Writer\",\n        system_prompt=\"You create well-structured content.\",\n        llm=get_model(\"gpt-4\")\n    )\n\n    result = writer_agent.run({\n        \"input\": f\"Write content based on:\\nTopic: {topic}\\nPlan: {plan}\\nResearch: {research}\"\n    })\n\n    draft = result.get(\"output\", \"\")\n    return {\"topic\": topic, \"plan\": plan, \"research\": research, \"draft\": draft}\n\ndef editing_node(state):\n    \"\"\"Edit and improve the draft.\"\"\"\n    draft = state.get(\"draft\", \"\")\n\n    editor_agent = Agent(\n        agent_name=\"Editor\",\n        system_prompt=\"You improve content for clarity and correctness.\",\n        llm=get_model(\"gpt-4\")\n    )\n\n    result = editor_agent.run({\n        \"input\": f\"Edit and improve this draft:\\n{draft}\"\n    })\n\n    final_content = result.get(\"output\", \"\")\n    return {\"output\": final_content}\n\n# Build the graph\nbuilder = GraphBuilder()\nbuilder.add_node(\"planning\", planning_node)\nbuilder.add_node(\"research\", research_node)\nbuilder.add_node(\"writing\", writing_node)\nbuilder.add_node(\"editing\", editing_node)\n\n# Define the flow\nflow = [\n    (None, \"planning\"),\n    (\"planning\", \"research\"),\n    (\"research\", \"writing\"),\n    (\"writing\", \"editing\"),\n    (\"editing\", None)\n]\n\n# Build and use the graph\ncontent_graph = builder.build(flow=flow)\nresult = content_graph.invoke({\"input\": \"renewable energy\"})\nprint(result[\"output\"])\n</code></pre>"},{"location":"examples/multi_agent_workflow/#full-examples","title":"Full Examples","text":"<p>For more detailed examples, see the full code in the <code>docs/examples</code> directory:</p> <ul> <li>multi_agent_chat.py: A complete example of group chat and router implementations </li> </ul>"},{"location":"examples/rag/","title":"RAG Examples","text":"<p>This section provides examples of building Retrieval-Augmented Generation (RAG) applications using LG-ADK.</p>"},{"location":"examples/rag/#simple-rag-example","title":"Simple RAG Example","text":"<p>The following example demonstrates how to create a simple RAG application using FAISS as the vector store:</p> <pre><code>import os\nfrom typing import Dict, Any, List\nfrom dotenv import load_dotenv\n\n# Import LG-ADK components\nfrom lg_adk import Agent, get_model\nfrom lg_adk.tools.retrieval import SimpleVectorRetrievalTool\n\n# Load environment variables\nload_dotenv()\n\n# Set up vector store\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.embeddings import OpenAIEmbeddings\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Load and process documents\nloader = TextLoader(\"path/to/document.txt\")\ndocuments = loader.load()\n\n# Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=100\n)\nchunks = text_splitter.split_documents(documents)\n\n# Create embeddings and vector store\nembeddings = OpenAIEmbeddings()\nvector_store = FAISS.from_documents(chunks, embeddings)\n\n# Create a retrieval tool\nretrieval_tool = SimpleVectorRetrievalTool(\n    name=\"retrieve_documentation\",\n    description=\"Use this tool to retrieve documentation and reference materials from the knowledge base.\",\n    vector_store=vector_store,\n    top_k=3,\n    score_threshold=0.7\n)\n\n# Create the RAG agent\nrag_agent = Agent(\n    agent_name=\"DocumentationAssistant\",\n    system_prompt=\"\"\"\n    You are a helpful assistant with access to documentation.\n    When asked questions, use the retrieval tool to find relevant information.\n    Always reference where your information came from.\n    \"\"\",\n    llm=get_model(\"gpt-4\"),\n    tools=[retrieval_tool]\n)\n\n# Use the agent\nresponse = rag_agent.run({\"input\": \"What information do we have about X?\"})\nprint(response[\"output\"])\n</code></pre>"},{"location":"examples/rag/#chromadb-example","title":"ChromaDB Example","text":"<p>This example shows how to use ChromaDB as the vector store:</p> <pre><code>import chromadb\nfrom lg_adk import Agent, get_model\nfrom lg_adk.tools.retrieval import ChromaDBRetrievalTool\nfrom langchain_community.embeddings import OpenAIEmbeddings\n\n# Set up ChromaDB\nchroma_client = chromadb.PersistentClient(path=\"path/to/chromadb\")\n\n# Create embedding function wrapper for ChromaDB\nclass OpenAIEmbeddingFunction:\n    def __call__(self, texts):\n        return embeddings.embed_documents(texts)\n\nembeddings = OpenAIEmbeddings()\nembedding_function = OpenAIEmbeddingFunction()\n\n# Get or create collection\ncollection = chroma_client.get_or_create_collection(\n    name=\"your_collection\",\n    embedding_function=embedding_function\n)\n\n# Add documents if needed\n# collection.add(\n#     documents=[\"doc1\", \"doc2\", \"doc3\"],\n#     metadatas=[{\"source\": \"source1\"}, {\"source\": \"source2\"}, {\"source\": \"source3\"}],\n#     ids=[\"id1\", \"id2\", \"id3\"]\n# )\n\n# Create a ChromaDB retrieval tool\nretrieval_tool = ChromaDBRetrievalTool(\n    name=\"chromadb_retrieval\",\n    description=\"Use this tool to retrieve information from the ChromaDB knowledge base.\",\n    collection_name=\"your_collection\",\n    chroma_client=chroma_client,\n    embedding_function=embedding_function,\n    top_k=3,\n    score_threshold=0.3\n)\n\n# Create a RAG agent\nchromadb_agent = Agent(\n    agent_name=\"ChromaDBAssistant\",\n    system_prompt=\"You are an assistant with access to a ChromaDB knowledge base. Use the retrieval tool to find information.\",\n    llm=get_model(\"gpt-4\"),\n    tools=[retrieval_tool]\n)\n\n# Use the agent\nresponse = chromadb_agent.run({\"input\": \"What information do we have about X?\"})\nprint(response[\"output\"])\n</code></pre>"},{"location":"examples/rag/#google-adk-style-rag","title":"Google ADK-Style RAG","text":"<p>This example shows how to create a RAG application in a style similar to Google's Agent Development Kit:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\n# Import LG-ADK components\nfrom lg_adk import Agent\nfrom lg_adk.tools.retrieval import SimpleVectorRetrievalTool\n\n# Load environment variables\nload_dotenv()\n\n# System prompt\ndef return_instructions_root():\n    return \"\"\"\n    You are a helpful assistant with access to a knowledge base.\n    Use the retrieval tool to find information when answering questions.\n\n    When responding:\n    1. First retrieve relevant information using the tool\n    2. Then synthesize a clear and helpful response\n    3. If the information isn't in the knowledge base, say so\n\n    Always cite where you found your information.\n    \"\"\"\n\ndef setup_rag_agent():\n    \"\"\"Set up a RAG agent similar to Google's ADK style.\"\"\"\n    # Set up vector store (code omitted for brevity)\n    from lg_adk.models import get_model\n\n    # Create the retrieval tool\n    lg_adk_retrieval = SimpleVectorRetrievalTool(\n        name='retrieve_documentation',\n        description=(\n            'Use this tool to retrieve documentation and reference materials'\n        ),\n        vector_store=your_vector_store,\n        top_k=5,\n        score_threshold=0.6,\n    )\n\n    # Create the agent\n    rag_agent = Agent(\n        agent_name='documentation_agent',\n        system_prompt=return_instructions_root(),\n        llm=get_model('gpt-4'),\n        tools=[\n            lg_adk_retrieval,\n        ]\n    )\n\n    return rag_agent\n\n# Create and use the agent\nagent = setup_rag_agent()\nresponse = agent.run({\"input\": \"What does the documentation say about X?\"})\nprint(response[\"output\"])\n</code></pre>"},{"location":"examples/rag/#advanced-rag-with-memory","title":"Advanced RAG with Memory","text":"<p>This example shows how to create a RAG application with memory to maintain context across interactions:</p> <pre><code>from lg_adk import Agent, GraphBuilder\nfrom lg_adk.memory import MemoryManager\nfrom lg_adk.sessions import SessionManager\nfrom lg_adk.tools.retrieval import SimpleVectorRetrievalTool\n\n# Set up components\nmemory_manager = MemoryManager()\nsession_manager = SessionManager()\nretrieval_tool = SimpleVectorRetrievalTool(...)\n\n# Define node functions\ndef get_or_create_session(state):\n    \"\"\"Get or create a session.\"\"\"\n    import uuid\n\n    session_id = state.get(\"session_id\")\n    if not session_id:\n        session_id = str(uuid.uuid4())\n        session_manager.create_session(session_id)\n\n    session_data = session_manager.get_session(session_id)\n    return {\"session_id\": session_id, \"session_data\": session_data}\n\ndef retrieve_history(state):\n    \"\"\"Retrieve conversation history.\"\"\"\n    session_id = state[\"session_id\"]\n    conversation_history = memory_manager.get_conversation_history(session_id)\n    return {\"conversation_history\": conversation_history}\n\ndef retrieve_context(state):\n    \"\"\"Retrieve relevant documents.\"\"\"\n    query = state[\"input\"]\n    context = retrieval_tool.run(query)\n    return {\"context\": context}\n\ndef generate_response(state):\n    \"\"\"Generate a response based on context and history.\"\"\"\n    rag_agent = Agent(\n        agent_name=\"RAGWithMemory\",\n        system_prompt=\"Answer based on the context and conversation history.\",\n        llm=get_model(\"gpt-4\")\n    )\n\n    result = rag_agent.run({\n        \"input\": state[\"input\"],\n        \"context\": state[\"context\"],\n        \"conversation_history\": state[\"conversation_history\"]\n    })\n\n    return {\"output\": result[\"output\"]}\n\ndef update_memory(state):\n    \"\"\"Add the interaction to memory.\"\"\"\n    session_id = state[\"session_id\"]\n    memory_manager.add_message(session_id, {\"role\": \"user\", \"content\": state[\"input\"]})\n    memory_manager.add_message(session_id, {\"role\": \"assistant\", \"content\": state[\"output\"]})\n    return state\n\n# Build the graph\nbuilder = GraphBuilder()\nbuilder.add_node(\"get_or_create_session\", get_or_create_session)\nbuilder.add_node(\"retrieve_history\", retrieve_history)\nbuilder.add_node(\"retrieve_context\", retrieve_context)\nbuilder.add_node(\"generate_response\", generate_response)\nbuilder.add_node(\"update_memory\", update_memory)\n\n# Define the flow\nflow = [\n    (None, \"get_or_create_session\"),\n    (\"get_or_create_session\", \"retrieve_history\"),\n    (\"retrieve_history\", \"retrieve_context\"),\n    (\"retrieve_context\", \"generate_response\"),\n    (\"generate_response\", \"update_memory\"),\n    (\"update_memory\", None)\n]\n\n# Build and use the graph\nrag_graph = builder.build(flow=flow)\nresult = rag_graph.invoke({\"input\": \"Tell me about X\"})\nprint(result[\"output\"])\n\n# Continue the conversation\nresult = rag_graph.invoke({\"input\": \"Tell me more about it\", \"session_id\": result[\"session_id\"]})\nprint(result[\"output\"])\n</code></pre>"},{"location":"examples/rag/#full-examples","title":"Full Examples","text":"<p>For more detailed examples, see the full code in the <code>docs/examples</code> directory:</p> <ul> <li>simple_rag.py: A complete example of creating RAG agents with FAISS and ChromaDB</li> <li>google_style_rag.py: An example showing the Google ADK-style approach</li> <li>rag_with_memory.py: An example demonstrating RAG with conversation memory </li> </ul>"},{"location":"examples/tool_usage/","title":"Tool Usage with LG-ADK","text":"<p>This guide demonstrates how to incorporate tools into your agents using LG-ADK. Tools extend your agent's capabilities, allowing them to perform actions like retrieving information, calculating values, or interacting with external systems.</p>"},{"location":"examples/tool_usage/#basic-tool-integration","title":"Basic Tool Integration","text":""},{"location":"examples/tool_usage/#step-1-define-a-simple-tool","title":"Step 1: Define a Simple Tool","text":"<p>First, let's define a simple calculator tool:</p> <pre><code>from typing import Dict, Any\nfrom lg_adk.tools.base import BaseTool\n\nclass CalculatorTool(BaseTool):\n    name = \"calculator\"\n    description = \"Performs basic arithmetic operations\"\n\n    def _run(self, operation: str, a: float, b: float) -&gt; Dict[str, Any]:\n        \"\"\"\n        Perform a basic arithmetic operation.\n\n        Args:\n            operation: The operation to perform (add, subtract, multiply, divide)\n            a: First number\n            b: Second number\n\n        Returns:\n            Dictionary containing the result\n        \"\"\"\n        result = None\n        if operation == \"add\":\n            result = a + b\n        elif operation == \"subtract\":\n            result = a - b\n        elif operation == \"multiply\":\n            result = a * b\n        elif operation == \"divide\":\n            if b == 0:\n                return {\"error\": \"Cannot divide by zero\"}\n            result = a / b\n        else:\n            return {\"error\": f\"Unknown operation: {operation}\"}\n\n        return {\"result\": result}\n</code></pre>"},{"location":"examples/tool_usage/#step-2-register-the-tool-with-your-agent","title":"Step 2: Register the Tool with Your Agent","text":"<p>Now let's create an agent that can use this tool:</p> <pre><code>from lg_adk.agents import Agent\nfrom lg_adk.models import get_model\n\n# Initialize your agent\nagent = Agent(\n    name=\"math_assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a helpful math assistant. Use the calculator tool when needed.\",\n    tools=[CalculatorTool()]\n)\n\n# Now the agent can use the calculator tool during conversations\nresponse = agent.run(\"What is 1234 \u00d7 5678?\")\nprint(response)\n</code></pre>"},{"location":"examples/tool_usage/#example-web-search-tool","title":"Example: Web Search Tool","text":"<p>Here's a more complex example using a web search tool:</p> <pre><code>from lg_adk.tools.web import WebSearchTool\nfrom lg_adk.agents import Agent\nfrom lg_adk.models import get_model\n\n# Initialize an agent with the web search tool\nagent = Agent(\n    name=\"research_assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a research assistant. Use the web search tool to find up-to-date information.\",\n    tools=[WebSearchTool(api_key=\"your_search_api_key\")]\n)\n\n# Ask the agent a question that requires searching for information\nresponse = agent.run(\"What were the major tech news headlines yesterday?\")\nprint(response)\n</code></pre>"},{"location":"examples/tool_usage/#complete-example-multi-tool-agent","title":"Complete Example: Multi-Tool Agent","text":"<p>This example shows how to create an agent with multiple tools:</p> <pre><code>import os\nfrom lg_adk.agents import Agent\nfrom lg_adk.models import get_model\nfrom lg_adk.tools.web import WebSearchTool\nfrom lg_adk.tools.file import FileReadTool, FileWriteTool\nfrom lg_adk.tools.memory import MemoryTool\n\n# Setup tools\nweb_search = WebSearchTool(api_key=os.environ.get(\"SEARCH_API_KEY\"))\nfile_read = FileReadTool()\nfile_write = FileWriteTool()\nmemory = MemoryTool()\n\n# Create an agent with multiple tools\nagent = Agent(\n    name=\"assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a helpful assistant with multiple capabilities:\n    - You can search the web for information\n    - You can read and write files\n    - You can store and retrieve information from your memory\n    Use these tools whenever appropriate to help the user.\"\"\",\n    tools=[web_search, file_read, file_write, memory]\n)\n\n# Example interaction\nconversation = agent.run(\"\"\"\nPlease help me with these tasks:\n1. Find the current price of Bitcoin\n2. Save that information to a file called 'crypto_prices.txt'\n3. Remember that I'm interested in cryptocurrency prices\n\"\"\")\n\nprint(conversation)\n</code></pre>"},{"location":"examples/tool_usage/#tool-output-processing","title":"Tool Output Processing","text":"<p>LG-ADK automatically handles parsing tool outputs and sending them back to the model. You can also customize how tool outputs are processed:</p> <pre><code>from lg_adk.agents import Agent\nfrom lg_adk.models import get_model\nfrom lg_adk.tools.base import BaseTool\n\nclass CustomTool(BaseTool):\n    name = \"custom_tool\"\n    description = \"A custom tool with output processing\"\n\n    def _run(self, query: str) -&gt; dict:\n        # Tool implementation\n        return {\"data\": f\"Processed: {query}\"}\n\n    def process_output(self, output: dict) -&gt; str:\n        \"\"\"Custom processing of tool output before sending to model\"\"\"\n        return f\"TOOL RESULT: {output['data'].upper()}\"\n\n# Create agent with custom tool\nagent = Agent(\n    name=\"custom_agent\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"Use the custom tool when appropriate.\",\n    tools=[CustomTool()]\n)\n</code></pre>"},{"location":"examples/tool_usage/#tool-error-handling","title":"Tool Error Handling","text":"<p>Tools in LG-ADK include built-in error handling mechanisms:</p> <pre><code>from lg_adk.tools.base import BaseTool\n\nclass RiskyTool(BaseTool):\n    name = \"risky_tool\"\n    description = \"A tool that might fail\"\n\n    def _run(self, input_param: str) -&gt; dict:\n        try:\n            # Some operation that might fail\n            if input_param == \"fail\":\n                raise ValueError(\"Demonstration error\")\n            return {\"result\": f\"Successfully processed {input_param}\"}\n        except Exception as e:\n            # Error handling\n            return {\n                \"error\": str(e),\n                \"status\": \"failed\"\n            }\n</code></pre> <p>With proper tool design, your agents can handle errors gracefully and provide helpful feedback to users. </p>"},{"location":"examples/enhanced_session_management/","title":"Enhanced Session Management Example","text":"<p>This example demonstrates advanced session management capabilities in LG-ADK with features such as:</p> <ul> <li>Custom session manager with analytics</li> <li>Session creation and tracking</li> <li>Usage statistics and metadata management</li> <li>Session expiration and cleanup</li> </ul>"},{"location":"examples/enhanced_session_management/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>Dependencies listed in <code>requirements.txt</code></li> </ul>"},{"location":"examples/enhanced_session_management/#installation","title":"Installation","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"examples/enhanced_session_management/#running-the-example","title":"Running the Example","text":"<pre><code>python main.py\n</code></pre>"},{"location":"examples/enhanced_session_management/#key-features","title":"Key Features","text":"<ul> <li>Session Analytics: Track and analyze session metrics</li> <li>Extended Metadata: Store and update rich metadata for each session</li> <li>Usage Statistics: Monitor conversation counts and session duration</li> <li>Automated Cleanup: Manage expired sessions with configurable timeouts</li> </ul> <p>This example shows how to build enterprise-ready applications with robust session management using the LG-ADK framework. </p>"},{"location":"getting_started/installation/","title":"Installation","text":"<p>LG-ADK can be installed using pip or Poetry.</p>"},{"location":"getting_started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11 or higher</li> <li>(Optional) Ollama for local model support</li> <li>(Optional) Access to Google AI services for Gemini models</li> </ul>"},{"location":"getting_started/installation/#using-pip","title":"Using pip","text":"<pre><code>pip install lg-adk\n</code></pre>"},{"location":"getting_started/installation/#using-poetry","title":"Using Poetry","text":"<pre><code>poetry add lg-adk\n</code></pre>"},{"location":"getting_started/installation/#development-installation","title":"Development Installation","text":"<p>If you want to contribute to LG-ADK, you can install it in development mode:</p> <pre><code># Clone the repository\ngit clone https://github.com/yourusername/lg-adk.git\ncd lg-adk\n\n# Install dependencies\npoetry install\n\n# Alternatively with pip\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting_started/installation/#setting-up-environment-variables","title":"Setting Up Environment Variables","text":"<p>LG-ADK uses environment variables for configuration. You can create a <code>.env</code> file in your project directory:</p> <pre><code># .env file\nOPENAI_API_KEY=your_openai_api_key  # If using OpenAI models\nGOOGLE_API_KEY=your_google_api_key  # If using Gemini models\nOLLAMA_BASE_URL=http://localhost:11434  # For local Ollama models\nDEFAULT_LLM=ollama/llama3  # Default model to use\n</code></pre>"},{"location":"getting_started/installation/#verifying-installation","title":"Verifying Installation","text":"<p>You can verify your installation by running a simple example:</p> <pre><code>from lg_adk import Agent\n\n# This should print the version number\nprint(f\"LG-ADK version: {lg_adk.__version__}\")\n\n# Create a simple agent\nagent = Agent(\n    name=\"test_agent\",\n    llm=\"ollama/llama3\",\n    description=\"Test agent\"\n)\n</code></pre>"},{"location":"getting_started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting_started/installation/#common-issues","title":"Common Issues","text":"<ul> <li>Import Errors: Make sure you have installed all the required dependencies.</li> <li>Model Connection Errors: </li> <li>For Ollama: Ensure Ollama is running locally (<code>ollama serve</code>).</li> <li>For Gemini/OpenAI: Check your API keys are set correctly.</li> </ul>"},{"location":"getting_started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues, please:</p> <ol> <li>Check the Troubleshooting Guide</li> <li>Search existing GitHub Issues</li> <li>Open a new issue if needed </li> </ol>"},{"location":"getting_started/quick_start/","title":"Quick Start Guide","text":"<p>This guide will help you get started with LG-ADK and build your first agent in minutes.</p>"},{"location":"getting_started/quick_start/#installation","title":"Installation","text":"<p>First, install LG-ADK:</p> <pre><code>pip install lg-adk\n</code></pre> <p>Or with Poetry:</p> <pre><code>poetry add lg-adk\n</code></pre>"},{"location":"getting_started/quick_start/#creating-a-simple-agent","title":"Creating a Simple Agent","text":"<p>Let's create a simple assistant agent:</p> <pre><code>from lg_adk import Agent\n\n# Create a simple agent\nagent = Agent(\n    name=\"assistant\",\n    llm=\"ollama/llama3\",  # You can use \"gemini/gemini-pro\" or other models\n    description=\"A helpful assistant that answers questions\"\n)\n\n# Run the agent with a user query\nresult = agent.run({\"input\": \"What is artificial intelligence?\"})\nprint(result[\"output\"])\n</code></pre>"},{"location":"getting_started/quick_start/#building-an-agent-with-a-graph","title":"Building an Agent with a Graph","text":"<p>For more complex workflows, you can use a graph builder:</p> <pre><code>from lg_adk import Agent, GraphBuilder\n\n# Create an agent\nagent = Agent(\n    name=\"assistant\",\n    llm=\"ollama/llama3\",\n    description=\"A helpful assistant that answers questions\"\n)\n\n# Create a graph builder\nbuilder = GraphBuilder()\nbuilder.add_agent(agent)\n\n# Build the graph\ngraph = builder.build()\n\n# Run the graph\nresult = graph.invoke({\"input\": \"What is machine learning?\"})\nprint(result[\"output\"])\n</code></pre>"},{"location":"getting_started/quick_start/#creating-a-multi-agent-system","title":"Creating a Multi-Agent System","text":"<p>For complex tasks, you can create a multi-agent system:</p> <pre><code>from lg_adk import Agent, MultiAgentSystem\n\n# Create a coordinator agent\ncoordinator = Agent(\n    name=\"coordinator\",\n    llm=\"ollama/llama3\",\n    description=\"Coordinates tasks between specialized agents\"\n)\n\n# Create specialized agents\nresearcher = Agent(\n    name=\"researcher\",\n    llm=\"ollama/llama3\",\n    description=\"Researches information and provides detailed answers\"\n)\n\nwriter = Agent(\n    name=\"writer\",\n    llm=\"ollama/llama3\",\n    description=\"Writes concise and clear content\"\n)\n\n# Create a multi-agent system\nsystem = MultiAgentSystem(\n    name=\"research_team\",\n    coordinator=coordinator,\n    agents=[researcher, writer],\n    description=\"A team that researches and writes about topics\"\n)\n\n# Run the system\nresult = system.run({\"input\": \"Explain quantum computing\"})\nprint(result[\"output\"])\n</code></pre>"},{"location":"getting_started/quick_start/#using-different-model-providers","title":"Using Different Model Providers","text":"<p>LG-ADK supports multiple model providers:</p> <pre><code># Using Ollama (local models)\nlocal_agent = Agent(\n    name=\"local_assistant\",\n    llm=\"ollama/llama3\",\n    description=\"An assistant running on a local model\"\n)\n\n# Using Google's Gemini models\ngemini_agent = Agent(\n    name=\"gemini_assistant\",\n    llm=\"gemini/gemini-pro\",\n    description=\"An assistant powered by Gemini\"\n)\n\n# Using OpenAI models\nopenai_agent = Agent(\n    name=\"openai_assistant\",\n    llm=\"openai/gpt-4\",\n    description=\"An assistant powered by GPT-4\"\n)\n</code></pre>"},{"location":"getting_started/quick_start/#evaluating-your-agent","title":"Evaluating Your Agent","text":"<p>LG-ADK includes tools for evaluating agent performance:</p> <pre><code>from lg_adk import Agent, EvalDataset, Evaluator\n\n# Create an agent\nagent = Agent(\n    name=\"assistant\",\n    llm=\"ollama/llama3\",\n    description=\"A helpful assistant\"\n)\n\n# Create or load an evaluation dataset\ndataset = EvalDataset(\n    name=\"Simple Questions\",\n    description=\"Basic knowledge questions\",\n    examples=[\n        {\n            \"id\": \"q1\",\n            \"input\": \"What is the capital of France?\",\n            \"expected_output\": \"The capital of France is Paris.\"\n        },\n        {\n            \"id\": \"q2\",\n            \"input\": \"Who wrote Romeo and Juliet?\",\n            \"expected_output\": \"William Shakespeare wrote Romeo and Juliet.\"\n        }\n    ]\n)\n\n# Create an evaluator and run evaluation\nevaluator = Evaluator()\nresults = evaluator.evaluate(agent, dataset)\n\n# Print evaluation results\nprint(f\"Accuracy: {results.metric_scores.get('AccuracyMetric', 0)}\")\nprint(f\"Latency: {results.metric_scores.get('LatencyMetric', 0)} seconds\")\n</code></pre>"},{"location":"getting_started/quick_start/#running-an-interactive-session","title":"Running an Interactive Session","text":"<p>You can use the built-in CLI to run an interactive session:</p> <pre><code># Run an agent interactively\nlg-adk run path/to/your_agent.py\n\n# Evaluate an agent against a dataset\nlg-adk eval path/to/your_agent.py path/to/dataset.json\n\n# Debug an agent visually (requires langgraph-cli)\nlg-adk debug path/to/your_agent.py\n</code></pre>"},{"location":"getting_started/quick_start/#next-steps","title":"Next Steps","text":"<p>Now that you've seen the basics, check out these resources:</p> <ul> <li>Agent Guide - Learn more about creating and customizing agents</li> <li>Multi-Agent Systems - Explore building complex multi-agent systems</li> <li>Examples - Browse complete code examples</li> <li>API Reference - Detailed API documentation </li> </ul>"},{"location":"guides/building_graphs/","title":"Building Graphs with LG-ADK","text":"<p>This guide covers how to build agent graphs using the LangGraph Agent Development Kit (LG-ADK).</p>"},{"location":"guides/building_graphs/#understanding-graph-architecture","title":"Understanding Graph Architecture","text":"<p>In LG-ADK, a graph is a collection of connected agents and components that work together to accomplish complex tasks. Graphs allow you to:</p> <ol> <li>Connect multiple agents in a workflow</li> <li>Define how information flows between agents</li> <li>Create conditional logic between agent interactions</li> <li>Maintain state throughout the entire process</li> </ol>"},{"location":"guides/building_graphs/#getting-started-with-graphbuilder","title":"Getting Started with GraphBuilder","text":"<p>The <code>GraphBuilder</code> class is your main tool for constructing agent graphs:</p> <pre><code>from lg_adk.builders import GraphBuilder\nfrom lg_adk.agents import Agent\nfrom lg_adk.models import get_model\n\n# Create a new graph builder\nbuilder = GraphBuilder(name=\"simple_graph\")\n</code></pre>"},{"location":"guides/building_graphs/#adding-agents-to-a-graph","title":"Adding Agents to a Graph","text":"<p>You can add pre-configured agents to your graph:</p> <pre><code># Create some agents\nresearcher = Agent(\n    name=\"researcher\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a research agent who finds information on topics.\"\n)\n\nwriter = Agent(\n    name=\"writer\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a writer agent who creates content based on research.\"\n)\n\n# Add agents to the graph\nbuilder.add_agent(researcher)\nbuilder.add_agent(writer)\n</code></pre>"},{"location":"guides/building_graphs/#defining-node-connections","title":"Defining Node Connections","text":"<p>Connect nodes to establish the flow of information:</p> <pre><code># Connect the researcher to the writer\nbuilder.connect(\n    source=\"researcher\",\n    target=\"writer\"\n)\n\n# Connect the writer back to the user (end of the graph)\nbuilder.connect(\n    source=\"writer\",\n    target=\"__end__\"  # Special node that represents the end of the graph\n)\n</code></pre>"},{"location":"guides/building_graphs/#conditional-routing","title":"Conditional Routing","text":"<p>You can create more complex flows with conditional routing:</p> <pre><code># Define a routing function\ndef route_based_on_complexity(state):\n    \"\"\"Route to different agents based on query complexity\"\"\"\n    complexity = state.get(\"complexity\", \"simple\")\n    if complexity == \"complex\":\n        return \"deep_researcher\"\n    else:\n        return \"basic_researcher\"\n\n# Add a conditional branch\nbuilder.add_conditional_edge(\n    source=\"user_input\",\n    condition_function=route_based_on_complexity,\n    targets=[\"basic_researcher\", \"deep_researcher\"]\n)\n</code></pre>"},{"location":"guides/building_graphs/#adding-memory-to-a-graph","title":"Adding Memory to a Graph","text":"<p>Memory allows your graph to maintain context across interactions:</p> <pre><code>from lg_adk.memory import MemoryManager\nfrom lg_adk.database import DatabaseManager\n\n# Create memory manager\nmemory_manager = MemoryManager(\n    database_manager=DatabaseManager(connection_string=\"sqlite:///graph_memory.db\")\n)\n\n# Add memory to the graph\nbuilder.add_memory(memory_manager)\n</code></pre>"},{"location":"guides/building_graphs/#human-in-the-loop-integration","title":"Human-in-the-Loop Integration","text":"<p>For tasks that require human oversight:</p> <pre><code># Enable human-in-the-loop for the entire graph\nbuilder.enable_human_in_the_loop()\n\n# Or for specific transitions\nbuilder.enable_human_in_the_loop(\n    source=\"critical_decision\",\n    target=\"high_impact_action\"\n)\n</code></pre>"},{"location":"guides/building_graphs/#building-and-running-the-graph","title":"Building and Running the Graph","text":"<p>Once you've configured your graph, build and run it:</p> <pre><code># Build the graph\ngraph = builder.build()\n\n# Run the graph with an initial input\nresult = graph.run(\"Research the latest advancements in quantum computing and write a summary.\")\nprint(result)\n</code></pre>"},{"location":"guides/building_graphs/#streaming-graph-output","title":"Streaming Graph Output","text":"<p>For long-running processes, you might want to stream the results:</p> <pre><code># Enable streaming for the graph\ngraph = builder.build(stream=True)\n\n# Stream the results\nfor chunk in graph.stream(\"Tell me about artificial intelligence.\"):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"guides/building_graphs/#asynchronous-graph-execution","title":"Asynchronous Graph Execution","text":"<p>For non-blocking operation:</p> <pre><code>import asyncio\n\nasync def main():\n    # Build the graph with async support\n    graph = builder.build()\n\n    # Run the graph asynchronously\n    result = await graph.arun(\"Analyze the current trends in renewable energy.\")\n    print(result)\n\n    # Stream results asynchronously\n    async for chunk in graph.astream(\"Explain machine learning concepts.\"):\n        print(chunk, end=\"\", flush=True)\n\n# Run the async function\nasyncio.run(main())\n</code></pre>"},{"location":"guides/building_graphs/#advanced-graph-patterns","title":"Advanced Graph Patterns","text":""},{"location":"guides/building_graphs/#parallel-processing","title":"Parallel Processing","text":"<p>Execute multiple agents in parallel:</p> <pre><code># Create a parallel processing section\nbuilder.add_parallel_nodes(\n    [\"market_researcher\", \"technical_researcher\", \"social_researcher\"]\n)\n\n# Add a node to combine the parallel results\nbuilder.add_agent(combiner)\n\n# Connect the parallel nodes to the combiner\nfor node in [\"market_researcher\", \"technical_researcher\", \"social_researcher\"]:\n    builder.connect(source=node, target=\"combiner\")\n</code></pre>"},{"location":"guides/building_graphs/#iterative-refinement","title":"Iterative Refinement","text":"<p>Create loops for iterative improvement:</p> <pre><code># Create a drafting loop\nbuilder.connect(source=\"writer\", target=\"editor\")\nbuilder.connect(source=\"editor\", target=\"quality_check\")\n\n# Add a conditional to either continue refining or finish\ndef is_quality_sufficient(state):\n    quality_score = state.get(\"quality_score\", 0)\n    if quality_score &gt;= 8:\n        return \"complete\"\n    else:\n        return \"writer\"  # Back to writer for another draft\n\nbuilder.add_conditional_edge(\n    source=\"quality_check\",\n    condition_function=is_quality_sufficient,\n    targets=[\"writer\", \"complete\"]\n)\n</code></pre>"},{"location":"guides/building_graphs/#example-complete-research-assistant-graph","title":"Example: Complete Research Assistant Graph","text":"<p>Here's a complete example of a research assistant graph:</p> <pre><code>from lg_adk.builders import GraphBuilder\nfrom lg_adk.agents import Agent\nfrom lg_adk.models import get_model\nfrom lg_adk.tools import WebSearchTool\nfrom lg_adk.memory import MemoryManager\nfrom lg_adk.database import DatabaseManager\n\n# Setup agents\nresearcher = Agent(\n    name=\"researcher\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a research agent. Your job is to find information about a given topic.\n    Use your web search tool to gather relevant information.\"\"\",\n    tools=[WebSearchTool()]\n)\n\nanalyzer = Agent(\n    name=\"analyzer\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are an analysis agent. Your job is to analyze the research provided\n    and extract key insights. Focus on what's most important and relevant.\"\"\"\n)\n\nwriter = Agent(\n    name=\"writer\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a content writing agent. Your job is to take analyzed research\n    and create a well-structured, informative article about the topic.\"\"\"\n)\n\n# Setup memory\nmemory_manager = MemoryManager(\n    database_manager=DatabaseManager(connection_string=\"sqlite:///research_graph.db\")\n)\n\n# Create graph builder\nbuilder = GraphBuilder(name=\"research_assistant\")\n\n# Add agents\nbuilder.add_agent(researcher)\nbuilder.add_agent(analyzer)\nbuilder.add_agent(writer)\n\n# Add memory\nbuilder.add_memory(memory_manager)\n\n# Connect agents\nbuilder.connect(source=\"__start__\", target=\"researcher\")  # Start with researcher\nbuilder.connect(source=\"researcher\", target=\"analyzer\")   # Send research to analyzer\nbuilder.connect(source=\"analyzer\", target=\"writer\")       # Send analysis to writer\nbuilder.connect(source=\"writer\", target=\"__end__\")        # End with writer's output\n\n# Build the graph\ngraph = builder.build()\n\n# Run the graph\nresult = graph.run(\"What are the latest advancements in fusion energy research?\")\nprint(result)\n</code></pre>"},{"location":"guides/building_graphs/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Plan Your Graph: Sketch out the flow before implementation to visualize node relationships.</p> </li> <li> <p>Name Agents Clearly: Use descriptive names that indicate function.</p> </li> <li> <p>Keep Prompts Focused: Each agent should have a clear, specific role.</p> </li> <li> <p>Test Incremental Builds: Add and test a few nodes at a time rather than building the entire graph at once.</p> </li> <li> <p>Use Conditional Routing: Leverage conditional logic to create more adaptive workflows.</p> </li> <li> <p>Manage State Carefully: Be mindful of what information needs to be passed between nodes.</p> </li> <li> <p>Include Error Handling: Add error handling nodes or conditional paths for unexpected scenarios.</p> </li> <li> <p>Monitor Performance: Track execution time and success rates to optimize your graph.</p> </li> </ol> <p>With these techniques, you can build powerful, multi-agent systems that can tackle complex, multi-stage tasks with LG-ADK. For more details on specific components, refer to the Creating Agents, Tool Integration, and Memory Management guides. </p>"},{"location":"guides/creating_agents/","title":"Creating Agents with LG-ADK","text":"<p>This guide covers how to create agents using the LangGraph Agent Development Kit (LG-ADK).</p>"},{"location":"guides/creating_agents/#agent-fundamentals","title":"Agent Fundamentals","text":"<p>An agent in LG-ADK is an entity that can: - Process and understand natural language - Make decisions based on provided information - Use tools to perform actions - Maintain state across interactions</p>"},{"location":"guides/creating_agents/#basic-agent-creation","title":"Basic Agent Creation","text":""},{"location":"guides/creating_agents/#step-1-import-the-required-classes","title":"Step 1: Import the Required Classes","text":"<pre><code>from lg_adk.agents import Agent\nfrom lg_adk.models import get_model\n</code></pre>"},{"location":"guides/creating_agents/#step-2-initialize-your-agent","title":"Step 2: Initialize Your Agent","text":"<pre><code># Create a simple agent\nagent = Agent(\n    name=\"assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a helpful AI assistant that answers user questions.\"\n)\n</code></pre> <p>The basic parameters for creating an agent are: - <code>name</code>: A unique identifier for your agent - <code>model</code>: The language model the agent will use (from the model registry) - <code>system_prompt</code>: Instructions that define the agent's behavior and capabilities</p>"},{"location":"guides/creating_agents/#step-3-using-your-agent","title":"Step 3: Using Your Agent","text":"<pre><code># Run the agent with a user input\nresponse = agent.run(\"What is the capital of France?\")\nprint(response)\n</code></pre>"},{"location":"guides/creating_agents/#advanced-agent-configuration","title":"Advanced Agent Configuration","text":"<p>You can configure more advanced settings for your agents:</p> <pre><code>from lg_adk.tools import WebSearchTool\nfrom lg_adk.memory import MemoryManager\nfrom lg_adk.database import DatabaseManager\n\n# Create a more advanced agent\nadvanced_agent = Agent(\n    name=\"research_assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a research assistant that helps users find and summarize information.\n    Use your tools when appropriate to provide accurate and up-to-date information.\"\"\",\n    tools=[WebSearchTool()],\n    memory_manager=MemoryManager(\n        database_manager=DatabaseManager(connection_string=\"sqlite:///memory.db\")\n    ),\n    max_tokens=1024,\n    temperature=0.7\n)\n</code></pre>"},{"location":"guides/creating_agents/#agent-with-custom-behavior","title":"Agent with Custom Behavior","text":"<p>You can customize how your agent processes inputs and outputs:</p> <pre><code>class CustomAgent(Agent):\n    def preprocess_input(self, user_input: str) -&gt; str:\n        \"\"\"Customize how user input is processed before being sent to the model\"\"\"\n        # Add a timestamp to each input\n        import datetime\n        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        return f\"[{timestamp}] User said: {user_input}\"\n\n    def postprocess_output(self, model_output: str) -&gt; str:\n        \"\"\"Customize how model output is processed before being returned to the user\"\"\"\n        # Add a signature to each response\n        return f\"{model_output}\\n\\n- CustomAgent\"\n\n# Initialize the custom agent\ncustom_agent = CustomAgent(\n    name=\"custom_assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a helpful assistant with a custom personality.\"\n)\n</code></pre>"},{"location":"guides/creating_agents/#streaming-responses","title":"Streaming Responses","text":"<p>LG-ADK supports streaming responses from your agent:</p> <pre><code># Initialize an agent with streaming enabled\nstreaming_agent = Agent(\n    name=\"streaming_assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a helpful AI assistant.\",\n    stream=True\n)\n\n# Process a streaming response\nfor chunk in streaming_agent.stream(\"Tell me a short story about a robot.\"):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"guides/creating_agents/#asynchronous-agents","title":"Asynchronous Agents","text":"<p>For applications requiring non-blocking operation, use async versions:</p> <pre><code>import asyncio\n\nasync def main():\n    # Initialize an async agent\n    agent = Agent(\n        name=\"async_assistant\",\n        model=get_model(\"openai/gpt-4\"),\n        system_prompt=\"You are a helpful AI assistant.\"\n    )\n\n    # Run the agent asynchronously\n    response = await agent.arun(\"What is the meaning of life?\")\n    print(response)\n\n    # Stream responses asynchronously\n    async for chunk in agent.astream(\"Tell me about quantum computing.\"):\n        print(chunk, end=\"\", flush=True)\n\n# Run the async function\nasyncio.run(main())\n</code></pre>"},{"location":"guides/creating_agents/#agent-with-multiple-models","title":"Agent with Multiple Models","text":"<p>You can create agents that can switch between different models:</p> <pre><code>from lg_adk.models import ModelRegistry\n\n# Register multiple models\nregistry = ModelRegistry()\nregistry.register(\"default\", \"openai/gpt-4\")\nregistry.register(\"fast\", \"openai/gpt-3.5-turbo\")\nregistry.register(\"local\", \"ollama/llama2\")\n\n# Create an agent that can switch models\nmulti_model_agent = Agent(\n    name=\"adaptive_assistant\",\n    model=registry.get(\"default\"),\n    system_prompt=\"You are a helpful assistant that adapts to user needs.\"\n)\n\n# Later, switch to a different model\nmulti_model_agent.model = registry.get(\"fast\")\nresponse = multi_model_agent.run(\"Give me a quick answer!\")\n</code></pre>"},{"location":"guides/creating_agents/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Clear System Prompts: Define precisely what your agent should and shouldn't do.</p> </li> <li> <p>Appropriate Tools: Only give your agent the tools it needs for its specific tasks.</p> </li> <li> <p>Memory Management: Configure memory appropriately for your use case.</p> </li> <li> <p>Error Handling: Implement proper error handling, especially for tool usage.</p> </li> <li> <p>Testing: Test your agents thoroughly with different inputs to ensure they behave as expected.</p> </li> </ol>"},{"location":"guides/creating_agents/#example-complete-agent-setup","title":"Example: Complete Agent Setup","text":"<p>Here's a complete example that brings together various concepts:</p> <pre><code>from lg_adk.agents import Agent\nfrom lg_adk.models import get_model\nfrom lg_adk.tools import WebSearchTool, CalculatorTool\nfrom lg_adk.memory import MemoryManager\nfrom lg_adk.database import DatabaseManager\n\n# Setup tools\ntools = [\n    WebSearchTool(),\n    CalculatorTool()\n]\n\n# Setup memory\nmemory_manager = MemoryManager(\n    database_manager=DatabaseManager(connection_string=\"sqlite:///agent_memory.db\")\n)\n\n# Create the agent\nresearch_agent = Agent(\n    name=\"research_assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a research assistant with these capabilities:\n    1. You can search the web for current information\n    2. You can perform calculations\n    3. You remember previous conversations with the user\n\n    Always be helpful, accurate, and concise in your responses.\n    When you don't know something, use your tools rather than guessing.\n    \"\"\",\n    tools=tools,\n    memory_manager=memory_manager,\n    temperature=0.2,\n    max_tokens=1024\n)\n\n# Use the agent\nresponse = research_agent.run(\"What was the population of Tokyo in 2022, and what percentage of Japan's total population does that represent?\")\nprint(response)\n</code></pre> <p>With this guide, you should now have a good understanding of how to create and configure agents using LG-ADK. For more advanced use cases, see the related guides on Building Graphs, Tool Integration, and Memory Management. </p>"},{"location":"guides/human_in_the_loop/","title":"Human-in-the-Loop with LG-ADK","text":"<p>This guide explains how to implement human-in-the-loop interactions in the LangGraph Agent Development Kit, allowing agents to request human input during their processing.</p>"},{"location":"guides/human_in_the_loop/#understanding-human-in-the-loop","title":"Understanding Human-in-the-Loop","text":"<p>Human-in-the-loop (HITL) enables:</p> <ol> <li>Agents to request human input or clarification</li> <li>Humans to review and approve agent actions</li> <li>Collaborative problem-solving between humans and agents</li> <li>Building systems with appropriate human oversight</li> </ol>"},{"location":"guides/human_in_the_loop/#core-components","title":"Core Components","text":"<p>The HITL system in LG-ADK consists of:</p> <ul> <li>HumanInputNode: A specialized graph node that pauses for human input</li> <li>HumanManager: Manages the interaction between agents and humans</li> <li>GraphBuilder HITL methods: Helper methods to enable HITL in your graphs</li> </ul>"},{"location":"guides/human_in_the_loop/#basic-implementation","title":"Basic Implementation","text":"<p>To enable human-in-the-loop in your graph:</p> <pre><code>from lg_adk.builders import GraphBuilder\nfrom lg_adk.agents import Agent\nfrom lg_adk.models import get_model\nfrom lg_adk.human import HumanManager\n\n# Create an agent\nagent = Agent(\n    name=\"assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a helpful assistant that knows when to ask humans for help.\"\n)\n\n# Create a human manager\nhuman_manager = HumanManager()\n\n# Create a graph with human-in-the-loop\nbuilder = GraphBuilder()\nbuilder.add_agent(agent)\nbuilder.enable_human_in_the_loop(human_manager)\n\n# Build the graph\ngraph = builder.build()\n</code></pre>"},{"location":"guides/human_in_the_loop/#running-a-graph-with-human-input","title":"Running a Graph with Human Input","text":"<p>When running a graph with HITL enabled:</p> <pre><code># Start a conversation that might require human input\nresult = graph.run(\"What's the best approach for our new marketing campaign?\")\n\n# Check if human input is required\nif result.state.get(\"requires_human_input\", False):\n    # Provide human input\n    human_response = input(\"Human response: \")\n\n    # Continue the conversation with human input\n    updated_result = graph.continue_run(\n        state=result.state,\n        human_input=human_response\n    )\n</code></pre>"},{"location":"guides/human_in_the_loop/#agent-initiated-human-input","title":"Agent-Initiated Human Input","text":"<p>You can configure your agent to recognize when it should request human input:</p> <pre><code>agent = Agent(\n    name=\"customer_service\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a customer service agent.\n\n    If the customer request involves:\n    - High-value refunds (over $100)\n    - Account closures\n    - Complaints requiring escalation\n\n    Ask for human supervisor input by saying \"I need to consult with a supervisor on this.\"\n    Otherwise, handle the request yourself.\"\"\"\n)\n</code></pre> <p>The HumanManager will automatically detect this pattern and pause for human input.</p>"},{"location":"guides/human_in_the_loop/#configuring-human-intervention-triggers","title":"Configuring Human Intervention Triggers","text":"<p>You can customize when the system should pause for human input:</p> <pre><code>from lg_adk.human import HumanManager\n\n# Create a human manager with custom triggers\nhuman_manager = HumanManager(\n    trigger_phrases=[\n        \"I need human input\",\n        \"Please ask the user\",\n        \"Human assistance required\"\n    ],\n    # Also trigger on specific actions or keywords\n    trigger_keywords=[\"refund\", \"escalate\", \"override\"],\n    # Require approval for high-risk actions\n    require_approval_for=[\"database_update\", \"payment_processing\"]\n)\n</code></pre>"},{"location":"guides/human_in_the_loop/#approval-workflows","title":"Approval Workflows","text":"<p>You can implement approval workflows for critical agent actions:</p> <pre><code># Create an agent that proposes actions requiring approval\nagent = Agent(\n    name=\"finance_assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a finance assistant.\n\n    For any transaction over $1000, create a PROPOSED_ACTION that\n    requires human approval before proceeding.\"\"\"\n)\n\n# Configure human manager for approvals\nhuman_manager = HumanManager(\n    approval_required=True,\n    approval_prompt=\"A financial action requires your approval. Do you approve? (yes/no)\"\n)\n\n# In the graph runner\nresult = graph.run(\"Please transfer $5000 to account #12345\")\n\nif result.state.get(\"requires_approval\", False):\n    # Display the proposed action to the human\n    print(f\"Proposed action: {result.state['proposed_action']}\")\n\n    # Get approval decision\n    approval = input(\"Do you approve? (yes/no): \")\n\n    # Continue with the approval decision\n    updated_result = graph.continue_run(\n        state=result.state,\n        human_input={\"approval\": approval == \"yes\"}\n    )\n</code></pre>"},{"location":"guides/human_in_the_loop/#multiple-human-input-points","title":"Multiple Human Input Points","text":"<p>You can configure a graph to pause for human input at multiple points:</p> <pre><code>from lg_adk.builders import GraphBuilder\nfrom lg_adk.agents import Agent\nfrom lg_adk.human import HumanManager, HumanInputNode\n\n# Create agents\nplanner = Agent(\n    name=\"planner\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You create plans based on user requests.\"\n)\n\nexecutor = Agent(\n    name=\"executor\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You execute plans after they've been approved.\"\n)\n\n# Create human input nodes\nplan_approval = HumanInputNode(\n    name=\"plan_approval\",\n    prompt=\"Please review the proposed plan. Do you approve it or want changes?\"\n)\n\nexecution_confirmation = HumanInputNode(\n    name=\"execution_confirmation\",\n    prompt=\"Execution is about to begin. Final confirmation?\"\n)\n\n# Build graph with multiple human input points\nbuilder = GraphBuilder()\nbuilder.add_agent(planner)\nbuilder.add_agent(executor)\nbuilder.add_human_node(plan_approval)\nbuilder.add_human_node(execution_confirmation)\n\n# Connect the components\nbuilder.add_edge(planner, plan_approval)\nbuilder.add_edge(plan_approval, executor)\nbuilder.add_edge(executor, execution_confirmation)\n\n# Build the graph\ngraph = builder.build()\n</code></pre>"},{"location":"guides/human_in_the_loop/#human-in-the-loop-with-streaming","title":"Human-in-the-Loop with Streaming","text":"<p>For a more interactive experience, you can use streaming with HITL:</p> <pre><code># Start a streaming run that might require human input\nfor chunk in graph.stream(\"What's your recommendation for our product roadmap?\"):\n    if chunk.get(\"requires_human_input\", False):\n        # Get human input\n        human_response = input(\"Human input required: \")\n\n        # Continue the stream with human input\n        for updated_chunk in graph.continue_stream(\n            state=chunk,\n            human_input=human_response\n        ):\n            print(updated_chunk.get(\"agent_response\", \"\"))\n    else:\n        print(chunk.get(\"agent_response\", \"\"), end=\"\", flush=True)\n</code></pre>"},{"location":"guides/human_in_the_loop/#configuring-human-input-timeouts","title":"Configuring Human Input Timeouts","text":"<p>You can configure timeouts for human input:</p> <pre><code>from lg_adk.human import HumanManager\n\n# Configure timeouts\nhuman_manager = HumanManager(\n    input_timeout=300,  # 5 minutes in seconds\n    fallback_response=\"No human input received within the timeout period. I'll proceed with the best course of action.\"\n)\n</code></pre> <p>If no human input is received within the timeout, the system will use the fallback response.</p>"},{"location":"guides/human_in_the_loop/#custom-input-and-output-interfaces","title":"Custom Input and Output Interfaces","text":"<p>You can customize how human input is collected and presented:</p> <pre><code>from lg_adk.human import HumanManager, InputInterface, OutputInterface\n\n# Custom input interface (e.g., from a web form)\nclass WebFormInput(InputInterface):\n    def get_input(self, prompt, state):\n        # Code to display prompt in web UI and collect input\n        # This is a placeholder implementation\n        return web_form_submit_value\n\n# Custom output interface (e.g., to a chat UI)\nclass ChatUIOutput(OutputInterface):\n    def display_output(self, output, state):\n        # Code to display output in chat UI\n        # This is a placeholder implementation\n        chat_ui.add_message(output)\n\n# Use custom interfaces\nhuman_manager = HumanManager(\n    input_interface=WebFormInput(),\n    output_interface=ChatUIOutput()\n)\n</code></pre>"},{"location":"guides/human_in_the_loop/#asynchronous-human-input","title":"Asynchronous Human Input","text":"<p>For web applications or services, you can use asynchronous human input:</p> <pre><code>import asyncio\nfrom lg_adk.human import AsyncHumanManager\n\n# Create an async human manager\nasync_human_manager = AsyncHumanManager()\n\n# In an async context\nasync def process_request(user_input, user_id):\n    # Start a graph run\n    result = await graph.arun(user_input, session_id=user_id)\n\n    if result.state.get(\"requires_human_input\", False):\n        # Store the state waiting for human input\n        await async_human_manager.store_pending_state(user_id, result.state)\n        # Return indication that human input is needed\n        return {\"status\": \"waiting_for_human_input\"}\n\n    return {\"status\": \"complete\", \"response\": result.response}\n\n# When human input arrives later\nasync def process_human_input(user_id, human_input):\n    # Retrieve the pending state\n    pending_state = await async_human_manager.get_pending_state(user_id)\n\n    if pending_state:\n        # Continue the run with the human input\n        updated_result = await graph.acontinue_run(\n            state=pending_state,\n            human_input=human_input\n        )\n\n        # Clear the pending state\n        await async_human_manager.clear_pending_state(user_id)\n\n        return {\"status\": \"complete\", \"response\": updated_result.response}\n\n    return {\"status\": \"error\", \"message\": \"No pending state found\"}\n</code></pre>"},{"location":"guides/human_in_the_loop/#human-feedback-collection","title":"Human Feedback Collection","text":"<p>You can use HITL to collect and incorporate human feedback:</p> <pre><code>from lg_adk.human import HumanFeedbackNode\n\n# Create a feedback node\nfeedback_node = HumanFeedbackNode(\n    name=\"result_feedback\",\n    prompt=\"How satisfied are you with this response? (1-5)\",\n    feedback_options=[\"1\", \"2\", \"3\", \"4\", \"5\"]\n)\n\n# Add to graph\nbuilder = GraphBuilder()\nbuilder.add_agent(agent)\nbuilder.add_human_node(feedback_node)\nbuilder.add_edge(agent, feedback_node)\n\n# Process feedback\ndef handle_feedback(feedback_value, session_id):\n    # Store feedback for later analysis\n    feedback_store.add({\n        \"session_id\": session_id,\n        \"rating\": feedback_value,\n        \"timestamp\": datetime.now()\n    })\n\n    # For low ratings, trigger review\n    if int(feedback_value) &lt;= 2:\n        trigger_human_review(session_id)\n</code></pre>"},{"location":"guides/human_in_the_loop/#complete-example-customer-support-with-human-escalation","title":"Complete Example: Customer Support with Human Escalation","text":"<p>Here's a complete example of a customer support system with HITL:</p> <pre><code>from lg_adk.agents import Agent\nfrom lg_adk.builders import GraphBuilder\nfrom lg_adk.models import get_model\nfrom lg_adk.human import HumanManager, HumanInputNode\nfrom lg_adk.tools import MemoryTool\nfrom lg_adk.memory import MemoryManager\nfrom lg_adk.database import DatabaseManager\n\n# Set up memory\ndb_manager = DatabaseManager(connection_string=\"sqlite:///customer_support.db\")\nmemory_manager = MemoryManager(database_manager=db_manager)\nmemory_tool = MemoryTool(memory_manager=memory_manager)\n\n# Create tier-1 support agent\ntier1_agent = Agent(\n    name=\"tier1_support\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a Tier 1 customer support agent.\n\n    Handle basic customer inquiries about:\n    - Account information\n    - Basic troubleshooting\n    - Product information\n\n    If the customer issue involves:\n    - Technical problems you can't solve\n    - Billing disputes\n    - Complaints about service\n\n    Say \"This requires escalation to a specialist.\" to trigger escalation.\"\"\",\n    tools=[memory_tool]\n)\n\n# Create tier-2 support agent\ntier2_agent = Agent(\n    name=\"tier2_support\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a Tier 2 specialist with advanced knowledge.\n\n    Review the conversation history and provide expert assistance.\n    If you need additional customer information, ask for it.\n\n    If the issue is beyond your scope, say \"This requires human manager review.\".\"\"\",\n    tools=[memory_tool]\n)\n\n# Create human input nodes\nhuman_specialist = HumanInputNode(\n    name=\"human_specialist\",\n    prompt=\"This customer issue requires human specialist review. Please provide guidance:\"\n)\n\n# Create human manager\nhuman_manager = HumanManager(\n    trigger_phrases=[\"This requires escalation to a specialist.\", \n                     \"This requires human manager review.\"]\n)\n\n# Build the graph\nbuilder = GraphBuilder()\nbuilder.add_agent(tier1_agent)\nbuilder.add_agent(tier2_agent)\nbuilder.add_human_node(human_specialist)\nbuilder.enable_human_in_the_loop(human_manager)\n\n# Connect the components\nbuilder.add_edge(tier1_agent, tier2_agent, \n                 condition=lambda state: \"requires escalation\" in state.get(\"agent_response\", \"\").lower())\nbuilder.add_edge(tier2_agent, human_specialist, \n                 condition=lambda state: \"requires human manager review\" in state.get(\"agent_response\", \"\").lower())\n\n# Build the graph\nsupport_graph = builder.build()\n\n# Example usage\nsession_id = \"customer_123\"\n\n# Initial customer inquiry\nresult = support_graph.run(\"I'm having trouble with my billing. My last invoice shows charges for services I didn't use.\", \n                        session_id=session_id)\n\n# If human input is needed at any point\nif result.state.get(\"requires_human_input\", False):\n    # Get input from human specialist\n    specialist_input = input(\"Specialist input: \")\n\n    # Continue with the human input\n    updated_result = support_graph.continue_run(\n        state=result.state,\n        human_input=specialist_input\n    )\n\n    print(\"Final response:\", updated_result.response)\nelse:\n    print(\"Response:\", result.response)\n</code></pre>"},{"location":"guides/human_in_the_loop/#best-practices-for-human-in-the-loop","title":"Best Practices for Human-in-the-Loop","text":"<ol> <li>Clear Prompting: Ensure agents have clear instructions about when to request human input</li> <li>Meaningful Context: Provide humans with sufficient context to make informed decisions</li> <li>Appropriate Timeouts: Set reasonable timeouts based on the urgency of the task</li> <li>Fallback Mechanisms: Implement fallback behaviors when human input isn't available</li> <li>User Experience: Design the human interaction to be intuitive and unobtrusive</li> <li>Feedback Loop: Collect data on which interactions require human intervention to improve the system</li> <li>Progressive Autonomy: Start with more human oversight and gradually reduce it as the system proves reliable</li> </ol> <p>By implementing human-in-the-loop functionality with LG-ADK, you can build AI systems that combine the strengths of automated agents with human judgment and expertise. For more information on related topics, see the Building Graphs, Creating Agents, and Tool Integration guides. </p>"},{"location":"guides/langgraph_cli_integration/","title":"LangGraph CLI Integration","text":"<p>This guide explains how to integrate your LG-ADK applications with the LangGraph CLI for development, debugging, and deployment.</p>"},{"location":"guides/langgraph_cli_integration/#understanding-langgraph-cli","title":"Understanding LangGraph CLI","text":"<p>The LangGraph CLI provides several useful commands for working with LangGraph applications:</p> <ul> <li><code>langgraph dev</code>: Start a development server for testing your graphs</li> <li><code>langgraph serve</code>: Serve your graphs in production</li> <li><code>langgraph deploy</code>: Deploy your graphs to the cloud</li> <li><code>langgraph list</code>: List available graphs in your project</li> </ul>"},{"location":"guides/langgraph_cli_integration/#project-configuration","title":"Project Configuration","text":"<p>To use the LangGraph CLI with your LG-ADK project, you need a proper configuration file:</p>"},{"location":"guides/langgraph_cli_integration/#the-langgraphjson-file","title":"The <code>langgraph.json</code> File","text":"<p>Create a <code>langgraph.json</code> file in your project root:</p> <pre><code>{\n  \"graphs\": {\n    \"chat\": \"lg_adk.graphs.chat:graph\",\n    \"rag\": \"lg_adk.graphs.rag:graph\",\n    \"multi_agent\": \"lg_adk.graphs.multi_agent:graph\"\n  }\n}\n</code></pre> <p>Each entry specifies: - A name for the graph (e.g., \"chat\") - The import path in the format: <code>module.path:graph_variable_name</code></p>"},{"location":"guides/langgraph_cli_integration/#structuring-your-graphs-for-discovery","title":"Structuring Your Graphs for Discovery","text":"<p>To make your graphs discoverable by the LangGraph CLI, follow these conventions:</p>"},{"location":"guides/langgraph_cli_integration/#1-export-graph-variables","title":"1. Export Graph Variables","text":"<p>When defining graphs, export them as top-level variables:</p> <pre><code># In lg_adk/graphs/chat.py\nfrom langgraph.graph import Graph\nfrom lg_adk.agents import Agent\nfrom lg_adk.builders import GraphBuilder\n\n# Create and build the graph\nbuilder = GraphBuilder(name=\"chat\")\nbuilder.add_agent(Agent(name=\"assistant\", model=\"openai/gpt-4\"))\ngraph = builder.build()  # This is the variable the CLI will look for\n</code></pre>"},{"location":"guides/langgraph_cli_integration/#2-use-type-annotations","title":"2. Use Type Annotations","text":"<p>Properly type your graph for better tooling support:</p> <pre><code>from langgraph.graph import Graph\nfrom typing import Dict, Any, TypedDict\n\nclass ChatState(TypedDict):\n    messages: list\n    session_id: str\n\n# Create typed graph\ngraph: Graph[ChatState] = builder.build()\n</code></pre>"},{"location":"guides/langgraph_cli_integration/#session-management-with-langgraph-cli","title":"Session Management with LangGraph CLI","text":"<p>Proper session handling is crucial for multi-turn conversations with LangGraph CLI:</p>"},{"location":"guides/langgraph_cli_integration/#implementing-session-aware-graphs","title":"Implementing Session-Aware Graphs","text":"<p>Here's how to create graphs that correctly handle session state:</p> <pre><code>from langgraph.graph import Graph\nfrom typing import Dict, Any, TypedDict\nfrom lg_adk.agents import Agent\nfrom lg_adk.builders import GraphBuilder\n\n# Define state type\nclass GraphState(TypedDict):\n    messages: list\n    session_id: str\n    metadata: Dict[str, Any]\n\ndef build_graph():\n    # Create an agent\n    agent = Agent(\n        name=\"assistant\",\n        model=\"openai/gpt-4\",\n        system_prompt=\"You are a helpful assistant.\"\n    )\n\n    # Create graph builder\n    builder = GraphBuilder(name=\"chat\")\n    builder.add_agent(agent)\n\n    # Configure to track session in state\n    builder.configure_state_tracking(\n        include_session_id=True,\n        include_metadata=True\n    )\n\n    # Build the graph\n    return builder.build()\n\n# Export the graph for LangGraph CLI\ngraph = build_graph()\n</code></pre>"},{"location":"guides/langgraph_cli_integration/#correct-context-handling","title":"Correct Context Handling","text":"<p>Ensure your agent functions properly handle the checkpointer state:</p> <pre><code>from langgraph.checkpoint.base import Checkpointer\nfrom lg_adk.agents import Agent\nfrom typing import Dict, Any\n\nclass SessionAwareAgent(Agent):\n    def run(self, input_text: str, session_id: str = None, config: Dict[str, Any] = None):\n        \"\"\"Run the agent with proper session handling.\"\"\"\n        # Configure for checkpointer\n        if not config:\n            config = {}\n\n        # Set session ID in config for langgraph checkpointer\n        config[\"configurable\"] = {\n            \"thread_id\": session_id\n        }\n\n        # Get current state from checkpointer if available\n        current_state = None\n        if hasattr(self, \"graph\") and self.graph:\n            try:\n                current_state = self.graph.get_state(config)\n            except Exception:\n                current_state = None\n\n        # Process messages based on state\n        graph_messages = (\n            current_state.values.get(\"messages\", [])\n            if current_state and hasattr(current_state, \"values\")\n            else []\n        )\n\n        # Initialize with system message if needed\n        messages = []\n        if self.system_prompt and not graph_messages:\n            messages.append({\"role\": \"system\", \"content\": self.system_prompt})\n\n        # Add user message\n        messages.append({\"role\": \"user\", \"content\": input_text})\n\n        # Invoke the graph\n        result = self.model.generate(\n            messages=messages,\n            config=config\n        )\n\n        return result\n</code></pre>"},{"location":"guides/langgraph_cli_integration/#using-the-development-server","title":"Using the Development Server","text":"<p>Start the development server to test your graphs:</p> <pre><code>langgraph dev\n</code></pre> <p>This will: 1. Discover graphs in your <code>langgraph.json</code> 2. Start a local server (by default at http://localhost:3000) 3. Provide a web interface for testing your graphs</p>"},{"location":"guides/langgraph_cli_integration/#simplified-development-with-makefile","title":"Simplified Development with Makefile","text":"<p>Create a <code>Makefile</code> in your project root to simplify common tasks:</p> <pre><code>.PHONY: dev serve deploy list install test docs\n\n# Installation\ninstall:\n    pip install -e \".[dev]\"\n\n# LangGraph CLI commands\ndev:\n    langgraph dev\n\nserve:\n    langgraph serve\n\ndeploy:\n    langgraph deploy\n\nlist:\n    langgraph list\n\n# Testing\ntest:\n    pytest tests/\n\n# Documentation\ndocs:\n    mkdocs serve\n</code></pre> <p>This allows you to use simple commands like: - <code>make dev</code> - Start development server - <code>make test</code> - Run tests - <code>make docs</code> - Start documentation server</p>"},{"location":"guides/langgraph_cli_integration/#example-graphs-in-lg-adk","title":"Example Graphs in LG-ADK","text":"<p>LG-ADK includes several example graphs that are ready to use with the LangGraph CLI:</p>"},{"location":"guides/langgraph_cli_integration/#chat-graph","title":"Chat Graph","text":"<p>The <code>lg_adk.graphs.chat</code> module provides a simple chat graph that:</p> <ul> <li>Processes user messages</li> <li>Maintains conversation context across multiple turns</li> <li>Uses a single agent to generate responses</li> <li>Properly handles session state for the LangGraph CLI</li> </ul> <p>Key features of this implementation: - Type-annotated state - Session ID tracking in state - Proper checkpointer configuration - Graph exported as a top-level variable</p>"},{"location":"guides/langgraph_cli_integration/#rag-graph","title":"RAG Graph","text":"<p>The <code>lg_adk.graphs.rag</code> module implements a Retrieval-Augmented Generation graph that:</p> <ul> <li>Retrieves relevant documents based on user queries</li> <li>Incorporates document content into the agent's response</li> <li>Maintains session and conversation context</li> <li>Uses typed state for better tooling support</li> </ul>"},{"location":"guides/langgraph_cli_integration/#multi-agent-graph","title":"Multi-Agent Graph","text":"<p>The <code>lg_adk.graphs.multi_agent</code> module provides a sophisticated multi-agent collaboration system:</p> <ul> <li>Uses a researcher, writer, and critic agent working together</li> <li>Implements task-based workflows</li> <li>Tracks agent contributions in state</li> <li>Properly handles routing between agents</li> <li>Maintains session context across the entire workflow</li> </ul> <p>Each of these examples can be run directly with LangGraph CLI using the <code>langgraph dev</code> command or imported and extended for your own applications.</p>"},{"location":"guides/langgraph_cli_integration/#complete-example-chat-application-with-cli-support","title":"Complete Example: Chat Application with CLI Support","text":"<p>Here's a complete example of a chat application designed to work with LangGraph CLI:</p> <pre><code># File: lg_adk/graphs/chat.py\nfrom typing import TypedDict, List, Dict, Any\nfrom pydantic import BaseModel\nfrom langgraph.graph import Graph\nfrom lg_adk.agents import Agent\nfrom lg_adk.models import get_model\nfrom lg_adk.memory import MemoryManager\nfrom lg_adk.builders import GraphBuilder\nfrom lg_adk.sessions import SessionManager\n\n# Define message type\nclass Message(BaseModel):\n    role: str\n    content: str\n\n# Define state type\nclass ChatState(TypedDict):\n    messages: List[Message]\n    session_id: str\n    metadata: Dict[str, Any]\n\n# Create components\nagent = Agent(\n    name=\"chat_assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a helpful, friendly assistant.\"\n)\n\nmemory_manager = MemoryManager()\nsession_manager = SessionManager()\n\n# Build graph\nbuilder = GraphBuilder(name=\"chat\")\nbuilder.add_agent(agent)\nbuilder.add_memory(memory_manager)\nbuilder.configure_state_tracking(\n    include_session_id=True,\n    include_metadata=True\n)\n\n# Define state handlers\ndef add_message_to_state(state: ChatState, message: Message) -&gt; ChatState:\n    \"\"\"Add a message to the state.\"\"\"\n    messages = state.get(\"messages\", [])\n    return {\n        **state,\n        \"messages\": messages + [message]\n    }\n\n# Configure message processing\nbuilder.on_message(add_message_to_state)\n\n# Build and export the graph\ngraph: Graph[ChatState] = builder.build()\n</code></pre> <p>When you run <code>langgraph dev</code>, this graph will be available for testing via the web interface.</p>"},{"location":"guides/langgraph_cli_integration/#advanced-custom-state-persistence","title":"Advanced: Custom State Persistence","text":"<p>For applications requiring custom state persistence:</p> <pre><code>from lg_adk.database import DatabaseManager\nfrom lg_adk.sessions import DatabaseSessionManager\n\n# Create a database-backed session manager\ndb_manager = DatabaseManager(connection_string=\"postgresql://user:pass@localhost:5432/db\")\nsession_manager = DatabaseSessionManager(database_manager=db_manager)\n\n# Configure the builder to use this session manager\nbuilder = GraphBuilder(name=\"persistent_chat\")\nbuilder.add_agent(agent)\nbuilder.configure_session_management(session_manager)\nbuilder.build()\n</code></pre>"},{"location":"guides/langgraph_cli_integration/#best-practices-for-langgraph-cli-integration","title":"Best Practices for LangGraph CLI Integration","text":"<ol> <li>Typed Graphs: Always use type hints for graph states</li> <li>State Immutability: Treat graph states as immutable to avoid unexpected behaviors</li> <li>Session IDs: Generate consistent session IDs for multi-turn conversations</li> <li>Error Handling: Add proper error handling for state persistence</li> <li>Development/Production Split: Use separate configurations for development and production</li> <li>Environment Variables: Use environment variables for sensitive configuration</li> <li>Testing Support: Create test utilities for your graphs</li> </ol> <p>By following these guidelines, you can build LG-ADK applications that work seamlessly with the LangGraph CLI tools for development, debugging, and deployment. </p>"},{"location":"guides/memory_management/","title":"Memory Management in LG-ADK","text":"<p>This guide explains how to implement and use memory management in the LangGraph Agent Development Kit (LG-ADK), allowing agents to store, retrieve, and utilize information across conversations.</p>"},{"location":"guides/memory_management/#understanding-memory-in-lg-adk","title":"Understanding Memory in LG-ADK","text":"<p>Memory in LG-ADK enables agents and graphs to:</p> <ol> <li>Maintain conversation context across multiple interactions</li> <li>Store important information for later retrieval</li> <li>Build knowledge bases specific to user sessions</li> <li>Enable collaborative work between multiple agents</li> </ol> <p>The primary components of the memory system are:</p> <ul> <li>MemoryManager: Orchestrates memory operations</li> <li>DatabaseManager: Handles the underlying storage</li> <li>Memory Tools: Enable agents to interact with memory</li> </ul>"},{"location":"guides/memory_management/#setting-up-memory","title":"Setting Up Memory","text":"<p>To use memory in your agents and graphs, start by setting up the memory components:</p> <pre><code>from lg_adk.memory import MemoryManager\nfrom lg_adk.database import DatabaseManager\n\n# Create a database manager\ndb_manager = DatabaseManager(\n    connection_string=\"sqlite:///agent_memory.db\"\n)\n\n# Create a memory manager\nmemory_manager = MemoryManager(\n    database_manager=db_manager\n)\n</code></pre> <p>The <code>connection_string</code> can be configured for different database types:</p> <ul> <li>SQLite (local): <code>\"sqlite:///agent_memory.db\"</code></li> <li>PostgreSQL: <code>\"postgresql://username:password@localhost:5432/db_name\"</code></li> <li>MySQL: <code>\"mysql://username:password@localhost:3306/db_name\"</code></li> </ul>"},{"location":"guides/memory_management/#adding-memory-to-agents","title":"Adding Memory to Agents","text":"<p>To equip an agent with memory capabilities:</p> <pre><code>from lg_adk.agents import Agent\nfrom lg_adk.models import get_model\nfrom lg_adk.tools import MemoryTool\n\n# Create a memory tool\nmemory_tool = MemoryTool(memory_manager=memory_manager)\n\n# Add to an agent\nagent = Agent(\n    name=\"assistant\",\n    model=get_model(\"openai/gpt-4\"), \n    system_prompt=\"You are a helpful assistant that remembers information.\",\n    tools=[memory_tool]\n)\n</code></pre> <p>The agent can now use the memory tool to store and retrieve information:</p> <pre><code># The agent will use the memory tool to store this information\nagent.run(\"Remember that the user's favorite color is blue.\")\n\n# Later, the agent can retrieve this information\nagent.run(\"What is the user's favorite color?\")\n</code></pre>"},{"location":"guides/memory_management/#adding-memory-to-graphs","title":"Adding Memory to Graphs","text":"<p>To add memory capabilities to a graph:</p> <pre><code>from lg_adk.builders import GraphBuilder\n\n# Create agents\nresearcher = Agent(\n    name=\"researcher\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a research agent that finds information.\",\n    tools=[MemoryTool(memory_manager=memory_manager)]\n)\n\nwriter = Agent(\n    name=\"writer\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a writer that summarizes information.\",\n    tools=[MemoryTool(memory_manager=memory_manager)]\n)\n\n# Create a graph with shared memory\nbuilder = GraphBuilder()\nbuilder.add_agent(researcher)\nbuilder.add_agent(writer)\nbuilder.add_memory(memory_manager)\n\n# Connect agents\nbuilder.add_edge(researcher, writer)\n\n# Build the graph\ngraph = builder.build()\n</code></pre> <p>With this setup, both the researcher and writer agents can share information through memory.</p>"},{"location":"guides/memory_management/#memory-operations","title":"Memory Operations","text":"<p>The MemoryTool provides several operations for agents to use:</p>"},{"location":"guides/memory_management/#storing-information","title":"Storing Information","text":"<p>Agents can store information in memory:</p> <pre><code># Example of how an agent would use the memory tool to store information\nagent.run(\"\"\"\nUse the memory tool to store the following information:\nThe capital of France is Paris.\n\"\"\")\n</code></pre> <p>The memory tool will generate a unique memory ID, store the content with metadata, and return the ID to the agent.</p>"},{"location":"guides/memory_management/#retrieving-information","title":"Retrieving Information","text":"<p>Agents can retrieve information from memory:</p> <pre><code># Example of how an agent would retrieve information from memory\nagent.run(\"What is the capital of France?\")\n</code></pre> <p>The agent will use the memory tool to search for relevant information and incorporate it into its response.</p>"},{"location":"guides/memory_management/#retrieving-by-tags","title":"Retrieving by Tags","text":"<p>Agents can use tags to organize and retrieve related information:</p> <pre><code># Example of how an agent would store information with tags\nagent.run(\"\"\"\nStore this information with the tags 'geography', 'europe':\nFrance is a country in Western Europe.\n\"\"\")\n\n# Later, retrieve information by tags\nagent.run(\"Tell me about European geography.\")\n</code></pre>"},{"location":"guides/memory_management/#deleting-information","title":"Deleting Information","text":"<p>Agents can delete information when it's no longer needed:</p> <pre><code># Example of how an agent would delete a specific memory by ID\nagent.run(\"Delete the information about France's capital.\")\n</code></pre>"},{"location":"guides/memory_management/#session-based-memory","title":"Session-Based Memory","text":"<p>LG-ADK memory is session-based, allowing for separate memory contexts for different users or conversations:</p> <pre><code># Create a session for a specific user\nsession_id = \"user_123\"\n\n# Run the agent or graph with the session ID\nresult = agent.run(\"Remember my name is Alice.\", session_id=session_id)\n\n# Later, continue the same session\nresult = agent.run(\"What's my name?\", session_id=session_id)\n</code></pre> <p>Different sessions won't share memories, ensuring privacy and context separation.</p>"},{"location":"guides/memory_management/#memory-schemas-and-structure","title":"Memory Schemas and Structure","text":"<p>Memory in LG-ADK is stored as documents with the following structure:</p> <pre><code>{\n    \"id\": \"unique_memory_id\",\n    \"session_id\": \"session_identifier\",\n    \"content\": \"The actual information stored\",\n    \"tags\": [\"tag1\", \"tag2\"],\n    \"metadata\": {\n        \"source\": \"user_input\",\n        \"importance\": \"high\",\n        # Any additional metadata\n    },\n    \"timestamp\": \"2023-10-23T14:30:00Z\"\n}\n</code></pre> <p>You can customize how agents use these fields through their system prompts.</p>"},{"location":"guides/memory_management/#advanced-memory-patterns","title":"Advanced Memory Patterns","text":""},{"location":"guides/memory_management/#collaborative-memory-use","title":"Collaborative Memory Use","text":"<p>Multiple agents can collaborate using shared memory:</p> <pre><code># First agent stores information\nagent1.run(\"Store that the meeting is scheduled for Tuesday at 3 PM.\", session_id=\"team_project\")\n\n# Second agent accesses the same information\nagent2.run(\"When is our next meeting?\", session_id=\"team_project\")\n</code></pre>"},{"location":"guides/memory_management/#memory-prioritization","title":"Memory Prioritization","text":"<p>Guide agents to prioritize certain memories:</p> <pre><code># Store information with importance metadata\nagent.run(\"\"\"\nStore this information with metadata importance=high:\nThe client deadline is October 30th.\n\"\"\")\n\n# Update the agent's system prompt to use importance\nagent.system_prompt = \"\"\"You are a helpful assistant.\nWhen retrieving memories, prioritize those with high importance.\n\"\"\"\n</code></pre>"},{"location":"guides/memory_management/#memory-summarization","title":"Memory Summarization","text":"<p>Implement periodic memory summarization:</p> <pre><code>from lg_adk.agents import Agent\nfrom lg_adk.models import get_model\n\n# Create a summarizer agent\nsummarizer = Agent(\n    name=\"memory_summarizer\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You summarize memories into concise knowledge.\n    Create a single paragraph summary of all related memories.\"\"\",\n    tools=[MemoryTool(memory_manager=memory_manager)]\n)\n\n# Function to periodically summarize memories\ndef summarize_memories(session_id):\n    # Retrieve all memories for the session\n    all_memories = memory_manager.retrieve(session_id=session_id)\n\n    # Run the summarizer agent\n    summary = summarizer.run(f\"Summarize the following memories: {all_memories}\")\n\n    # Store the summary as a new memory with a special tag\n    memory_manager.store(\n        session_id=session_id,\n        content=summary,\n        tags=[\"summary\"],\n        metadata={\"type\": \"memory_summary\"}\n    )\n</code></pre>"},{"location":"guides/memory_management/#memory-persistence","title":"Memory Persistence","text":"<p>LG-ADK memory is persistent across application restarts. To ensure data isn't lost:</p> <ol> <li>Use a production-grade database for the <code>DatabaseManager</code></li> <li>Implement regular database backups</li> <li>Consider database migration strategies for version upgrades</li> </ol>"},{"location":"guides/memory_management/#complete-example-question-answering-with-memory","title":"Complete Example: Question-Answering with Memory","text":"<p>Here's a complete example of a question-answering agent with memory:</p> <pre><code>from lg_adk.agents import Agent\nfrom lg_adk.models import get_model\nfrom lg_adk.memory import MemoryManager\nfrom lg_adk.database import DatabaseManager\nfrom lg_adk.tools import MemoryTool, WebSearchTool\n\n# Setup memory\ndb_manager = DatabaseManager(connection_string=\"sqlite:///qa_system.db\")\nmemory_manager = MemoryManager(database_manager=db_manager)\n\n# Create tools\nmemory_tool = MemoryTool(memory_manager=memory_manager)\nsearch_tool = WebSearchTool()\n\n# Create the QA agent\nqa_agent = Agent(\n    name=\"qa_system\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a question-answering assistant that learns from interactions.\n\n    When a user asks a question:\n    1. Check your memory for relevant information first\n    2. If the answer isn't in memory, use web search to find it\n    3. Store any new information you learn in memory for future reference\n    4. Answer the user's question accurately and concisely\n\n    Always cite your sources, whether from memory or search results.\"\"\",\n    tools=[memory_tool, search_tool]\n)\n\n# Example usage with a session\nsession_id = \"user_session_123\"\n\n# First interaction - agent will search and remember\nresult1 = qa_agent.run(\"What is the capital of Canada?\", session_id=session_id)\nprint(result1)\n\n# Second interaction - agent will use memory\nresult2 = qa_agent.run(\"What's the capital city of Canada again?\", session_id=session_id)\nprint(result2)\n\n# New question - agent will search again\nresult3 = qa_agent.run(\"What's the population of Toronto?\", session_id=session_id)\nprint(result3)\n</code></pre>"},{"location":"guides/memory_management/#best-practices-for-memory-management","title":"Best Practices for Memory Management","text":"<ol> <li>Session Management: Use consistent session IDs to maintain conversation context</li> <li>Memory Cleanup: Implement policies for removing outdated or unused memories</li> <li>Privacy Considerations: Be transparent about data storage and implement retention policies</li> <li>Tagging Strategy: Develop a consistent tagging strategy for easy information retrieval</li> <li>Memory Validation: Consider validating information before storing it in memory</li> <li>Performance Optimization: Index frequently accessed memory fields for faster retrieval</li> <li>Memory Monitoring: Implement monitoring to track memory usage and growth</li> </ol> <p>By following this guide, you can effectively implement and utilize memory management in your LG-ADK applications, enabling more contextually aware and helpful agent interactions. For more information on related topics, see the Building Graphs, Creating Agents, and Tool Integration guides. </p>"},{"location":"guides/model_providers/","title":"Model Providers in LG-ADK","text":"<p>This guide explains how to work with different model providers in the LangGraph Agent Development Kit.</p>"},{"location":"guides/model_providers/#understanding-model-providers","title":"Understanding Model Providers","text":"<p>LG-ADK abstracts away the complexity of working with different language model APIs through a unified provider system. This allows you to:</p> <ol> <li>Switch Models Easily: Change between different models with minimal code changes</li> <li>Support Multiple Providers: Work with OpenAI, Google Gemini, Anthropic, and other models </li> <li>Use Local Models: Integrate with Ollama for local model inference</li> <li>Custom Providers: Create your own providers for specialized needs</li> </ol>"},{"location":"guides/model_providers/#getting-started-with-models","title":"Getting Started with Models","text":"<p>The simplest way to get a model is through the <code>get_model</code> function:</p> <pre><code>from lg_adk.models import get_model\n\n# Get an OpenAI model\nopenai_model = get_model(\"openai/gpt-4\")\n\n# Get a Google Gemini model\ngemini_model = get_model(\"google/gemini-pro\")\n\n# Get an Anthropic model\nclaude_model = get_model(\"anthropic/claude-3-opus\")\n\n# Get an Ollama model (local)\nlocal_model = get_model(\"ollama/llama3\")\n</code></pre>"},{"location":"guides/model_providers/#available-model-providers","title":"Available Model Providers","text":"<p>LG-ADK supports these model providers out of the box:</p>"},{"location":"guides/model_providers/#openai","title":"OpenAI","text":"<pre><code># OpenAI GPT-4 model\nmodel = get_model(\"openai/gpt-4\")\n\n# OpenAI GPT-4o model\nmodel = get_model(\"openai/gpt-4o\")\n\n# OpenAI GPT-3.5 Turbo model\nmodel = get_model(\"openai/gpt-3.5-turbo\")\n</code></pre>"},{"location":"guides/model_providers/#google-gemini","title":"Google Gemini","text":"<pre><code># Google Gemini Pro model\nmodel = get_model(\"google/gemini-pro\")\n\n# Google Gemini Ultra\nmodel = get_model(\"google/gemini-1.5-pro\")\n</code></pre>"},{"location":"guides/model_providers/#anthropic","title":"Anthropic","text":"<pre><code># Anthropic Claude 3 Opus\nmodel = get_model(\"anthropic/claude-3-opus\")\n\n# Anthropic Claude 3 Sonnet\nmodel = get_model(\"anthropic/claude-3-sonnet\")\n\n# Anthropic Claude 3 Haiku\nmodel = get_model(\"anthropic/claude-3-haiku\")\n</code></pre>"},{"location":"guides/model_providers/#ollama-local-models","title":"Ollama (Local Models)","text":"<pre><code># Local Llama 3 model via Ollama\nmodel = get_model(\"ollama/llama3\")\n\n# Local Mixtral model\nmodel = get_model(\"ollama/mixtral\")\n</code></pre>"},{"location":"guides/model_providers/#model-configuration","title":"Model Configuration","text":"<p>Each model can be configured with provider-specific parameters:</p> <pre><code>from lg_adk.models import get_model\n\n# Configure an OpenAI model\nopenai_model = get_model(\n    \"openai/gpt-4\",\n    temperature=0.7,\n    max_tokens=2000,\n    streaming=True\n)\n\n# Configure a Google model\ngoogle_model = get_model(\n    \"google/gemini-pro\",\n    temperature=0.2,\n    top_p=0.95,\n    top_k=40\n)\n</code></pre>"},{"location":"guides/model_providers/#working-with-the-modelregistry","title":"Working with the ModelRegistry","text":"<p>For more advanced usage, you can interact directly with the <code>ModelRegistry</code>:</p> <pre><code>from lg_adk.models import ModelRegistry\n\n# Get the registry singleton\nregistry = ModelRegistry.get_instance()\n\n# Register a custom model configuration\nregistry.register(\n    \"openai/gpt-4-custom\",\n    provider=\"openai\",\n    model_name=\"gpt-4\",\n    temperature=0.5,\n    top_p=0.9\n)\n\n# Get the custom model\ncustom_model = registry.get_model(\"openai/gpt-4-custom\")\n</code></pre>"},{"location":"guides/model_providers/#provider-specific-authentication","title":"Provider-Specific Authentication","text":"<p>Different providers require different authentication mechanisms:</p>"},{"location":"guides/model_providers/#openai-authentication","title":"OpenAI Authentication","text":"<pre><code>import os\nfrom lg_adk.models import ModelRegistry\n\n# Set OpenAI API key in environment variable\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\n# Or configure explicitly\nregistry = ModelRegistry.get_instance()\nregistry.configure_provider(\n    \"openai\", \n    api_key=\"your-api-key\",\n    organization=\"your-org-id\"  # Optional\n)\n</code></pre>"},{"location":"guides/model_providers/#google-authentication","title":"Google Authentication","text":"<pre><code>import os\nfrom lg_adk.models import ModelRegistry\n\n# Set Google API key in environment variable\nos.environ[\"GOOGLE_API_KEY\"] = \"your-api-key\"\n\n# Or configure explicitly\nregistry = ModelRegistry.get_instance()\nregistry.configure_provider(\n    \"google\", \n    api_key=\"your-api-key\"\n)\n</code></pre>"},{"location":"guides/model_providers/#anthropic-authentication","title":"Anthropic Authentication","text":"<pre><code>import os\nfrom lg_adk.models import ModelRegistry\n\n# Set Anthropic API key in environment variable\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"\n\n# Or configure explicitly\nregistry = ModelRegistry.get_instance()\nregistry.configure_provider(\n    \"anthropic\", \n    api_key=\"your-api-key\"\n)\n</code></pre>"},{"location":"guides/model_providers/#ollama-authentication","title":"Ollama Authentication","text":"<pre><code>from lg_adk.models import ModelRegistry\n\n# Configure Ollama endpoint (defaults to http://localhost:11434)\nregistry = ModelRegistry.get_instance()\nregistry.configure_provider(\n    \"ollama\",\n    base_url=\"http://localhost:11434\"\n)\n</code></pre>"},{"location":"guides/model_providers/#making-direct-model-calls","title":"Making Direct Model Calls","text":"<p>You can use the model objects directly for generation:</p> <pre><code>from lg_adk.models import get_model\n\n# Get a model\nmodel = get_model(\"openai/gpt-4\")\n\n# Generate a response\nresponse = model.generate(\n    \"Explain the theory of relativity briefly.\",\n    system_prompt=\"You are a helpful physics tutor.\"\n)\n\nprint(response)\n</code></pre>"},{"location":"guides/model_providers/#streaming-responses","title":"Streaming Responses","text":"<p>Enable streaming for real-time responses:</p> <pre><code>from lg_adk.models import get_model\n\n# Get a model with streaming enabled\nmodel = get_model(\"openai/gpt-4\", streaming=True)\n\n# Stream a response\nfor chunk in model.generate_stream(\n    \"Write a short poem about the ocean.\",\n    system_prompt=\"You are a poet.\"\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"guides/model_providers/#tool-calling-with-models","title":"Tool Calling with Models","text":"<p>Enable tool calling for models that support it:</p> <pre><code>from lg_adk.models import get_model\nfrom typing import List, Dict, Any\n\n# Define your tools\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get the current weather for a location.\"\"\"\n    # In a real app, you would call a weather API\n    return f\"Sunny and 75\u00b0F in {location}\"\n\ntools = [\n    {\n        \"name\": \"get_weather\",\n        \"description\": \"Get the current weather for a location\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n                }\n            },\n            \"required\": [\"location\"]\n        }\n    }\n]\n\n# Tool calling functions\ndef execute_tool_call(tool_call: Dict[str, Any]) -&gt; str:\n    if tool_call[\"name\"] == \"get_weather\":\n        location = tool_call[\"parameters\"][\"location\"]\n        return get_weather(location)\n    return \"Unknown tool\"\n\n# Get a model\nmodel = get_model(\"openai/gpt-4\", tools=tools)\n\n# Generate with tool calling\nresponse = model.generate(\n    \"What's the weather like in Miami?\",\n    system_prompt=\"You can use tools to answer questions.\"\n)\n\n# Check for tool calls in the response\nif hasattr(response, \"tool_calls\") and response.tool_calls:\n    for tool_call in response.tool_calls:\n        tool_result = execute_tool_call(tool_call)\n\n        # Send the result back to the model\n        follow_up = model.generate(\n            f\"Tool result: {tool_result}\",\n            system_prompt=\"You can use tools to answer questions.\",\n            previous_messages=[\n                {\"role\": \"user\", \"content\": \"What's the weather like in Miami?\"},\n                {\"role\": \"assistant\", \"content\": response.content}\n            ]\n        )\n        print(follow_up)\nelse:\n    print(response)\n</code></pre>"},{"location":"guides/model_providers/#async-model-usage","title":"Async Model Usage","text":"<p>For asynchronous applications:</p> <pre><code>import asyncio\nfrom lg_adk.models import get_model\n\nasync def generate_async():\n    # Get a model\n    model = get_model(\"openai/gpt-4\")\n\n    # Generate asynchronously\n    response = await model.agenerate(\n        \"What are the benefits of asynchronous programming?\",\n        system_prompt=\"You are a programming instructor.\"\n    )\n\n    print(response)\n\n    # Stream asynchronously\n    async for chunk in model.agenerate_stream(\n        \"Explain coroutines in Python.\",\n        system_prompt=\"You are a Python expert.\"\n    ):\n        print(chunk, end=\"\", flush=True)\n\n# Run the async function\nasyncio.run(generate_async())\n</code></pre>"},{"location":"guides/model_providers/#custom-model-providers","title":"Custom Model Providers","text":"<p>You can create your own model provider:</p> <pre><code>from lg_adk.models import ModelProvider, ModelRegistry\nfrom typing import Dict, Any, AsyncGenerator, Optional, List\n\nclass CustomModelProvider(ModelProvider):\n    \"\"\"Custom model provider implementation.\"\"\"\n\n    def __init__(self, api_key: str, **kwargs):\n        super().__init__(**kwargs)\n        self.api_key = api_key\n        # Initialize your custom API client here\n\n    def generate(\n        self, \n        prompt: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_tokens: int = 1000,\n        previous_messages: Optional[List[Dict[str, str]]] = None,\n        **kwargs\n    ) -&gt; str:\n        \"\"\"Generate text using your custom API.\"\"\"\n        # Implement synchronous generation\n        # ...\n        return \"Generated response\"\n\n    async def agenerate(\n        self, \n        prompt: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_tokens: int = 1000,\n        previous_messages: Optional[List[Dict[str, str]]] = None,\n        **kwargs\n    ) -&gt; str:\n        \"\"\"Generate text asynchronously using your custom API.\"\"\"\n        # Implement asynchronous generation\n        # ...\n        return \"Generated response\"\n\n    def generate_stream(\n        self, \n        prompt: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_tokens: int = 1000,\n        previous_messages: Optional[List[Dict[str, str]]] = None,\n        **kwargs\n    ):\n        \"\"\"Stream generated text from your custom API.\"\"\"\n        # Implement synchronous streaming\n        yield \"Generated \"\n        yield \"response \"\n        yield \"in chunks\"\n\n    async def agenerate_stream(\n        self, \n        prompt: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_tokens: int = 1000,\n        previous_messages: Optional[List[Dict[str, str]]] = None,\n        **kwargs\n    ) -&gt; AsyncGenerator[str, None]:\n        \"\"\"Stream generated text asynchronously from your custom API.\"\"\"\n        # Implement asynchronous streaming\n        yield \"Generated \"\n        yield \"response \"\n        yield \"in chunks\"\n\n# Register your custom provider\nregistry = ModelRegistry.get_instance()\nregistry.register_provider(\"custom\", CustomModelProvider)\n\n# Configure your provider\nregistry.configure_provider(\"custom\", api_key=\"your-api-key\")\n\n# Register a model using your provider\nregistry.register(\n    \"custom/my-model\",\n    provider=\"custom\",\n    model_name=\"my-model-name\",\n    temperature=0.5\n)\n\n# Get your custom model\nmodel = registry.get_model(\"custom/my-model\")\n</code></pre>"},{"location":"guides/model_providers/#model-caching","title":"Model Caching","text":"<p>Improve performance with model response caching:</p> <pre><code>from lg_adk.models import ModelRegistry, get_model\nfrom lg_adk.utils.caching import enable_model_caching\n\n# Enable caching for all models\nenable_model_caching()\n\n# Or enable caching for specific models\nmodel = get_model(\"openai/gpt-4\")\nmodel.enable_caching(cache_ttl=3600)  # Cache for 1 hour\n\n# First call (will make API request)\nresponse1 = model.generate(\n    \"What is the capital of France?\",\n    system_prompt=\"You are a geography expert.\"\n)\n\n# Second call with same inputs (will use cache)\nresponse2 = model.generate(\n    \"What is the capital of France?\",\n    system_prompt=\"You are a geography expert.\"\n)\n\n# Different prompt (will make new API request)\nresponse3 = model.generate(\n    \"What is the capital of Italy?\",\n    system_prompt=\"You are a geography expert.\"\n)\n</code></pre>"},{"location":"guides/model_providers/#model-fallbacks","title":"Model Fallbacks","text":"<p>Create fallback chains for reliability:</p> <pre><code>from lg_adk.models import ModelRegistry, get_model\n\n# Create a fallback chain\nregistry = ModelRegistry.get_instance()\nregistry.register_fallback_chain(\n    \"reliable-completion\",\n    [\n        \"openai/gpt-4\",\n        \"anthropic/claude-3-sonnet\",\n        \"ollama/llama3\"  # Local fallback\n    ]\n)\n\n# Use the fallback chain\nmodel = registry.get_model(\"reliable-completion\")\nresponse = model.generate(\"Explain quantum computing.\")\n</code></pre>"},{"location":"guides/model_providers/#model-authentication-from-config-files","title":"Model Authentication from Config Files","text":"<p>Load provider configurations from a file:</p> <pre><code>from lg_adk.models import ModelRegistry\nfrom lg_adk.config import load_config\n\n# Load config from a YAML file\nconfig = load_config(\"config.yaml\")\n\n# Initialize the registry with config\nregistry = ModelRegistry.get_instance()\nregistry.configure_from_config(config.model_providers)\n\n# Now you can use get_model without explicit configuration\nmodel = get_model(\"openai/gpt-4\")\n</code></pre> <p>Example <code>config.yaml</code>:</p> <pre><code>model_providers:\n  openai:\n    api_key: ${OPENAI_API_KEY}  # Environment variable\n    organization: \"org-123\"\n  google:\n    api_key: ${GOOGLE_API_KEY}\n  anthropic:\n    api_key: ${ANTHROPIC_API_KEY}\n  ollama:\n    base_url: \"http://localhost:11434\"\n</code></pre>"},{"location":"guides/model_providers/#complete-example-multi-provider-application","title":"Complete Example: Multi-Provider Application","text":"<p>Here's a complete example using multiple model providers:</p> <pre><code>import os\nimport asyncio\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Any, Optional\nfrom lg_adk.models import get_model, ModelRegistry\n\n# Set up environment variables for API keys\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"\n\n# Model configuration\nclass ModelConfig(BaseModel):\n    provider: str\n    model_name: str\n    temperature: float = 0.7\n    max_tokens: int = 1000\n\n# Application class\nclass MultiModelApp:\n    def __init__(self):\n        self.registry = ModelRegistry.get_instance()\n\n        # Configure the OpenAI provider\n        self.registry.configure_provider(\n            \"openai\",\n            api_key=os.environ.get(\"OPENAI_API_KEY\")\n        )\n\n        # Configure the Anthropic provider\n        self.registry.configure_provider(\n            \"anthropic\",\n            api_key=os.environ.get(\"ANTHROPIC_API_KEY\")\n        )\n\n        # Configure Ollama for local models\n        self.registry.configure_provider(\n            \"ollama\",\n            base_url=\"http://localhost:11434\"\n        )\n\n        # Register models\n        self.models = {\n            \"creative\": get_model(\n                \"openai/gpt-4\",\n                temperature=0.8,\n                max_tokens=2000\n            ),\n            \"precise\": get_model(\n                \"anthropic/claude-3-opus\",\n                temperature=0.2,\n                max_tokens=1000\n            ),\n            \"fast\": get_model(\n                \"anthropic/claude-3-haiku\",\n                temperature=0.7,\n                max_tokens=500\n            ),\n            \"local\": get_model(\n                \"ollama/llama3\",\n                temperature=0.7\n            )\n        }\n\n    def generate(self, prompt: str, model_type: str) -&gt; str:\n        \"\"\"Generate using the specified model type.\"\"\"\n        if model_type not in self.models:\n            raise ValueError(f\"Unknown model type: {model_type}\")\n\n        model = self.models[model_type]\n        return model.generate(prompt)\n\n    async def generate_from_all(self, prompt: str) -&gt; Dict[str, str]:\n        \"\"\"Generate responses from all models asynchronously.\"\"\"\n        tasks = {}\n\n        for model_type, model in self.models.items():\n            tasks[model_type] = asyncio.create_task(\n                model.agenerate(prompt)\n            )\n\n        # Await all tasks\n        results = {}\n        for model_type, task in tasks.items():\n            try:\n                results[model_type] = await task\n            except Exception as e:\n                results[model_type] = f\"Error: {str(e)}\"\n\n        return results\n\n# Example usage\nasync def main():\n    app = MultiModelApp()\n\n    # Single model generation\n    creative_response = app.generate(\n        \"Write a story about a robot learning to paint.\",\n        \"creative\"\n    )\n    print(f\"Creative model response:\\n{creative_response}\\n\")\n\n    precise_response = app.generate(\n        \"Explain the process of photosynthesis.\",\n        \"precise\"\n    )\n    print(f\"Precise model response:\\n{precise_response}\\n\")\n\n    # Generate from all models\n    prompt = \"What are the ethical implications of artificial intelligence?\"\n    all_responses = await app.generate_from_all(prompt)\n\n    print(\"\\nResponses from all models:\")\n    for model_type, response in all_responses.items():\n        print(f\"\\n--- {model_type.upper()} MODEL ---\")\n        print(response[:200] + \"...\" if len(response) &gt; 200 else response)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/model_providers/#best-practices-for-working-with-model-providers","title":"Best Practices for Working with Model Providers","text":"<ol> <li>Environment Variables: Store API keys in environment variables for security</li> <li>Model Selection: Use the right model for the task (creative vs. precise)</li> <li>Fallbacks: Set up fallback chains for mission-critical applications</li> <li>Caching: Enable caching for frequently used prompts</li> <li>Provider Abstraction: Use the provider abstraction to make your code provider-agnostic</li> <li>Local Testing: Use Ollama models for local testing before deploying</li> <li>Monitoring: Track usage and performance of different providers</li> <li>Rate Limiting: Be aware of rate limits for different providers</li> <li>Cost Management: Use cheaper models for less critical tasks</li> <li>Error Handling: Implement robust error handling for API failures</li> </ol> <p>By leveraging LG-ADK's model provider system, you can build applications that are not tied to any specific model provider, allowing for flexibility, reliability, and optimal performance. </p>"},{"location":"guides/multi_agent/","title":"Multi-Agent Systems","text":"<p>LG-ADK provides powerful abstractions for building multi-agent systems using LangGraph. This guide explains how to create, configure, and use multi-agent systems with LG-ADK.</p>"},{"location":"guides/multi_agent/#what-is-a-multi-agent-system","title":"What is a Multi-Agent System?","text":"<p>A multi-agent system is a collection of specialized agents working together to accomplish tasks. Each agent has a specific role and expertise, and a coordinator agent orchestrates their interactions and workflow.</p> <p>LG-ADK's implementation simplifies the creation of multi-agent systems by handling the complex LangGraph orchestration behind the scenes.</p>"},{"location":"guides/multi_agent/#creating-a-multi-agent-system","title":"Creating a Multi-Agent System","text":""},{"location":"guides/multi_agent/#basic-structure","title":"Basic Structure","text":"<p>The basic structure of a multi-agent system in LG-ADK includes:</p> <ol> <li>A coordinator agent that manages workflow</li> <li>Specialized agents that handle specific tasks</li> <li>A <code>MultiAgentSystem</code> instance that orchestrates everything</li> </ol>"},{"location":"guides/multi_agent/#example","title":"Example","text":"<p>Here's a simple example of creating a multi-agent system:</p> <pre><code>from lg_adk import Agent, MultiAgentSystem\n\n# Create a coordinator agent\ncoordinator = Agent(\n    name=\"coordinator\",\n    llm=\"ollama/llama3\",\n    description=\"Coordinates tasks between specialized agents\"\n)\n\n# Create specialized agents\nresearcher = Agent(\n    name=\"researcher\",\n    llm=\"ollama/llama3\",\n    description=\"Researches information and provides detailed answers\"\n)\n\nsummarizer = Agent(\n    name=\"summarizer\",\n    llm=\"ollama/llama3\",\n    description=\"Summarizes information concisely\"\n)\n\n# Create the multi-agent system\nmulti_agent_system = MultiAgentSystem(\n    name=\"research_team\",\n    coordinator=coordinator,\n    agents=[researcher, summarizer],\n    description=\"A team that researches topics and creates summaries\"\n)\n\n# Run the system\nresult = multi_agent_system.run({\"input\": \"Tell me about climate change\"})\nprint(result[\"output\"])\n</code></pre>"},{"location":"guides/multi_agent/#how-it-works","title":"How It Works","text":"<p>When you run a multi-agent system, the following happens:</p> <ol> <li>The user input is sent to the coordinator agent</li> <li>The coordinator analyzes the request and decides which specialized agent(s) should handle it</li> <li>The coordinator routes the request to the appropriate agent(s)</li> <li>The specialized agent(s) process the request and return results</li> <li>The coordinator compiles the results and provides a final response</li> </ol> <p>Under the hood, LG-ADK creates a LangGraph orchestration graph that handles the message routing and state management.</p>"},{"location":"guides/multi_agent/#advanced-usage-conversation-history","title":"Advanced Usage: Conversation History","text":"<p>For multi-turn conversations, LG-ADK provides a <code>Conversation</code> class that maintains conversation history:</p> <pre><code>from lg_adk import Conversation\n\n# Create a conversation handler\nconversation = Conversation(multi_agent_system=multi_agent_system)\n\n# First user message\nresponse1 = conversation.send_message(\"Tell me about climate change\")\nprint(response1)\n\n# Follow-up question (conversation history is maintained)\nresponse2 = conversation.send_message(\"What are the main mitigation strategies?\")\nprint(response2)\n</code></pre>"},{"location":"guides/multi_agent/#customizing-agent-behavior","title":"Customizing Agent Behavior","text":"<p>Each agent in the system can be customized with:</p> <ul> <li>Different language models</li> <li>System messages</li> <li>Tools (if supported by your implementation)</li> </ul> <pre><code># Customizing an agent with a specific system message\nresearcher = Agent(\n    name=\"researcher\",\n    llm=\"ollama/llama3\",\n    description=\"Researches information and provides detailed answers\",\n    system_message=\"\"\"You are a research specialist who provides detailed,\n    accurate information. Always cite your sources and provide \n    comprehensive answers with evidence-based reasoning.\"\"\"\n)\n</code></pre>"},{"location":"guides/multi_agent/#scaling-with-multiple-agents","title":"Scaling with Multiple Agents","text":"<p>You can add any number of specialized agents to your multi-agent system:</p> <pre><code># Add agents after creation\ncritique_agent = Agent(\n    name=\"critique\",\n    llm=\"ollama/llama3\",\n    description=\"Provides critical analysis and identifies potential biases\"\n)\n\nmulti_agent_system.add_agent(critique_agent)\n\n# Or add multiple agents at once\nfact_checker = Agent(...)\nsource_finder = Agent(...)\nmulti_agent_system.add_agents([fact_checker, source_finder])\n</code></pre>"},{"location":"guides/multi_agent/#complete-example","title":"Complete Example","text":"<p>For a complete working example of a multi-agent system, see the Multi-Agent Example in the examples directory.</p>"},{"location":"guides/multi_agent/#best-practices","title":"Best Practices","text":"<ul> <li>Clear Agent Roles: Give each agent a clear and specific role</li> <li>Descriptive Names: Use descriptive names for your agents</li> <li>Coordinator Instructions: The coordinator agent works best when it has a clear understanding of all available agents</li> <li>Model Selection: Choose appropriate models for each agent based on their tasks</li> <li>Testing: Test your multi-agent system with a variety of inputs to ensure proper routing </li> </ul>"},{"location":"guides/multi_agent_communication/","title":"Multi-Agent Communication","text":"<p>This guide explains how to use LG-ADK's multi-agent communication tools to build sophisticated agent systems that can collaborate effectively.</p>"},{"location":"guides/multi_agent_communication/#overview","title":"Overview","text":"<p>LG-ADK provides two main tools for multi-agent communication:</p> <ol> <li>GroupChatTool: For facilitating conversations between multiple agents with different specialties</li> <li>AgentRouter: For routing tasks to the most appropriate agent or for sequential agent workflows</li> </ol> <p>These tools enable you to create systems where multiple agents with different capabilities can work together to solve complex problems.</p>"},{"location":"guides/multi_agent_communication/#groupchattool","title":"GroupChatTool","text":"<p>The <code>GroupChatTool</code> enables agents to have conversations with each other, similar to how humans might collaborate in a group chat.</p>"},{"location":"guides/multi_agent_communication/#basic-usage","title":"Basic Usage","text":"<pre><code>from lg_adk import Agent, get_model\nfrom lg_adk.tools.group_chat import GroupChatTool\n\n# Create specialized agents\nfinance_agent = Agent(\n    agent_name=\"FinanceExpert\",\n    system_prompt=\"You are a financial expert. Provide accurate financial advice.\",\n    llm=get_model(\"gpt-4\")\n)\n\nlegal_agent = Agent(\n    agent_name=\"LegalExpert\",\n    system_prompt=\"You are a legal expert. Provide accurate legal advice.\",\n    llm=get_model(\"gpt-4\")\n)\n\n# Create a registry of agents\nagents = {\n    \"finance\": finance_agent,\n    \"legal\": legal_agent\n}\n\n# Create the group chat tool\nchat_tool = GroupChatTool(agent_registry=agents)\n\n# Create a new chat\nchat_id = chat_tool.create_chat(\n    name=\"Financial Legal Consultation\",\n    agent_ids=[\"finance\", \"legal\"]\n)\n\n# Run a conversation\nmessages = chat_tool.run_conversation(\n    chat_id=chat_id,\n    initial_prompt=\"What are the tax implications of starting a small business?\",\n    max_turns=4  # Number of turns in the conversation\n)\n\n# Print the conversation\nfor msg in messages:\n    print(f\"{msg.agent_id}: {msg.content}\")\n</code></pre>"},{"location":"guides/multi_agent_communication/#custom-speaker-selection","title":"Custom Speaker Selection","text":"<p>By default, agents take turns speaking in a round-robin fashion. You can customize this behavior by providing a speaker selection function:</p> <pre><code>def expertise_based_selection(chat, history):\n    \"\"\"Select the next speaker based on keyword expertise.\"\"\"\n    if not history:\n        return chat.agents[0]\n\n    last_message = history[-1].content.lower()\n\n    # If last message mentions taxes, select the finance expert\n    if \"tax\" in last_message or \"finance\" in last_message:\n        return \"finance\"\n\n    # If last message mentions legal terms, select the legal expert\n    if \"legal\" in last_message or \"law\" in last_message:\n        return \"legal\"\n\n    # Default to alternating speakers\n    last_speaker_idx = chat.agents.index(history[-1].agent_id)\n    next_speaker_idx = (last_speaker_idx + 1) % len(chat.agents)\n    return chat.agents[next_speaker_idx]\n\n# Run conversation with custom speaker selection\nmessages = chat_tool.run_conversation(\n    chat_id=chat_id,\n    initial_prompt=\"What are the legal and tax implications of starting a business?\",\n    max_turns=6,\n    speaker_selection=expertise_based_selection\n)\n</code></pre>"},{"location":"guides/multi_agent_communication/#agentrouter","title":"AgentRouter","text":"<p>The <code>AgentRouter</code> allows you to route tasks to the most appropriate agent and supports different routing strategies.</p>"},{"location":"guides/multi_agent_communication/#routing-strategies","title":"Routing Strategies","text":"<ul> <li>SEQUENTIAL: Process a task through a sequence of agents, where each agent builds on the previous agent's output</li> <li>CONCURRENT: Process a task with multiple agents in parallel and combine their outputs</li> <li>SELECTOR: Select the most appropriate agent for a specific task</li> <li>MIXTURE: Get results from multiple agents and combine them into a comprehensive response</li> </ul>"},{"location":"guides/multi_agent_communication/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from lg_adk import Agent, get_model\nfrom lg_adk.tools.agent_router import AgentRouter, RouterType\n\n# Create specialized agents\nresearch_agent = Agent(\n    agent_name=\"Researcher\",\n    system_prompt=\"You are a research specialist. Find and present factual information.\",\n    llm=get_model(\"gpt-4\")\n)\n\nwriter_agent = Agent(\n    agent_name=\"Writer\",\n    system_prompt=\"You are a writing specialist. Create well-structured content.\",\n    llm=get_model(\"gpt-4\")\n)\n\neditor_agent = Agent(\n    agent_name=\"Editor\",\n    system_prompt=\"You are an editor. Improve content for clarity and correctness.\",\n    llm=get_model(\"gpt-4\")\n)\n\n# Create a sequential router (research -&gt; write -&gt; edit)\nsequential_router = AgentRouter(\n    name=\"ContentCreationPipeline\",\n    agents=[research_agent, writer_agent, editor_agent],\n    router_type=RouterType.SEQUENTIAL\n)\n\n# Run a task through the sequential pipeline\nresult = sequential_router.run(\"Explain how blockchain technology works\")\nprint(result[\"output\"])\n</code></pre>"},{"location":"guides/multi_agent_communication/#selector-router","title":"Selector Router","text":"<p>The selector router automatically chooses the best agent for a given task:</p> <pre><code>from lg_adk.tools.agent_router import RouterType\n\n# Create a selector router\nselector_router = AgentRouter(\n    name=\"ExpertSelector\",\n    agents=[research_agent, writer_agent, editor_agent],\n    router_type=RouterType.SELECTOR\n)\n\n# The router will select the most appropriate agent based on the task\nresult = selector_router.run(\"Research the latest advances in quantum computing\")\nprint(f\"Selected agent: {result.get('agent', 'Unknown')}\")\nprint(f\"Output: {result.get('output', '')}\")\n</code></pre>"},{"location":"guides/multi_agent_communication/#custom-agent-selection","title":"Custom Agent Selection","text":"<p>You can provide a custom agent selection function for the selector router:</p> <pre><code>def keyword_based_selector(task, agents):\n    \"\"\"Select an agent based on keywords in the task.\"\"\"\n    task_lower = task.lower()\n\n    if \"research\" in task_lower or \"find\" in task_lower:\n        return next(a for a in agents if a.agent_name == \"Researcher\")\n\n    if \"write\" in task_lower or \"create\" in task_lower:\n        return next(a for a in agents if a.agent_name == \"Writer\")\n\n    if \"edit\" in task_lower or \"improve\" in task_lower:\n        return next(a for a in agents if a.agent_name == \"Editor\")\n\n    # Default to the first agent\n    return agents[0]\n\n# Create a router with custom agent selection\ncustom_router = AgentRouter(\n    name=\"CustomSelector\",\n    agents=[research_agent, writer_agent, editor_agent],\n    router_type=RouterType.SELECTOR,\n    agent_selector=keyword_based_selector\n)\n\n# The router will use your custom function to select an agent\nresult = custom_router.run(\"Research the history of artificial intelligence\")\n</code></pre>"},{"location":"guides/multi_agent_communication/#combining-groupchat-and-router","title":"Combining GroupChat and Router","text":"<p>You can combine both tools for more complex agent systems:</p> <pre><code># Create a team of specialized agents\nagents = {\n    \"researcher\": research_agent,\n    \"writer\": writer_agent,\n    \"editor\": editor_agent,\n    \"fact_checker\": fact_checker_agent\n}\n\n# Create a group chat for initial brainstorming\nchat_tool = GroupChatTool(agent_registry=agents)\nchat_id = chat_tool.create_chat(\n    name=\"Content Planning\",\n    agent_ids=list(agents.keys())\n)\n\n# Run a planning conversation\nplanning_messages = chat_tool.run_conversation(\n    chat_id=chat_id,\n    initial_prompt=\"We need to create content about renewable energy. Let's plan our approach.\",\n    max_turns=8\n)\n\n# Extract the plan from the last message\nplan = planning_messages[-1].content\n\n# Create a sequential router for execution\nexecution_router = AgentRouter(\n    name=\"ContentExecution\",\n    agents=[agents[\"researcher\"], agents[\"writer\"], agents[\"editor\"], agents[\"fact_checker\"]],\n    router_type=RouterType.SEQUENTIAL\n)\n\n# Execute the plan\nfinal_content = execution_router.run(f\"Based on this plan: {plan}\\nCreate content about renewable energy\")\nprint(final_content[\"output\"])\n</code></pre>"},{"location":"guides/multi_agent_communication/#conclusion","title":"Conclusion","text":"<p>The multi-agent communication tools in LG-ADK allow you to build sophisticated agent systems where agents can collaborate effectively. Whether you need agents to discuss a problem in a group chat or process tasks in a specific sequence, these tools provide the flexibility to create the right architecture for your needs. </p>"},{"location":"guides/retrieval_augmented_generation/","title":"Retrieval-Augmented Generation (RAG)","text":"<p>This guide explains how to use LG-ADK's retrieval tools to build powerful RAG applications that enhance LLM capabilities with external knowledge.</p>"},{"location":"guides/retrieval_augmented_generation/#overview","title":"Overview","text":"<p>LG-ADK provides a set of retrieval tools that make it easy to build RAG applications:</p> <ol> <li>SimpleVectorRetrievalTool: For retrieving from vector stores (FAISS, Chroma, etc.)</li> <li>ChromaDBRetrievalTool: A specialized tool for ChromaDB</li> </ol> <p>These tools can be used with any LangChain-compatible vector store to create agents that can retrieve and reason over external knowledge.</p>"},{"location":"guides/retrieval_augmented_generation/#basic-rag-setup","title":"Basic RAG Setup","text":"<p>Here's a simple example of setting up a RAG application:</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom lg_adk import Agent, get_model\nfrom lg_adk.tools.retrieval import SimpleVectorRetrievalTool\n\n# Load environment variables\nload_dotenv()\n\n# Set up vector store (using FAISS as an example)\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.embeddings import OpenAIEmbeddings\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Load and process your documents\nloader = TextLoader(\"path/to/your/document.txt\")\ndocuments = loader.load()\n\n# Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=100\n)\nchunks = text_splitter.split_documents(documents)\n\n# Create embeddings and vector store\nembeddings = OpenAIEmbeddings()\nvector_store = FAISS.from_documents(chunks, embeddings)\n\n# Create a retrieval tool\nretrieval_tool = SimpleVectorRetrievalTool(\n    name=\"document_retrieval\",\n    description=\"Use this tool to retrieve information from the knowledge base.\",\n    vector_store=vector_store,\n    top_k=3,  # Number of documents to retrieve\n    score_threshold=0.7  # Minimum similarity score (0-1)\n)\n\n# Create a RAG agent\nrag_agent = Agent(\n    agent_name=\"KnowledgeAssistant\",\n    system_prompt=\"\"\"\n    You are a helpful assistant with access to a knowledge base.\n    When answering questions, use the retrieval tool to find relevant information.\n    Always cite where your information came from.\n    If the information is not available in the knowledge base, acknowledge that.\n    \"\"\",\n    llm=get_model(\"gpt-4\"),\n    tools=[retrieval_tool]\n)\n\n# Use the agent\nresponse = rag_agent.run({\"input\": \"What information do we have about X?\"})\nprint(response[\"output\"])\n</code></pre>"},{"location":"guides/retrieval_augmented_generation/#using-chromadb","title":"Using ChromaDB","text":"<p>ChromaDB is a popular vector database that can be used for RAG applications. LG-ADK provides a specialized tool for ChromaDB:</p> <pre><code>import chromadb\nfrom lg_adk.tools.retrieval import ChromaDBRetrievalTool\n\n# Set up ChromaDB\nchroma_client = chromadb.PersistentClient(path=\"path/to/chromadb\")\n\n# Create embedding function wrapper for ChromaDB\nclass OpenAIEmbeddingFunction:\n    def __call__(self, texts):\n        return embeddings.embed_documents(texts)\n\nembedding_function = OpenAIEmbeddingFunction()\n\n# Create a ChromaDB retrieval tool\nchromadb_retrieval = ChromaDBRetrievalTool(\n    name=\"chromadb_retrieval\",\n    description=\"Use this tool to retrieve information from the ChromaDB knowledge base.\",\n    collection_name=\"your_collection\",\n    chroma_client=chroma_client,\n    embedding_function=embedding_function,\n    top_k=5,\n    score_threshold=0.3\n)\n\n# Create a RAG agent with ChromaDB\nchromadb_agent = Agent(\n    agent_name=\"ChromaDBAssistant\",\n    system_prompt=\"You are an assistant with access to a ChromaDB knowledge base. Use the retrieval tool to find information.\",\n    llm=get_model(\"gpt-4\"),\n    tools=[chromadb_retrieval]\n)\n\n# Use the agent\nresponse = chromadb_agent.run({\"input\": \"What information do we have about project X?\"})\nprint(response[\"output\"])\n</code></pre>"},{"location":"guides/retrieval_augmented_generation/#google-adk-style-simplified-api","title":"Google ADK-Style Simplified API","text":"<p>LG-ADK allows you to create RAG applications in a style similar to Google's Agent Development Kit:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\n# Import LG-ADK components\nfrom lg_adk import Agent\nfrom lg_adk.tools.retrieval import SimpleVectorRetrievalTool\n\n# Load environment variables\nload_dotenv()\n\n# Set up vector store (assuming it's already created)\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.embeddings import OpenAIEmbeddings\n\n# System instructions\ndef return_instructions():\n    return \"\"\"\n    You are a helpful assistant with access to a knowledge base.\n    Use the retrieval tool to find information when answering questions.\n\n    When responding:\n    1. First retrieve relevant information using the tool\n    2. Then synthesize a clear and helpful response\n    3. If the information isn't in the knowledge base, say so\n\n    Always cite where you found your information.\n    \"\"\"\n\n# Create the retrieval tool\nknowledge_base_retrieval = SimpleVectorRetrievalTool(\n    name='retrieve_documents',\n    description='Use this tool to retrieve documents from the knowledge base',\n    vector_store=your_vector_store,\n    top_k=5,\n    score_threshold=0.6,\n)\n\n# Create the agent\nfrom lg_adk.models import get_model\n\nrag_agent = Agent(\n    agent_name='knowledge_base_agent',\n    system_prompt=return_instructions(),\n    llm=get_model('gpt-4'),\n    tools=[knowledge_base_retrieval]\n)\n\n# Use the agent\nresponse = rag_agent.run({\"input\": \"What does the documentation say about X?\"})\nprint(response[\"output\"])\n</code></pre>"},{"location":"guides/retrieval_augmented_generation/#advanced-rag-patterns","title":"Advanced RAG Patterns","text":""},{"location":"guides/retrieval_augmented_generation/#1-query-enhancement","title":"1. Query Enhancement","text":"<p>Enhance queries based on conversation context to improve retrieval:</p> <pre><code>from lg_adk import Agent, GraphBuilder\nfrom lg_adk.tools.retrieval import SimpleVectorRetrievalTool\n\n# Create an agent for query enhancement\nquery_enhancer = Agent(\n    agent_name=\"QueryEnhancer\",\n    system_prompt=\"Your job is to enhance user queries to improve retrieval. Based on the conversation history and current query, create a more detailed query that will help retrieve relevant information.\",\n    llm=get_model(\"gpt-4\")\n)\n\n# Create a retriever agent\nretriever_agent = Agent(\n    agent_name=\"Retriever\",\n    system_prompt=\"You search for relevant information using the retrieval tool and return it.\",\n    llm=get_model(\"gpt-4\"),\n    tools=[retrieval_tool]\n)\n\n# Create a response generator\nresponse_generator = Agent(\n    agent_name=\"ResponseGenerator\",\n    system_prompt=\"Based on the retrieved information and the original query, generate a helpful response. Always cite your sources.\",\n    llm=get_model(\"gpt-4\")\n)\n\n# Build the RAG graph\nbuilder = GraphBuilder()\nbuilder.add_agent(\"query_enhancer\", query_enhancer)\nbuilder.add_agent(\"retriever\", retriever_agent)\nbuilder.add_agent(\"response_generator\", response_generator)\n\n# Define the flow\nflow = [\n    (None, \"query_enhancer\"),\n    (\"query_enhancer\", \"retriever\"),\n    (\"retriever\", \"response_generator\"),\n    (\"response_generator\", None)\n]\n\n# Build and use the graph\nrag_graph = builder.build(flow=flow)\nresult = rag_graph.invoke({\"input\": \"Tell me about X\", \"conversation_history\": previous_messages})\nprint(result[\"output\"])\n</code></pre>"},{"location":"guides/retrieval_augmented_generation/#2-multi-source-rag","title":"2. Multi-Source RAG","text":"<p>Retrieve from multiple knowledge sources and combine the results:</p> <pre><code># Create multiple retrieval tools\nkb1_retrieval = SimpleVectorRetrievalTool(\n    name=\"kb1_retrieval\",\n    description=\"Retrieve from knowledge base 1\",\n    vector_store=kb1_store\n)\n\nkb2_retrieval = SimpleVectorRetrievalTool(\n    name=\"kb2_retrieval\",\n    description=\"Retrieve from knowledge base 2\",\n    vector_store=kb2_store\n)\n\n# Create a RAG agent with multiple retrieval tools\nmulti_source_agent = Agent(\n    agent_name=\"MultiSourceAgent\",\n    system_prompt=\"\"\"\n    You have access to multiple knowledge bases. Use the appropriate retrieval tool(s)\n    based on the question. For general questions, you may need to query multiple sources.\n    Always cite which knowledge base provided the information.\n    \"\"\",\n    llm=get_model(\"gpt-4\"),\n    tools=[kb1_retrieval, kb2_retrieval]\n)\n</code></pre>"},{"location":"guides/retrieval_augmented_generation/#3-rag-with-memory","title":"3. RAG with Memory","text":"<p>Combine RAG with memory management for persistent context:</p> <pre><code>from lg_adk import Agent, GraphBuilder\nfrom lg_adk.memory import MemoryManager\nfrom lg_adk.sessions import SessionManager\nfrom lg_adk.tools.retrieval import SimpleVectorRetrievalTool\n\n# Set up components\nmemory_manager = MemoryManager()\nsession_manager = SessionManager()\nretrieval_tool = SimpleVectorRetrievalTool(...)\n\n# Create the RAG agent\nrag_agent = Agent(\n    agent_name=\"RagWithMemory\",\n    system_prompt=\"You answer questions using the retrieval tool and conversation history.\",\n    llm=get_model(\"gpt-4\"),\n    tools=[retrieval_tool]\n)\n\n# Build the graph with memory\nbuilder = GraphBuilder()\nbuilder.add_agent(rag_agent)\nbuilder.add_memory(memory_manager)\nbuilder.enable_session_management(session_manager)\n\n# Build and use the graph\nrag_graph = builder.build()\n\n# Example of using the graph with session tracking\nsession_id = \"user123\"\nresponse = rag_graph.invoke({\n    \"input\": \"What did we discuss about X previously?\",\n    \"session_id\": session_id\n})\n</code></pre>"},{"location":"guides/retrieval_augmented_generation/#conclusion","title":"Conclusion","text":"<p>The retrieval tools in LG-ADK make it easy to build powerful RAG applications that can access external knowledge. Whether you're using FAISS, ChromaDB, or another vector store, these tools provide a consistent interface and can be combined with other LG-ADK features like memory management and multi-agent systems to create sophisticated AI applications. </p>"},{"location":"guides/session_management/","title":"Session Management in LG-ADK","text":"<p>Session management is a critical aspect of building conversational applications with LangGraph. LG-ADK provides a powerful enhanced session management system that builds on top of LangGraph's native session capabilities while adding valuable features.</p>"},{"location":"guides/session_management/#understanding-sessions-and-their-importance","title":"Understanding Sessions and Their Importance","text":"<p>Sessions are crucial for:</p> <ol> <li>Conversation Persistence: Maintaining the state and history of a conversation across multiple interactions</li> <li>Context Preservation: Keeping context and memory accessible throughout a conversation</li> <li>Memory Scoping: Ensuring memories and state are isolated between different conversations</li> <li>Multi-tenant Applications: Supporting multiple concurrent users with separate contexts</li> </ol>"},{"location":"guides/session_management/#building-on-langgraphs-native-sessions","title":"Building on LangGraph's Native Sessions","text":"<p>LG-ADK's session management enhances LangGraph's built-in session capabilities:</p> <ul> <li>Uses LangGraph's native session management for core functionality</li> <li>Adds advanced features like user association, rich metadata, and analytics</li> <li>Provides a clean API that works with both LangGraph's sessions and our enhancements</li> <li>Transparently adapts to the available LangGraph features</li> </ul>"},{"location":"guides/session_management/#using-session-ids-with-lg-adk-graphs","title":"Using Session IDs with LG-ADK Graphs","text":"<p>When interacting with LG-ADK graphs, the session ID is a key parameter:</p> <pre><code>from lg_adk.builders.graph_builder import GraphBuilder\nfrom lg_adk.sessions.session_manager import SynchronizedSessionManager\n\n# Create a session manager\nsession_manager = SynchronizedSessionManager()\n\n# Create and configure a graph builder\nbuilder = GraphBuilder(name=\"my_graph\")\nbuilder.add_agent(my_agent)\nbuilder.configure_session_management(session_manager)\n\n# Build the graph\ngraph = builder.build()\n\n# Run without session ID (new session will be created)\nresponse = builder.run(message=\"Hello!\", metadata={\"user_id\": \"alice\"})\nsession_id = response[\"session_id\"]\n\n# Continue the conversation with the same session\nfollow_up = builder.run(message=\"Tell me more\", session_id=session_id)\n</code></pre>"},{"location":"guides/session_management/#enhanced-session-manager-types","title":"Enhanced Session Manager Types","text":"<p>LG-ADK provides several session manager implementations:</p> <ul> <li><code>SessionManager</code>: Base implementation with user tracking, metadata, and analytics</li> <li><code>SynchronizedSessionManager</code>: Thread-safe implementation for production use</li> <li><code>DatabaseSessionManager</code>: Persistent implementation that stores sessions in a database</li> <li><code>AsyncSessionManager</code>: Asynchronous implementation for async/await code</li> </ul>"},{"location":"guides/session_management/#tracking-users-and-sessions","title":"Tracking Users and Sessions","text":"<p>A key feature of LG-ADK's session management is user tracking:</p> <pre><code># Create a session with user association\nsession_id = session_manager.create_session(user_id=\"alice\")\n\n# Or add user information in metadata\nresponse = builder.run(\n    message=\"Hello!\",\n    metadata={\"user_id\": \"alice\", \"device\": \"mobile\"}\n)\n\n# Get all sessions for a user\nuser_sessions = session_manager.get_user_sessions(\"alice\")\n</code></pre>"},{"location":"guides/session_management/#session-metadata-management","title":"Session Metadata Management","text":"<p>LG-ADK provides rich metadata management for sessions:</p> <pre><code># Add metadata when creating a session\nsession_id = session_manager.create_session(metadata={\"source\": \"web\", \"locale\": \"en-US\"})\n\n# Update session metadata\nsession_manager.update_session_metadata(\n    session_id, \n    {\"last_page\": \"checkout\"},\n    merge=True  # Merge with existing metadata (default)\n)\n\n# Get session metadata\nmetadata = session_manager.get_session_metadata(session_id)\n</code></pre>"},{"location":"guides/session_management/#session-analytics-and-tracking","title":"Session Analytics and Tracking","text":"<p>Track and analyze session usage with built-in analytics:</p> <pre><code># Get session object with all tracking information\nsession = session_manager.get_session(session_id)\n\n# Access session statistics\ninteraction_count = session.interactions\ntotal_tokens_in = session.total_tokens_in\ntotal_tokens_out = session.total_tokens_out\nresponse_time = session.total_response_time\nlast_active = session.last_active\n\n# Track an interaction manually (usually done automatically by GraphBuilder)\nsession_manager.track_interaction(\n    session_id,\n    tokens_in=15,     # Input token count\n    tokens_out=25,    # Output token count\n    response_time=1.2 # Response time in seconds\n)\n</code></pre>"},{"location":"guides/session_management/#session-lifecycle-management","title":"Session Lifecycle Management","text":"<p>Manage the complete lifecycle of sessions:</p> <pre><code># Create a session\nsession_id = session_manager.create_session()\n\n# Check if a session exists\nif session_manager.session_exists(session_id):\n    # Use the session\n    pass\n\n# End a session when it's no longer needed\nsession_manager.end_session(session_id)\n\n# Clear expired sessions\nexpired_count = session_manager.cleanup_expired_sessions()\n</code></pre>"},{"location":"guides/session_management/#asynchronous-session-management","title":"Asynchronous Session Management","text":"<p>LG-ADK provides full async support for session management:</p> <pre><code>from lg_adk.sessions.session_manager import AsyncSessionManager\n\n# Create an async session manager\nasync_manager = AsyncSessionManager()\n\n# Register a session asynchronously\nsession_id = await async_manager.acreate_session(user_id=\"alice\")\n\n# Track interaction asynchronously\nawait async_manager.atrack_interaction(\n    session_id,\n    tokens_in=10,\n    tokens_out=20,\n    response_time=0.5\n)\n\n# Use with graph builder's async run method\nresult = await builder.arun(message=\"Hello!\", session_id=session_id)\n</code></pre>"},{"location":"guides/session_management/#thread-safety-with-synchronizedsessionmanager","title":"Thread Safety with SynchronizedSessionManager","text":"<p>For production applications, use the thread-safe session manager:</p> <pre><code>from lg_adk.sessions.session_manager import SynchronizedSessionManager\n\n# Create a thread-safe session manager\nsession_manager = SynchronizedSessionManager()\n\n# All operations are now thread-safe and can be called from multiple threads\nsession_id = session_manager.create_session()\n</code></pre>"},{"location":"guides/session_management/#persistent-sessions-with-databasesessionmanager","title":"Persistent Sessions with DatabaseSessionManager","text":"<p>For long-running applications, use database-backed sessions:</p> <pre><code>from lg_adk.sessions.session_manager import DatabaseSessionManager\nfrom lg_adk.database.database_manager import DatabaseManager\n\n# Create a database manager\ndb_manager = DatabaseManager()\n\n# Create a database-backed session manager\nsession_manager = DatabaseSessionManager(db_manager=db_manager)\n\n# Sessions will now persist across application restarts\nsession_id = session_manager.create_session()\n</code></pre>"},{"location":"guides/session_management/#session-timeout-and-expiration","title":"Session Timeout and Expiration","text":"<p>Configure session timeouts to automatically clean up inactive sessions:</p> <pre><code># Create a session with a 30-minute timeout\nsession_id = session_manager.create_session(timeout=1800)  # 30 minutes in seconds\n\n# Or with a timedelta\nfrom datetime import timedelta\nsession_id = session_manager.create_session(timeout=timedelta(hours=1))\n\n# Clean up expired sessions periodically\nexpired_count = session_manager.cleanup_expired_sessions()\n</code></pre>"},{"location":"guides/session_management/#complete-example","title":"Complete Example","text":"<p>Here's a complete example demonstrating session management with LG-ADK:</p> <pre><code>from lg_adk.builders.graph_builder import GraphBuilder\nfrom lg_adk.agents.base import Agent\nfrom lg_adk.sessions.session_manager import SynchronizedSessionManager\nfrom lg_adk.models import get_model\n\n# Create thread-safe session manager for production use\nsession_manager = SynchronizedSessionManager()\n\n# Create an agent\nagent = Agent(\n    name=\"assistant\",\n    model=get_model(\"openai/gpt-3.5-turbo\")\n)\n\n# Create a graph builder\nbuilder = GraphBuilder(name=\"my_app\")\nbuilder.add_agent(agent)\nbuilder.configure_session_management(session_manager)\ngraph = builder.build()\n\n# User 1: Start a new conversation\nalice_metadata = {\"user_id\": \"alice\", \"device\": \"mobile\"}\nalice_response = builder.run(\n    message=\"Hello! Can you help me with my project?\",\n    metadata=alice_metadata\n)\nalice_session_id = alice_response[\"session_id\"]\n\n# User 2: Start a different conversation\nbob_metadata = {\"user_id\": \"bob\", \"device\": \"web\"}\nbob_response = builder.run(\n    message=\"What's the weather like today?\",\n    metadata=bob_metadata\n)\nbob_session_id = bob_response[\"session_id\"]\n\n# Continue Alice's conversation\nalice_followup = builder.run(\n    message=\"I need help with Python.\",\n    session_id=alice_session_id\n)\n\n# Get analytics for both users\nalice_session = session_manager.get_session(alice_session_id)\nbob_session = session_manager.get_session(bob_session_id)\n\nprint(f\"Alice's interactions: {alice_session.interactions}\")\nprint(f\"Bob's interactions: {bob_session.interactions}\")\n\n# End sessions when done\nsession_manager.end_session(alice_session_id)\nsession_manager.end_session(bob_session_id)\n</code></pre>"},{"location":"guides/session_management/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use SynchronizedSessionManager for Production: The thread-safe implementation prevents race conditions in multi-threaded environments.</p> </li> <li> <p>Associate Users with Sessions: Always associate sessions with users when possible for better tracking and analytics.</p> </li> <li> <p>Clean Up Sessions: Call <code>end_session()</code> when a conversation is complete to free resources.</p> </li> <li> <p>Use Timeouts: Configure appropriate timeouts to automatically clean up inactive sessions.</p> </li> <li> <p>Track Interactions: Use the <code>track_interaction()</code> method to record token usage and response times.</p> </li> <li> <p>Store Minimal Metadata: Only store necessary information in session metadata to avoid bloat.</p> </li> <li> <p>Leverage Analytics: Use the built-in analytics to understand user behavior and optimize your application.</p> </li> <li> <p>Use Async When Appropriate: For async applications, use the AsyncSessionManager for better performance.</p> </li> </ol> <p>By leveraging LG-ADK's enhanced session management, you can build sophisticated conversational applications that maintain context, track users, and provide rich analytics while seamlessly integrating with LangGraph's native capabilities. </p>"},{"location":"guides/tool_integration/","title":"Tool Integration in LG-ADK","text":"<p>This guide covers how to integrate and use tools with agents in the LangGraph Agent Development Kit (LG-ADK).</p>"},{"location":"guides/tool_integration/#understanding-tools-in-lg-adk","title":"Understanding Tools in LG-ADK","text":"<p>Tools give agents the ability to interact with external systems and perform actions beyond just generating text. In LG-ADK, tools:</p> <ol> <li>Extend agent capabilities by enabling interaction with systems, APIs, and data sources</li> <li>Allow agents to retrieve information or perform actions in the real world</li> <li>Can be synchronous or asynchronous</li> <li>Can be shared across multiple agents in a graph</li> </ol>"},{"location":"guides/tool_integration/#built-in-tools","title":"Built-in Tools","text":"<p>LG-ADK comes with several built-in tools ready for immediate use:</p> <pre><code>from lg_adk.tools import (\n    WebSearchTool,\n    MemoryTool,\n    DelegationTool,\n    UserInfoTool,\n    FileReaderTool,\n    FileWriterTool\n)\n</code></pre>"},{"location":"guides/tool_integration/#web-search-tool","title":"Web Search Tool","text":"<p>Allows agents to search the internet for information:</p> <pre><code>from lg_adk.agents import Agent\nfrom lg_adk.models import get_model\nfrom lg_adk.tools import WebSearchTool\n\nagent = Agent(\n    name=\"researcher\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a research assistant that finds information.\",\n    tools=[WebSearchTool()]\n)\n\n# The agent can now use the web search tool\nresult = agent.run(\"What are the latest developments in quantum computing?\")\n</code></pre>"},{"location":"guides/tool_integration/#memory-tool","title":"Memory Tool","text":"<p>Enables agents to store and retrieve information:</p> <pre><code>from lg_adk.tools import MemoryTool\nfrom lg_adk.memory import MemoryManager\nfrom lg_adk.database import DatabaseManager\n\n# Create memory manager\nmemory_manager = MemoryManager(\n    database_manager=DatabaseManager(connection_string=\"sqlite:///agent_memory.db\")\n)\n\n# Create memory tool\nmemory_tool = MemoryTool(memory_manager=memory_manager)\n\n# Add to agent\nagent = Agent(\n    name=\"memory_agent\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are an assistant that remembers important information.\",\n    tools=[memory_tool]\n)\n</code></pre>"},{"location":"guides/tool_integration/#delegation-tool","title":"Delegation Tool","text":"<p>Allows one agent to delegate tasks to another:</p> <pre><code>from lg_adk.tools import DelegationTool\n\n# Create a specialized math agent\nmath_agent = Agent(\n    name=\"math_expert\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a mathematics expert who can solve complex problems.\"\n)\n\n# Create the delegation tool pointing to the math agent\ndelegation_tool = DelegationTool(target_agent=math_agent)\n\n# Add to the main agent\nprimary_agent = Agent(\n    name=\"coordinator\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You coordinate and delegate specialized tasks when needed.\",\n    tools=[delegation_tool]\n)\n</code></pre>"},{"location":"guides/tool_integration/#creating-custom-tools","title":"Creating Custom Tools","text":"<p>You can create custom tools by extending the <code>BaseTool</code> class:</p> <pre><code>from typing import Dict, Any, Optional\nfrom lg_adk.tools import BaseTool\n\nclass WeatherTool(BaseTool):\n    \"\"\"Tool for getting current weather conditions.\"\"\"\n\n    name: str = \"weather_tool\"\n    description: str = \"Get weather information for a specified location.\"\n\n    def _run(self, location: str) -&gt; Dict[str, Any]:\n        \"\"\"Get weather for a location.\"\"\"\n        # In a real implementation, you would call a weather API\n        # This is a simplified example\n        weather_data = self._call_weather_api(location)\n        return {\n            \"temperature\": weather_data[\"temp\"],\n            \"conditions\": weather_data[\"conditions\"],\n            \"location\": location\n        }\n\n    async def _arun(self, location: str) -&gt; Dict[str, Any]:\n        \"\"\"Async version of _run.\"\"\"\n        # Implement async API call here\n        return await self._async_call_weather_api(location)\n\n    def _call_weather_api(self, location: str) -&gt; Dict[str, Any]:\n        \"\"\"Mock API call to weather service.\"\"\"\n        # In a real implementation, this would be an actual API call\n        return {\"temp\": \"72\u00b0F\", \"conditions\": \"Sunny\"}\n\n    async def _async_call_weather_api(self, location: str) -&gt; Dict[str, Any]:\n        \"\"\"Mock async API call to weather service.\"\"\"\n        # In a real implementation, this would be an actual async API call\n        return {\"temp\": \"72\u00b0F\", \"conditions\": \"Sunny\"}\n</code></pre>"},{"location":"guides/tool_integration/#tool-parameters-and-schema","title":"Tool Parameters and Schema","text":"<p>Define the expected input parameters for a tool:</p> <pre><code>from typing import Dict, Any, List, Optional\nfrom pydantic import BaseModel, Field\nfrom lg_adk.tools import BaseTool\n\nclass StockLookupParams(BaseModel):\n    \"\"\"Parameters for stock lookup tool.\"\"\"\n    symbol: str = Field(..., description=\"Stock ticker symbol (e.g., AAPL)\")\n    timeframe: str = Field(\"1d\", description=\"Timeframe for stock data (1d, 5d, 1m, etc.)\")\n\nclass StockLookupTool(BaseTool):\n    \"\"\"Tool for looking up stock information.\"\"\"\n\n    name: str = \"stock_lookup\"\n    description: str = \"Get stock price and information for a ticker symbol.\"\n    parameters_schema: type = StockLookupParams\n\n    def _run(self, symbol: str, timeframe: str = \"1d\") -&gt; Dict[str, Any]:\n        \"\"\"Look up stock information.\"\"\"\n        # In a real implementation, call a financial API\n        return {\n            \"symbol\": symbol,\n            \"price\": \"150.00\",\n            \"change\": \"+2.5%\",\n            \"timeframe\": timeframe\n        }\n</code></pre>"},{"location":"guides/tool_integration/#tool-results-schema","title":"Tool Results Schema","text":"<p>Define the expected output from a tool:</p> <pre><code>from typing import Dict, Any, List, Optional\nfrom pydantic import BaseModel, Field\nfrom lg_adk.tools import BaseTool\n\nclass WeatherResult(BaseModel):\n    \"\"\"Weather data result schema.\"\"\"\n    temperature: str = Field(..., description=\"Current temperature\")\n    conditions: str = Field(..., description=\"Weather conditions (Sunny, Rainy, etc.)\")\n    humidity: Optional[str] = Field(None, description=\"Current humidity percentage\")\n    wind_speed: Optional[str] = Field(None, description=\"Current wind speed\")\n\nclass DetailedWeatherTool(BaseTool):\n    \"\"\"Tool for getting detailed weather information.\"\"\"\n\n    name: str = \"detailed_weather\"\n    description: str = \"Get comprehensive weather data for a location.\"\n    result_schema: type = WeatherResult\n\n    def _run(self, location: str) -&gt; Dict[str, Any]:\n        \"\"\"Get detailed weather for a location.\"\"\"\n        # Implementation would call a weather API\n        return {\n            \"temperature\": \"72\u00b0F\",\n            \"conditions\": \"Sunny\",\n            \"humidity\": \"45%\",\n            \"wind_speed\": \"5 mph\"\n        }\n</code></pre>"},{"location":"guides/tool_integration/#stateful-tools","title":"Stateful Tools","text":"<p>Create tools that maintain state between invocations:</p> <pre><code>from typing import Dict, Any, List, Optional\nfrom lg_adk.tools import BaseTool\n\nclass ConversationTrackingTool(BaseTool):\n    \"\"\"Tool that tracks conversation history.\"\"\"\n\n    name: str = \"conversation_tracker\"\n    description: str = \"Track and analyze conversation patterns.\"\n\n    def __init__(self):\n        super().__init__()\n        self.conversation_history = []\n\n    def _run(self, message: str, sender: str) -&gt; Dict[str, Any]:\n        \"\"\"Track a message in the conversation.\"\"\"\n        self.conversation_history.append({\n            \"message\": message,\n            \"sender\": sender,\n            \"timestamp\": self._get_current_time()\n        })\n\n        return {\n            \"history_length\": len(self.conversation_history),\n            \"summary\": self._analyze_conversation()\n        }\n\n    def _analyze_conversation(self) -&gt; str:\n        \"\"\"Analyze conversation patterns.\"\"\"\n        # Implement conversation analysis logic\n        return f\"Conversation has {len(self.conversation_history)} messages.\"\n\n    def _get_current_time(self) -&gt; str:\n        \"\"\"Get current time string.\"\"\"\n        from datetime import datetime\n        return datetime.now().isoformat()\n</code></pre>"},{"location":"guides/tool_integration/#error-handling-in-tools","title":"Error Handling in Tools","text":"<p>Implement robust error handling:</p> <pre><code>from typing import Dict, Any, Optional\nfrom lg_adk.tools import BaseTool\n\nclass DatabaseQueryTool(BaseTool):\n    \"\"\"Tool for querying a database.\"\"\"\n\n    name: str = \"database_query\"\n    description: str = \"Query a database for information.\"\n\n    def _run(self, query: str) -&gt; Dict[str, Any]:\n        \"\"\"Execute a database query.\"\"\"\n        try:\n            # Attempt to execute the query\n            result = self._execute_query(query)\n            return {\n                \"success\": True,\n                \"results\": result,\n                \"row_count\": len(result)\n            }\n        except Exception as e:\n            # Handle the error gracefully\n            return {\n                \"success\": False,\n                \"error\": str(e),\n                \"error_type\": type(e).__name__\n            }\n\n    def _execute_query(self, query: str) -&gt; List[Dict[str, Any]]:\n        \"\"\"Execute the actual database query.\"\"\"\n        # In a real implementation, this would query a database\n        # For demonstration, we'll return mock data\n        if \"invalid\" in query.lower():\n            raise ValueError(\"Invalid SQL query syntax\")\n        return [{\"id\": 1, \"name\": \"Example\"}, {\"id\": 2, \"name\": \"Test\"}]\n</code></pre>"},{"location":"guides/tool_integration/#combining-multiple-tools","title":"Combining Multiple Tools","text":"<p>Add multiple tools to an agent:</p> <pre><code>from lg_adk.agents import Agent\nfrom lg_adk.models import get_model\nfrom lg_adk.tools import WebSearchTool, MemoryTool, FileReaderTool\n\n# Create agent with multiple tools\nassistant = Agent(\n    name=\"multi_tool_assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a helpful assistant with multiple capabilities.\",\n    tools=[\n        WebSearchTool(),\n        MemoryTool(memory_manager=memory_manager),\n        FileReaderTool()\n    ]\n)\n\n# The agent can now use any of these tools\nresult = assistant.run(\"Search for information about climate change, save the important points to memory, and then read the local report.txt file.\")\n</code></pre>"},{"location":"guides/tool_integration/#tool-security-best-practices","title":"Tool Security Best Practices","text":"<p>When implementing tools, follow these security best practices:</p> <ol> <li>Validate Inputs: Always validate inputs to prevent injection attacks.</li> <li>Limit Permissions: Give tools only the minimum permissions needed for their function.</li> <li>Avoid Sensitive Data: Never expose API keys or credentials directly in the tool code.</li> <li>Rate Limiting: Implement rate limiting to prevent abuse of external APIs.</li> <li>Sanitize Outputs: Clean tool outputs before returning them to the agent.</li> <li>Audit Tool Usage: Log tool invocations for security auditing.</li> </ol>"},{"location":"guides/tool_integration/#complete-example-research-assistant-with-tools","title":"Complete Example: Research Assistant with Tools","text":"<p>Here's a complete example of an agent with multiple tools:</p> <pre><code>from lg_adk.agents import Agent\nfrom lg_adk.models import get_model\nfrom lg_adk.tools import WebSearchTool, MemoryTool, FileWriterTool\nfrom lg_adk.memory import MemoryManager\nfrom lg_adk.database import DatabaseManager\n\n# Setup memory manager\nmemory_manager = MemoryManager(\n    database_manager=DatabaseManager(connection_string=\"sqlite:///research_assistant.db\")\n)\n\n# Create tools\nweb_search = WebSearchTool()\nmemory_tool = MemoryTool(memory_manager=memory_manager)\nfile_writer = FileWriterTool()\n\n# Create the research assistant agent\nresearcher = Agent(\n    name=\"research_assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a research assistant that helps users find information.\n    Use your web search tool to find information on the internet.\n    Store important findings in memory for later retrieval.\n    Write comprehensive reports to files when requested.\"\"\",\n    tools=[web_search, memory_tool, file_writer]\n)\n\n# Run the agent with a research task\nresult = researcher.run(\"\"\"\nResearch the latest advancements in renewable energy storage technologies.\nSave important findings to memory and create a comprehensive report in a file called 'renewable_energy_report.txt'.\n\"\"\")\n\nprint(result)\n</code></pre> <p>By following this guide, you can effectively integrate tools into your agents, enhancing their capabilities and enabling them to perform a wide range of tasks beyond simple text generation. For more information on specific components, refer to the Creating Agents, Building Graphs, and Memory Management guides. </p>"}]}