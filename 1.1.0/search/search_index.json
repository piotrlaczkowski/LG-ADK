{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udc4b Welcome to LG-ADK: LangGraph Agent Development Kit \ud83d\ude80","text":"<p>Build next-generation AI agents and workflows with LangGraph, made easy!</p> <p>LG-ADK is a Python framework for rapidly building, composing, and deploying powerful agent systems using LangGraph. Inspired by Google's ADK, but supercharged for the open-source ecosystem.</p>"},{"location":"#why-lg-adk","title":"\u2728 Why LG-ADK?","text":"<ul> <li>\ud83e\udd16 Modular Agent Architecture: Easily define and customize agents with different capabilities</li> <li>\ud83d\udd17 Flexible Graph Construction: Build complex agent workflows using LangGraph's powerful graph-based approach</li> <li>\ud83e\udde0 Memory Management: Built-in support for short-term and long-term memory</li> <li>\ud83d\uddc2\ufe0f Session Management: Handle conversations and maintain context across interactions</li> <li>\ud83e\uddd1\u200d\ud83d\udcbb Human-in-the-Loop: Seamlessly integrate human feedback and intervention</li> <li>\ud83d\udee0\ufe0f Tool Integration: Easily connect agents to external tools and APIs</li> <li>\ud83d\udda5\ufe0f Local Model Support: Run with Ollama or Gemini for privacy and cost savings</li> <li>\ud83e\uddec Morphik Integration: Advanced document/graph retrieval and RAG workflows</li> <li>\ud83d\uddbc\ufe0f Visual Debugging: Inspect and debug agent workflows with langgraph-cli</li> <li>\ud83d\uddc4\ufe0f Database Flexibility: Use various databases (local or PostgreSQL) for storage</li> <li>\ud83e\uddec Vector Store Integration: Works with different vector stores for semantic search</li> </ul>"},{"location":"#quick-install","title":"\ud83d\udce6 Quick Install","text":"<p>Install with pip or Poetry</p> <pre><code>pip install lg-adk\n# or\npoetry add lg-adk\n</code></pre>"},{"location":"#basic-usage","title":"\u26a1 Basic Usage","text":"<pre><code>from lg_adk import Agent, GraphBuilder\nfrom lg_adk.memory import MemoryManager\nfrom lg_adk.tools import WebSearchTool\n\n# Create an agent\nagent = Agent(\n    name=\"assistant\",\n    llm=\"ollama/llama3\",  # Or use \"gemini/gemini-pro\"\n    description=\"A helpful AI assistant\"\n)\n\n# Add a tool\nagent.add_tool(WebSearchTool())\n\n# Create a graph\nbuilder = GraphBuilder()\nbuilder.add_agent(agent)\n\n# Build and run\ngraph = builder.build()\nresponse = graph.invoke({\"input\": \"Hello, how can you help me today?\"})\nprint(response[\"output\"])\n</code></pre>"},{"location":"#quick-links","title":"\ud83d\udea6 Quick Links","text":"<ul> <li>Getting Started \ud83d\udea6</li> <li>Guides \ud83d\udcda</li> <li>Examples \ud83d\udca1</li> <li>API Reference \ud83d\udee0\ufe0f</li> <li>Morphik Example \ud83e\uddec</li> </ul>"},{"location":"#license","title":"\ud83d\udcdd License","text":"<p>MIT License</p>"},{"location":"#acknowledgements","title":"\ud83d\ude4f Acknowledgements","text":"<p>This project is inspired by Google's Agent Development Kit and built on top of LangGraph by LangChain.</p> Made with \u2764\ufe0f by the LG-ADK community. GitHub \u00b7 PyPI"},{"location":"examples/","title":"\ud83e\uddd1\u200d\ud83d\udcbb LG-ADK Examples Index","text":"<p>Welcome to the LG-ADK examples gallery! Explore runnable, well-documented examples for every major feature of the framework. Click any example to view the code and instructions.</p>"},{"location":"examples/#retrieval-augmented-generation-rag","title":"\ud83d\udcda Retrieval-Augmented Generation (RAG)","text":"<ul> <li>Basic RAG Example \u2014 Build a simple RAG system with vector search and response generation.</li> <li>Simple RAG (FAISS &amp; ChromaDB) \u2014 RAG agent using both FAISS and ChromaDB vector stores.</li> </ul>"},{"location":"examples/#multi-agent-systems","title":"\ud83d\udc65 Multi-Agent Systems","text":"<ul> <li>Multi-Agent System \u2014 Coordinator agent delegates tasks to specialized agents.</li> <li>Multi-Agent Chat \u2014 Group chat and agent router collaboration.</li> </ul>"},{"location":"examples/#chat-graphs","title":"\ud83d\udcac Chat &amp; Graphs","text":"<ul> <li>Simple Chat \u2014 Basic chat agent with memory and session management.</li> <li>Graph Builder Example \u2014 Build a graph with sequential agent processing.</li> </ul>"},{"location":"examples/#session-management","title":"\ud83d\uddc2\ufe0f Session Management","text":"<ul> <li>Session Management Example \u2014 Enhanced session management, user tracking, and analytics.</li> <li>Enhanced Session Management \u2014 Multi-user conversations with rich analytics.</li> </ul>"},{"location":"examples/#evaluation","title":"\ud83e\uddea Evaluation","text":"<ul> <li>Agent Evaluation \u2014 Evaluate an agent using LG-ADK's evaluation tools.</li> </ul>"},{"location":"examples/#how-to-use","title":"\ud83c\udf1f How to Use","text":"<ul> <li>Click any example to view the code and instructions.</li> <li>All examples are runnable scripts\u2014copy, adapt, and experiment!</li> <li>See the Getting Started guide for setup instructions.</li> </ul>"},{"location":"examples/#related-guides","title":"\ud83d\udd17 Related Guides","text":"<ul> <li>RAG Guide \ud83d\udcda</li> <li>Multi-Agent Guide \ud83d\udc65</li> <li>Session Management Guide \ud83d\uddc2\ufe0f</li> <li>Evaluation Guide \ud83e\uddea</li> </ul>"},{"location":"examples/enhanced_session_management/","title":"\ud83e\uddd1\u200d\ud83d\udcbb Enhanced Session Management Example with LG-ADK","text":"<p>This example demonstrates how to use the enhanced session management features of LG-ADK to build a multi-user conversational application with rich analytics and user tracking.</p> <p>You can copy and run this example as a script.</p> <pre><code>import os\nimport time\nimport uuid\nfrom datetime import datetime, timedelta\nfrom typing import Any, Dict, List, Optional\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom lg_adk.agents.base import Agent\nfrom lg_adk.builders.graph_builder import GraphBuilder\nfrom lg_adk.models import get_model\nfrom lg_adk.sessions.session_manager import SynchronizedSessionManager\n\nclass SimpleAgent(Agent):\n    \"\"\"A simple agent that processes user queries and generates responses.\"\"\"\n\n    def __init__(self, name: str, model_name: str = \"gpt-3.5-turbo\"):\n        super().__init__(name=name)\n        self.model = ChatOpenAI(model=model_name, temperature=0.7)\n        self.prompt = ChatPromptTemplate.from_messages(\n            [\n                (\"system\", \"You are a helpful assistant that provides concise, accurate responses.\"),\n                (\"human\", \"{input}\"),\n                (\"ai\", \"{agent_scratchpad}\"),\n            ]\n        )\n\n    def __call__(self, state: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Process the user input and generate a response.\"\"\"\n        # Format messages for the model\n        messages = self.prompt.format_messages(input=state[\"input\"], agent_scratchpad=state.get(\"agent_scratchpad\", \"\"))\n\n        # Generate response\n        response = self.model.invoke(messages).content\n\n        # Update state with user input and assistant response\n        state[\"agent_scratchpad\"] = response\n        state[\"output\"] = response\n        state[\"conversation_history\"] = state.get(\"conversation_history\", []) + [\n            {\"role\": \"user\", \"content\": state[\"input\"]},\n            {\"role\": \"assistant\", \"content\": response},\n        ]\n\n        return state\n\n\ndef get_session_summary(session_manager, session_id):\n    \"\"\"Get a summary of session information.\"\"\"\n    try:\n        session = session_manager.get_session(session_id)\n        metadata = session_manager.get_session_metadata(session_id)\n\n        # Calculate duration\n        created_at = metadata.get(\"created_at\", datetime.now())\n        duration = (session.last_active - created_at).total_seconds()\n\n        # Calculate average response time\n        avg_response_time = 0\n        if session.interactions &gt; 0:\n            avg_response_time = session.total_response_time / session.interactions\n\n        return {\n            \"user_id\": metadata.get(\"user_id\", \"unknown\"),\n            \"interactions\": session.interactions,\n            \"total_tokens_in\": session.total_tokens_in,\n            \"total_tokens_out\": session.total_tokens_out,\n            \"avg_response_time\": avg_response_time,\n            \"duration\": duration,\n            \"last_active\": session.last_active,\n            \"created_at\": created_at,\n            \"metadata\": metadata,\n        }\n    except KeyError:\n        return None\n\n\ndef main():\n    \"\"\"Run the enhanced session management example.\"\"\"\n    # Set up OpenAI API key (replace with your key or use environment variable)\n    os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"your-api-key\")\n\n    # Create a thread-safe enhanced session manager\n    session_manager = SynchronizedSessionManager()\n\n    # Create a simple agent\n    agent = SimpleAgent(name=\"assistant\", model_name=\"gpt-3.5-turbo\")\n\n    # Create a graph builder with session management\n    builder = GraphBuilder(name=\"conversational_assistant\")\n    builder.add_agent(agent)\n    builder.configure_session_management(session_manager)\n\n    # Build the graph\n    graph = builder.build()\n\n    print(\"Enhanced Session Management Example\")\n    print(\"-----------------------------------\")\n\n    # User 1: Alice's conversation\n    print(\"\\n\ud83e\uddd1\u200d\ud83d\udcbb Alice starts a conversation:\")\n    alice_metadata = {\"user_id\": \"alice\", \"device\": \"laptop\", \"location\": \"New York\", \"language\": \"en-US\"}\n\n    # Start Alice's session with metadata\n    alice_session = session_manager.create_session(user_id=\"alice\", metadata=alice_metadata)\n    print(f\"  Session created for Alice: {alice_session}\")\n\n    # Alice sends a message\n    start_time = time.time()\n    alice_message = \"Hello! Can you tell me about machine learning?\"\n    alice_response = builder.run(message=alice_message, session_id=alice_session)\n    response_time = time.time() - start_time\n\n    # Track interaction metrics\n    session_manager.track_interaction(\n        alice_session,\n        tokens_in=len(alice_message) // 4,  # Rough estimate\n        tokens_out=len(alice_response[\"output\"]) // 4,  # Rough estimate\n        response_time=response_time,\n    )\n\n    print(f\"  Alice: {alice_message}\")\n    print(f\"  Assistant: {alice_response['output']}\")\n    print(f\"  Response time: {response_time:.2f}s\")\n\n    # User 2: Bob's conversation\n    print(\"\\n\ud83e\uddd1\u200d\ud83d\udcbb Bob starts a conversation:\")\n    bob_metadata = {\"user_id\": \"bob\", \"device\": \"mobile\", \"location\": \"London\", \"language\": \"en-GB\"}\n\n    # Start Bob's session with run (automatic session creation)\n    bob_message = \"Hi there! What's the weather like today?\"\n    start_time = time.time()\n    bob_response = builder.run(message=bob_message, metadata=bob_metadata)\n    response_time = time.time() - start_time\n\n    bob_session = bob_response[\"session_id\"]\n\n    # Track interaction metrics\n    session_manager.track_interaction(\n        bob_session,\n        tokens_in=len(bob_message) // 4,  # Rough estimate\n        tokens_out=len(bob_response[\"output\"]) // 4,  # Rough estimate\n        response_time=response_time,\n    )\n\n    print(f\"  Session created for Bob: {bob_session}\")\n    print(f\"  Bob: {bob_message}\")\n    print(f\"  Assistant: {bob_response['output']}\")\n    print(f\"  Response time: {response_time:.2f}s\")\n\n    # Alice continues her conversation\n    print(\"\\n\ud83e\uddd1\u200d\ud83d\udcbb Alice continues her conversation:\")\n    alice_followup = \"Can you explain neural networks specifically?\"\n    start_time = time.time()\n    alice_response = builder.run(message=alice_followup, session_id=alice_session)\n    response_time = time.time() - start_time\n\n    # Track interaction metrics\n    session_manager.track_interaction(\n        alice_session,\n        tokens_in=len(alice_followup) // 4,\n        tokens_out=len(alice_response[\"output\"]) // 4,\n        response_time=response_time,\n    )\n\n    print(f\"  Alice: {alice_followup}\")\n    print(f\"  Assistant: {alice_response['output']}\")\n    print(f\"  Response time: {response_time:.2f}s\")\n\n    # Bob continues his conversation\n    print(\"\\n\ud83e\uddd1\u200d\ud83d\udcbb Bob continues his conversation:\")\n    bob_followup = \"And what about tomorrow's forecast?\"\n    start_time = time.time()\n    bob_response = builder.run(message=bob_followup, session_id=bob_session)\n    response_time = time.time() - start_time\n\n    # Track interaction metrics\n    session_manager.track_interaction(\n        bob_session,\n        tokens_in=len(bob_followup) // 4,\n        tokens_out=len(bob_response[\"output\"]) // 4,\n        response_time=response_time,\n    )\n\n    print(f\"  Bob: {bob_followup}\")\n    print(f\"  Assistant: {bob_response['output']}\")\n    print(f\"  Response time: {response_time:.2f}s\")\n\n    # Demonstrate session tracking features\n    print(\"\\n\ud83d\udcca Session Analytics and Tracking:\")\n\n    # Get all of Alice's sessions\n    alice_sessions = session_manager.get_user_sessions(\"alice\")\n    print(f\"  Alice's active sessions: {len(alice_sessions)}\")\n\n    # Get Alice's session statistics\n    alice_summary = get_session_summary(session_manager, alice_session)\n    if alice_summary:\n        print(f\"  Alice's session statistics:\")\n        print(f\"    - Interactions: {alice_summary['interactions']}\")\n        print(f\"    - Total tokens in: {alice_summary['total_tokens_in']}\")\n        print(f\"    - Total tokens out: {alice_summary['total_tokens_out']}\")\n        print(f\"    - Avg response time: {alice_summary['avg_response_time']:.2f}s\")\n        print(f\"    - Session duration: {alice_summary['duration']:.2f}s\")\n\n    # Update session metadata\n    session_manager.update_session_metadata(\n        alice_session, {\"subscription\": \"premium\", \"last_topic\": \"neural networks\"}, merge=True\n    )\n\n    # Get updated metadata\n    alice_metadata = session_manager.get_session_metadata(alice_session)\n    print(f\"  Alice's updated metadata: {alice_metadata}\")\n\n    # End sessions\n    print(\"\\n\ud83d\udd1a Ending sessions:\")\n    session_manager.end_session(alice_session)\n    session_manager.end_session(bob_session)\n    print(f\"  Ended Alice's session: {alice_session}\")\n    print(f\"  Ended Bob's session: {bob_session}\")\n\n    # Verify sessions are gone\n    print(f\"  Alice's session exists: {session_manager.session_exists(alice_session)}\")\n    print(f\"  Bob's session exists: {session_manager.session_exists(bob_session)}\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/evaluation_example/","title":"\ud83e\uddea Agent Evaluation Example with LG-ADK","text":"<p>This example demonstrates how to evaluate an agent using the evaluation tools provided by LG-ADK.</p> <p>You can copy and run this example as a script.</p> <pre><code>import json\nimport os\n\nfrom lg_adk import Agent, EvalDataset, Evaluator\nfrom lg_adk.eval.metrics import AccuracyMetric, LatencyMetric\n\n# Create a simple agent to evaluate\nagent = Agent(name=\"qa_agent\", llm=\"ollama/llama3\", description=\"Answers factual questions\")\n\n# Create or load an evaluation dataset\n# First, define a dataset\ndataset_data = {\n    \"name\": \"General Knowledge Questions\",\n    \"description\": \"Basic factual questions to test agent knowledge\",\n    \"examples\": [\n        {\"id\": \"q1\", \"input\": \"What is the capital of France?\", \"expected_output\": \"The capital of France is Paris.\"},\n        {\n            \"id\": \"q2\",\n            \"input\": \"Who wrote the play Romeo and Juliet?\",\n            \"expected_output\": \"William Shakespeare wrote Romeo and Juliet.\",\n        },\n        {\n            \"id\": \"q3\",\n            \"input\": \"What is the largest planet in our solar system?\",\n            \"expected_output\": \"Jupiter is the largest planet in our solar system.\",\n        },\n        {\n            \"id\": \"q4\",\n            \"input\": \"What is the chemical symbol for gold?\",\n            \"expected_output\": \"The chemical symbol for gold is Au.\",\n        },\n        {\n            \"id\": \"q5\",\n            \"input\": \"Who painted the Mona Lisa?\",\n            \"expected_output\": \"Leonardo da Vinci painted the Mona Lisa.\",\n        },\n    ],\n}\n\n# Save the dataset to a file\ndataset_path = \"general_knowledge_dataset.json\"\nwith open(dataset_path, \"w\") as f:\n    json.dump(dataset_data, f, indent=2)\n\n# Load the dataset\ndataset = EvalDataset.from_json(dataset_path)\n\n# Create metrics for evaluation\nmetrics = [\n    AccuracyMetric(),  # Measures how accurate the answers are\n    LatencyMetric(),  # Measures response time\n]\n\n# Create an evaluator\nevaluator = Evaluator(metrics=metrics)\n\n# Run the evaluation\nprint(\"Starting evaluation...\")\nresults = evaluator.evaluate(agent, dataset)\n\n# Save the results\nresults_path = \"evaluation_results.json\"\nevaluator.save_results(results, results_path)\n\nprint(f\"\\nEvaluation complete. Results saved to {results_path}\")\nprint(f\"Accuracy: {results.metric_scores.get('AccuracyMetric', 0):.4f}\")\nprint(f\"Average Latency: {results.metric_scores.get('LatencyMetric', 0):.4f} seconds\")\n\n# Clean up the dataset file if needed\n# os.remove(dataset_path)\n</code></pre>"},{"location":"examples/graph_example/","title":"\ud83c\udfd7\ufe0f Graph Builder Example with LG-ADK","text":"<p>This example demonstrates how to create a graph using the <code>GraphBuilder</code>, with two agents that process the input sequentially.</p> <p>You can copy and run this example as a script.</p> <pre><code>from lg_adk import Agent, GraphBuilder\nfrom lg_adk.memory import MemoryManager\n\n# Create agents for different stages of processing\nanalyzer = Agent(\n    name=\"analyzer\", llm=\"ollama/llama3\", description=\"Analyzes the input and breaks it down into components\"\n)\n\nresponder = Agent(name=\"responder\", llm=\"ollama/llama3\", description=\"Generates a final response based on the analysis\")\n\n# Create a graph builder\nbuilder = GraphBuilder()\n\n# Add the agents to the graph\nbuilder.add_agent(analyzer)\nbuilder.add_agent(responder)\n\n# Add memory manager for keeping track of conversation history\nmemory_manager = MemoryManager()\nbuilder.add_memory(memory_manager)\n\n# Enable human-in-the-loop for interactive correction if needed\nbuilder.enable_human_in_loop()\n\n# Build the graph with a specific flow: analyzer -&gt; responder\ngraph = builder.build(\n    flow=[\n        (\"analyzer\", \"responder\"),  # analyzer output goes to responder\n        (\"responder\", None),  # responder output is the final output\n    ]\n)\n\n# Run the graph interactively\nif __name__ == \"__main__\":\n    print(\"Graph Example\")\n    print(\"=============\")\n    print(\"Type 'exit' to quit.\\n\")\n\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() in [\"exit\", \"quit\", \"q\"]:\n            break\n\n        # Run the graph\n        result = graph.invoke({\"input\": user_input})\n\n        # Print the response\n        print(f\"\\nSystem: {result.get('output', '')}\\n\")\n</code></pre>"},{"location":"examples/model_provider_example/","title":"Using Multiple Model Providers","text":"<p>This example demonstrates how to use LG-ADK's model provider system to create an application that can switch between different language model providers.</p>"},{"location":"examples/model_provider_example/#multi-provider-agent","title":"Multi-Provider Agent","text":"<p>In this example, we'll create a simple agent that can use different model providers (OpenAI, Google, Anthropic, or Ollama) based on user preferences:</p> <pre><code>import os\nfrom typing import Dict, Optional\nfrom lg_adk import Agent, GraphBuilder\nfrom lg_adk.models import get_model, ModelRegistry\nfrom lg_adk.human import HumanInputTool\nfrom lg_adk.tools import WebSearchTool\nfrom lg_adk.memory import MemoryManager\n\n# Set up API keys (in a real application, these would be in environment variables)\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"  # Replace with your actual key\nos.environ[\"GOOGLE_API_KEY\"] = \"your-google-key\"  # Replace with your actual key\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"  # Replace with your actual key\n\nclass ModelSwitchingAgent:\n    \"\"\"An agent that can switch between different model providers.\"\"\"\n\n    def __init__(self):\n        # Available models configuration\n        self.models = {\n            \"openai\": \"openai/gpt-4\",\n            \"google\": \"google/gemini-pro\",\n            \"anthropic\": \"anthropic/claude-3-sonnet\",\n            \"ollama\": \"ollama/llama3\"  # Local model\n        }\n\n        # Start with OpenAI as default\n        self.current_model = \"openai\"\n\n        # Initialize the agent with the default model\n        self.setup_agent()\n\n    def setup_agent(self):\n        \"\"\"Set up the agent with the current model.\"\"\"\n        model = get_model(self.models[self.current_model])\n\n        # Create a new agent with the selected model\n        self.agent = Agent(\n            agent_name=\"model_switcher\",\n            llm=model,\n            system_prompt=(\n                \"You are a helpful assistant powered by a language model. \"\n                \"You can search the web to answer questions and can switch between \"\n                \"different model providers based on user requests.\"\n            )\n        )\n\n        # Add tools\n        self.agent.add_tool(WebSearchTool())\n        self.agent.add_tool(HumanInputTool())\n\n        # Create a graph with the agent\n        self.builder = GraphBuilder()\n        self.builder.add_agent(self.agent)\n        self.builder.add_memory(MemoryManager())\n        self.builder.enable_human_feedback()\n\n        # Build the graph\n        self.graph = self.builder.build()\n\n    def switch_model(self, provider: str) -&gt; str:\n        \"\"\"Switch to a different model provider.\"\"\"\n        if provider not in self.models:\n            return f\"Unknown provider: {provider}. Available providers: {', '.join(self.models.keys())}\"\n\n        # Switch the model\n        self.current_model = provider\n        self.setup_agent()\n\n        return f\"Switched to {provider} model: {self.models[provider]}\"\n\n    def process_message(self, message: str, session_id: Optional[str] = None) -&gt; Dict:\n        \"\"\"Process a user message, handling model switching commands.\"\"\"\n        # Check for model switching command\n        if message.lower().startswith(\"switch to \"):\n            provider = message.lower().replace(\"switch to \", \"\").strip()\n            result = self.switch_model(provider)\n            return {\"output\": result}\n\n        # Regular message processing with the current model\n        return self.graph.invoke({\"input\": message}, {\"session_id\": session_id})\n\n# Usage example\nif __name__ == \"__main__\":\n    agent = ModelSwitchingAgent()\n\n    # Create a unique session ID for this conversation\n    import uuid\n    session_id = str(uuid.uuid4())\n\n    # Interactive chat loop\n    print(\"Multi-Provider Agent Chat (type 'exit' to quit)\")\n    print(\"Current model: \" + agent.current_model)\n    print(\"Available commands: 'switch to openai', 'switch to google', 'switch to anthropic', 'switch to ollama'\")\n    print(\"---------------------------------------------------\")\n\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() == \"exit\":\n            break\n\n        response = agent.process_message(user_input, session_id)\n        print(f\"Agent ({agent.current_model}): {response['output']}\")\n</code></pre>"},{"location":"examples/model_provider_example/#comparing-models-example","title":"Comparing Models Example","text":"<p>This example demonstrates how to compare responses from different model providers for the same prompt:</p> <pre><code>import asyncio\nimport os\nfrom typing import Dict, List\nfrom lg_adk.models import get_model, ModelRegistry\n\n# Set up API keys (in a real application, these would be in environment variables)\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"  # Replace with your actual key\nos.environ[\"GOOGLE_API_KEY\"] = \"your-google-key\"  # Replace with your actual key\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"  # Replace with your actual key\n\nasync def compare_models(prompt: str, system_prompt: str = None) -&gt; Dict[str, str]:\n    \"\"\"Compare responses from different model providers for the same prompt.\"\"\"\n    # Configure models to use\n    models = {\n        \"OpenAI GPT-4\": get_model(\"openai/gpt-4\"),\n        \"Google Gemini Pro\": get_model(\"google/gemini-pro\"),\n        \"Anthropic Claude\": get_model(\"anthropic/claude-3-sonnet\"),\n        \"Ollama Llama3\": get_model(\"ollama/llama3\", temperature=0.7)\n    }\n\n    # Create tasks for each model\n    tasks = {}\n    for name, model in models.items():\n        tasks[name] = asyncio.create_task(\n            model.agenerate(prompt, system_prompt=system_prompt)\n        )\n\n    # Wait for all tasks to complete\n    results = {}\n    for name, task in tasks.items():\n        try:\n            results[name] = await task\n        except Exception as e:\n            results[name] = f\"Error: {str(e)}\"\n\n    return results\n\n# Example usage\nasync def main():\n    prompt = \"Explain the concept of quantum entanglement in simple terms.\"\n    system_prompt = \"You are a physics teacher explaining concepts to high school students.\"\n\n    print(f\"Prompt: {prompt}\")\n    print(\"Comparing responses from different models...\")\n    print(\"-\" * 50)\n\n    results = await compare_models(prompt, system_prompt)\n\n    for model_name, response in results.items():\n        print(f\"\\n--- {model_name} ---\")\n        print(response[:300] + \"...\" if len(str(response)) &gt; 300 else response)\n        print(\"-\" * 50)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/model_provider_example/#fallback-chain-example","title":"Fallback Chain Example","text":"<p>This example shows how to use model fallback chains for reliability:</p> <pre><code>from lg_adk.models import ModelRegistry, get_model\nimport os\n\n# Set up API keys (in a real application, these would be in environment variables)\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"  # Replace with your actual key\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"  # Replace with your actual key\n\ndef create_reliable_agent():\n    \"\"\"Create an agent with a fallback chain of models.\"\"\"\n    # Get the model registry\n    registry = ModelRegistry.get_instance()\n\n    # Register a fallback chain named \"reliable-chain\"\n    registry.register_fallback_chain(\n        \"reliable-chain\",\n        [\n            \"openai/gpt-4\",  # Try OpenAI first\n            \"anthropic/claude-3-sonnet\",  # If OpenAI fails, try Anthropic\n            \"ollama/llama3\"  # Local fallback as last resort\n        ]\n    )\n\n    # Use the fallback chain\n    fallback_model = registry.get_model(\"reliable-chain\")\n\n    # Generate a response (will fall back if primary model fails)\n    response = fallback_model.generate(\n        \"Explain why reliability is important in AI systems.\",\n        system_prompt=\"You are a helpful AI expert.\"\n    )\n\n    return response\n\n# Example usage\nif __name__ == \"__main__\":\n    result = create_reliable_agent()\n    print(\"Response from fallback chain:\")\n    print(result)\n</code></pre>"},{"location":"examples/model_provider_example/#running-the-examples","title":"Running the Examples","text":"<p>To run these examples, you'll need to:</p> <ol> <li> <p>Install LG-ADK with all optional dependencies:    <pre><code>pip install \"lg-adk[all]\"\n</code></pre></p> </li> <li> <p>Set up API keys for the model providers you want to use:    <pre><code>export OPENAI_API_KEY=\"your-openai-key\"\nexport GOOGLE_API_KEY=\"your-google-key\"\nexport ANTHROPIC_API_KEY=\"your-anthropic-key\"\n</code></pre></p> </li> <li> <p>For local models, install Ollama from https://ollama.ai/ and run:    <pre><code>ollama pull llama3\n</code></pre></p> </li> <li> <p>Run any of the example scripts:    <pre><code>python model_switching_agent.py\n</code></pre></p> </li> </ol> <p>These examples demonstrate how LG-ADK's model provider system makes it easy to work with different LLM providers, switch between them, or create fallback chains for reliability.</p>"},{"location":"examples/multi_agent_chat/","title":"\ud83e\udd1d Multi-Agent Chat Example with LG-ADK","text":"<p>This example demonstrates how to use the GroupChatTool and AgentRouter classes to create a multi-agent system that can collaborate on tasks.</p> <p>You can copy and run this example as a script.</p> <pre><code>import os\nfrom typing import Any, Dict, List\n\nfrom lg_adk import Agent, get_model\nfrom lg_adk.tools.agent_router import AgentRouter, RouterType\nfrom lg_adk.tools.group_chat import GroupChatTool\n\ndef create_specialized_agents() -&gt; Dict[str, Agent]:\n    \"\"\"Create a set of specialized agents.\"\"\"\n    model = get_model(\"gpt-3.5-turbo\")\n    research_agent = Agent(\n        agent_name=\"ResearchAgent\",\n        system_prompt=\"You are a research specialist. Your role is to provide detailed, well-researched information about any topic.\",\n        llm=model,\n    )\n    code_agent = Agent(\n        agent_name=\"CodeAgent\",\n        system_prompt=\"You are a coding expert. Your role is to write, review, and explain code.\",\n        llm=model,\n    )\n    writing_agent = Agent(\n        agent_name=\"WritingAgent\",\n        system_prompt=\"You are a writing specialist. Your role is to create well-structured, engaging content.\",\n        llm=model,\n    )\n    critic_agent = Agent(\n        agent_name=\"CriticAgent\",\n        system_prompt=\"You are a critical thinker. Your role is to analyze information and provide constructive criticism.\",\n        llm=model,\n    )\n    return {\"research\": research_agent, \"code\": code_agent, \"writing\": writing_agent, \"critic\": critic_agent}\n\n\ndef group_chat_example(agents: Dict[str, Agent], query: str) -&gt; None:\n    print(\"\\n=== Running Group Chat Example ===\\n\")\n    chat_tool = GroupChatTool(agent_registry=agents)\n    chat_id = chat_tool.create_chat(\n        name=\"Collaborative Problem Solving\", agent_ids=list(agents.keys()), metadata={\"topic\": \"Problem Solving\"}\n    )\n    messages = chat_tool.run_conversation(chat_id=chat_id, initial_prompt=query, max_turns=4)\n    print(f\"Group chat on query: {query}\\n\")\n    for msg in messages:\n        print(f\"{msg.agent_id}: {msg.content}\\n\")\n\n\ndef agent_router_example(agents: Dict[str, Agent], query: str) -&gt; None:\n    print(\"\\n=== Running Agent Router Example ===\\n\")\n    sequential_router = AgentRouter(\n        name=\"SequentialThoughtProcess\", agents=list(agents.values()), router_type=RouterType.SEQUENTIAL\n    )\n    print(f\"Sequential routing on query: {query}\\n\")\n    sequential_result = sequential_router.run(query)\n    print(f\"Final output: {sequential_result.get('output', '')}\\n\")\n    selector_router = AgentRouter(name=\"ExpertSelector\", agents=list(agents.values()), router_type=RouterType.SELECTOR)\n    print(f\"Selector routing on query: {query}\\n\")\n    selector_result = selector_router.run(query)\n    selected_agent = selector_result.get(\"agent\", \"Unknown\")\n    print(f\"Selected agent: {selected_agent}\")\n    print(f\"Output: {selector_result.get('output', '')}\\n\")\n    mixture_router = AgentRouter(\n        name=\"CollaborativeThinking\", agents=list(agents.values()), router_type=RouterType.MIXTURE\n    )\n    print(f\"Mixture routing on query: {query}\\n\")\n    mixture_result = mixture_router.run(query)\n    print(f\"Combined output: {mixture_result.get('output', '')}\\n\")\n\n\ndef main() -&gt; None:\n    if not os.getenv(\"OPENAI_API_KEY\"):\n        print(\"Please set the OPENAI_API_KEY environment variable\")\n        return\n    agents = create_specialized_agents()\n    group_chat_example(agents, \"Explain how transformer models work and provide a simple example\")\n    agent_router_example(agents, \"I need to create a Python function that calculates Fibonacci numbers\")\n    complex_query = (\n        \"I'm building a web application that needs to process large datasets. \"\n        \"Can you recommend an architecture and provide some sample code for \"\n        \"handling data processing efficiently?\"\n    )\n    group_chat_example(agents, complex_query)\n    agent_router_example(agents, complex_query)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/multi_agent_example/","title":"\ud83d\udc65 Multi-Agent System Example with LG-ADK","text":"<p>This example demonstrates how to create a multi-agent system using LG-ADK, with a coordinator agent that delegates tasks to specialized agents.</p> <p>You can copy and run this example as a script.</p> <pre><code>from lg_adk import Agent, MultiAgentSystem, get_model\n\n# Create a coordinator agent\ncoordinator = Agent(name=\"coordinator\", llm=\"ollama/llama3\", description=\"Coordinates tasks between specialized agents\")\n\n# Create specialized agents\nresearcher = Agent(\n    name=\"researcher\", llm=\"ollama/llama3\", description=\"Researches information and provides detailed answers\"\n)\n\nsummarizer = Agent(name=\"summarizer\", llm=\"ollama/llama3\", description=\"Summarizes information concisely\")\n\ncreative_writer = Agent(\n    name=\"creative_writer\", llm=\"ollama/llama3\", description=\"Creates engaging and creative content\"\n)\n\n# Create the multi-agent system\nmulti_agent_system = MultiAgentSystem(\n    name=\"research_and_writing_team\",\n    coordinator=coordinator,\n    agents=[researcher, summarizer, creative_writer],\n    description=\"A team that researches topics and creates summaries or creative content\",\n)\n\n# Run the multi-agent system interactively\nif __name__ == \"__main__\":\n    print(\"Multi-Agent System Example\")\n    print(\"==========================\")\n    print(\"Type 'exit' to quit.\\n\")\n\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() in [\"exit\", \"quit\", \"q\"]:\n            break\n\n        # Run the multi-agent system\n        result = multi_agent_system.run({\"input\": user_input})\n\n        # Print the response\n        print(f\"\\nSystem: {result.get('output', '')}\\n\")\n</code></pre>"},{"location":"examples/multi_agent_workflow/","title":"Multi-Agent Workflow Examples","text":"<p>This section provides examples of building multi-agent systems using LG-ADK.</p>"},{"location":"examples/multi_agent_workflow/#group-chat-example","title":"Group Chat Example","text":"<p>The following example demonstrates how to create a group chat where multiple agents can collaborate:</p> <pre><code>import os\nfrom typing import Dict, Any, List\n\nfrom lg_adk import Agent, get_model\nfrom lg_adk.tools.group_chat import GroupChatTool\n\n# Create specialized agents\nfinance_agent = Agent(\n    agent_name=\"FinanceExpert\",\n    system_prompt=\"You are a financial expert. Provide accurate financial advice.\",\n    llm=get_model(\"gpt-4\")\n)\n\nlegal_agent = Agent(\n    agent_name=\"LegalExpert\",\n    system_prompt=\"You are a legal expert. Provide accurate legal advice.\",\n    llm=get_model(\"gpt-4\")\n)\n\n# Create a registry of agents\nagents = {\n    \"finance\": finance_agent,\n    \"legal\": legal_agent\n}\n\n# Create the group chat tool\nchat_tool = GroupChatTool(agent_registry=agents)\n\n# Create a new chat\nchat_id = chat_tool.create_chat(\n    name=\"Financial Legal Consultation\",\n    agent_ids=[\"finance\", \"legal\"]\n)\n\n# Run a conversation\nmessages = chat_tool.run_conversation(\n    chat_id=chat_id,\n    initial_prompt=\"What are the tax implications of starting a small business?\",\n    max_turns=4  # Number of turns in the conversation\n)\n\n# Print the conversation\nfor msg in messages:\n    print(f\"{msg.agent_id}: {msg.content}\")\n</code></pre>"},{"location":"examples/multi_agent_workflow/#agent-router-example","title":"Agent Router Example","text":"<p>This example shows how to use the AgentRouter to route tasks to different agents based on their specialties:</p> <pre><code>from lg_adk import Agent, get_model\nfrom lg_adk.tools.agent_router import AgentRouter, RouterType\n\n# Create specialized agents\nresearch_agent = Agent(\n    agent_name=\"Researcher\",\n    system_prompt=\"You are a research specialist. Find and present factual information.\",\n    llm=get_model(\"gpt-4\")\n)\n\nwriter_agent = Agent(\n    agent_name=\"Writer\",\n    system_prompt=\"You are a writing specialist. Create well-structured content.\",\n    llm=get_model(\"gpt-4\")\n)\n\neditor_agent = Agent(\n    agent_name=\"Editor\",\n    system_prompt=\"You are an editor. Improve content for clarity and correctness.\",\n    llm=get_model(\"gpt-4\")\n)\n\n# Create a sequential router (research -&gt; write -&gt; edit)\nsequential_router = AgentRouter(\n    name=\"ContentCreationPipeline\",\n    agents=[research_agent, writer_agent, editor_agent],\n    router_type=RouterType.SEQUENTIAL\n)\n\n# Run a task through the sequential pipeline\nresult = sequential_router.run(\"Explain how blockchain technology works\")\nprint(result[\"output\"])\n\n# Create a selector router\nselector_router = AgentRouter(\n    name=\"ExpertSelector\",\n    agents=[research_agent, writer_agent, editor_agent],\n    router_type=RouterType.SELECTOR\n)\n\n# The router will select the most appropriate agent based on the task\nresult = selector_router.run(\"Research the latest advances in quantum computing\")\nprint(f\"Selected agent: {result.get('agent', 'Unknown')}\")\nprint(f\"Output: {result.get('output', '')}\")\n\n# Create a mixture router\nmixture_router = AgentRouter(\n    name=\"CollaborativeThinking\",\n    agents=[research_agent, writer_agent, editor_agent],\n    router_type=RouterType.MIXTURE\n)\n\n# Get results from all agents\nresult = mixture_router.run(\"What are the best practices for writing technical documentation?\")\nprint(result[\"output\"])\n</code></pre>"},{"location":"examples/multi_agent_workflow/#complete-multi-agent-workflow","title":"Complete Multi-Agent Workflow","text":"<p>This example demonstrates a complete multi-agent workflow that combines different types of collaboration:</p> <pre><code>import os\nfrom typing import Dict, Any, List\n\nfrom lg_adk import Agent, get_model, GraphBuilder\nfrom lg_adk.tools.group_chat import GroupChatTool\nfrom lg_adk.tools.agent_router import AgentRouter, RouterType\n\n# Create a team of specialized agents\nresearch_agent = Agent(\n    agent_name=\"Researcher\",\n    system_prompt=\"You are a research specialist. Find relevant information on any topic.\",\n    llm=get_model(\"gpt-4\")\n)\n\nwriter_agent = Agent(\n    agent_name=\"Writer\",\n    system_prompt=\"You are a writing specialist. Create engaging, well-structured content.\",\n    llm=get_model(\"gpt-4\")\n)\n\neditor_agent = Agent(\n    agent_name=\"Editor\",\n    system_prompt=\"You are an editor. Review and improve content for clarity and correctness.\",\n    llm=get_model(\"gpt-4\")\n)\n\nfact_checker_agent = Agent(\n    agent_name=\"FactChecker\",\n    system_prompt=\"You are a fact checker. Verify the accuracy of information.\",\n    llm=get_model(\"gpt-4\")\n)\n\n# Register all agents\nagents = {\n    \"researcher\": research_agent,\n    \"writer\": writer_agent,\n    \"editor\": editor_agent,\n    \"fact_checker\": fact_checker_agent\n}\n\n# Phase 1: Planning using group chat\ndef planning_phase(topic):\n    \"\"\"Use group chat for collaborative planning.\"\"\"\n    print(\"\\n=== Phase 1: Planning ===\\n\")\n\n    chat_tool = GroupChatTool(agent_registry=agents)\n    chat_id = chat_tool.create_chat(\n        name=\"ContentPlanning\",\n        agent_ids=[\"researcher\", \"writer\", \"editor\"]\n    )\n\n    messages = chat_tool.run_conversation(\n        chat_id=chat_id,\n        initial_prompt=f\"We need to create comprehensive content about {topic}. Let's plan our approach.\",\n        max_turns=6\n    )\n\n    # Extract plan from the last message\n    plan = messages[-1].content\n    print(\"Planning completed:\")\n    for msg in messages:\n        print(f\"{msg.agent_id}: {msg.content}\\n\")\n\n    return plan\n\n# Phase 2: Research and content creation\ndef research_and_create_phase(topic, plan):\n    \"\"\"Use sequential router for research and content creation.\"\"\"\n    print(\"\\n=== Phase 2: Research and Content Creation ===\\n\")\n\n    # Create a sequential router for research and writing\n    creation_router = AgentRouter(\n        name=\"ContentCreation\",\n        agents=[agents[\"researcher\"], agents[\"writer\"]],\n        router_type=RouterType.SEQUENTIAL\n    )\n\n    result = creation_router.run(\n        f\"Based on this plan: {plan}\\nResearch and create content about {topic}\"\n    )\n\n    draft_content = result.get(\"output\", \"\")\n    print(f\"Draft content created:\\n{draft_content}\\n\")\n\n    return draft_content\n\n# Phase 3: Review and improvement\ndef review_phase(draft_content):\n    \"\"\"Use mixture router for review and improvement.\"\"\"\n    print(\"\\n=== Phase 3: Review and Improvement ===\\n\")\n\n    # Create a mixture router for review\n    review_router = AgentRouter(\n        name=\"ContentReview\",\n        agents=[agents[\"editor\"], agents[\"fact_checker\"]],\n        router_type=RouterType.MIXTURE\n    )\n\n    result = review_router.run(\n        f\"Review and improve this content:\\n{draft_content}\"\n    )\n\n    final_content = result.get(\"output\", \"\")\n    print(f\"Final content:\\n{final_content}\\n\")\n\n    return final_content\n\n# Main workflow function\ndef multi_agent_content_workflow(topic):\n    \"\"\"Run the complete multi-agent content creation workflow.\"\"\"\n    # Phase 1: Planning\n    plan = planning_phase(topic)\n\n    # Phase 2: Research and content creation\n    draft_content = research_and_create_phase(topic, plan)\n\n    # Phase 3: Review and improvement\n    final_content = review_phase(draft_content)\n\n    return final_content\n\n# Run the workflow\nif __name__ == \"__main__\":\n    if not os.getenv(\"OPENAI_API_KEY\"):\n        print(\"Please set the OPENAI_API_KEY environment variable\")\n    else:\n        result = multi_agent_content_workflow(\"artificial intelligence ethics\")\n        print(\"\\n=== Workflow Completed ===\\n\")\n        print(result)\n</code></pre>"},{"location":"examples/multi_agent_workflow/#graph-based-multi-agent-workflow","title":"Graph-Based Multi-Agent Workflow","text":"<p>This example demonstrates how to use LG-ADK's GraphBuilder to create a more complex multi-agent workflow:</p> <pre><code>from lg_adk import Agent, GraphBuilder\nfrom lg_adk.tools.agent_router import AgentRouter, RouterType\n\n# Create specialized agents (code omitted for brevity)\n\n# Define node functions\ndef planning_node(state):\n    \"\"\"Plan the content creation approach.\"\"\"\n    topic = state.get(\"input\", \"\")\n\n    planning_agent = Agent(\n        agent_name=\"Planner\",\n        system_prompt=\"You create detailed content plans.\",\n        llm=get_model(\"gpt-4\")\n    )\n\n    result = planning_agent.run({\n        \"input\": f\"Create a detailed plan for content about: {topic}\"\n    })\n\n    plan = result.get(\"output\", \"\")\n    return {\"topic\": topic, \"plan\": plan}\n\ndef research_node(state):\n    \"\"\"Research the topic.\"\"\"\n    topic = state.get(\"topic\", \"\")\n    plan = state.get(\"plan\", \"\")\n\n    research_agent = Agent(\n        agent_name=\"Researcher\",\n        system_prompt=\"You find factual information on any topic.\",\n        llm=get_model(\"gpt-4\")\n    )\n\n    result = research_agent.run({\n        \"input\": f\"Research this topic based on the plan:\\nTopic: {topic}\\nPlan: {plan}\"\n    })\n\n    research = result.get(\"output\", \"\")\n    return {\"topic\": topic, \"plan\": plan, \"research\": research}\n\ndef writing_node(state):\n    \"\"\"Write content based on research.\"\"\"\n    topic = state.get(\"topic\", \"\")\n    plan = state.get(\"plan\", \"\")\n    research = state.get(\"research\", \"\")\n\n    writer_agent = Agent(\n        agent_name=\"Writer\",\n        system_prompt=\"You create well-structured content.\",\n        llm=get_model(\"gpt-4\")\n    )\n\n    result = writer_agent.run({\n        \"input\": f\"Write content based on:\\nTopic: {topic}\\nPlan: {plan}\\nResearch: {research}\"\n    })\n\n    draft = result.get(\"output\", \"\")\n    return {\"topic\": topic, \"plan\": plan, \"research\": research, \"draft\": draft}\n\ndef editing_node(state):\n    \"\"\"Edit and improve the draft.\"\"\"\n    draft = state.get(\"draft\", \"\")\n\n    editor_agent = Agent(\n        agent_name=\"Editor\",\n        system_prompt=\"You improve content for clarity and correctness.\",\n        llm=get_model(\"gpt-4\")\n    )\n\n    result = editor_agent.run({\n        \"input\": f\"Edit and improve this draft:\\n{draft}\"\n    })\n\n    final_content = result.get(\"output\", \"\")\n    return {\"output\": final_content}\n\n# Build the graph\nbuilder = GraphBuilder()\nbuilder.add_node(\"planning\", planning_node)\nbuilder.add_node(\"research\", research_node)\nbuilder.add_node(\"writing\", writing_node)\nbuilder.add_node(\"editing\", editing_node)\n\n# Define the flow\nflow = [\n    (None, \"planning\"),\n    (\"planning\", \"research\"),\n    (\"research\", \"writing\"),\n    (\"writing\", \"editing\"),\n    (\"editing\", None)\n]\n\n# Build and use the graph\ncontent_graph = builder.build(flow=flow)\nresult = content_graph.invoke({\"input\": \"renewable energy\"})\nprint(result[\"output\"])\n</code></pre>"},{"location":"examples/multi_agent_workflow/#full-examples","title":"Full Examples","text":"<p>For more detailed examples, see the full code in the <code>docs/examples</code> directory:</p> <ul> <li>multi_agent_chat.py: A complete example of group chat and router implementations</li> </ul>"},{"location":"examples/rag/","title":"RAG Examples","text":"<p>This section provides examples of building Retrieval-Augmented Generation (RAG) applications using LG-ADK.</p>"},{"location":"examples/rag/#simple-rag-example","title":"Simple RAG Example","text":"<p>The following example demonstrates how to create a simple RAG application using FAISS as the vector store:</p> <pre><code>import os\nfrom typing import Dict, Any, List\nfrom dotenv import load_dotenv\n\n# Import LG-ADK components\nfrom lg_adk import Agent, get_model\nfrom lg_adk.tools.retrieval import SimpleVectorRetrievalTool\n\n# Load environment variables\nload_dotenv()\n\n# Set up vector store\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_community.embeddings import OpenAIEmbeddings\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# Load and process documents\nloader = TextLoader(\"path/to/document.txt\")\ndocuments = loader.load()\n\n# Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=100\n)\nchunks = text_splitter.split_documents(documents)\n\n# Create embeddings and vector store\nembeddings = OpenAIEmbeddings()\nvector_store = FAISS.from_documents(chunks, embeddings)\n\n# Create a retrieval tool\nretrieval_tool = SimpleVectorRetrievalTool(\n    name=\"retrieve_documentation\",\n    description=\"Use this tool to retrieve documentation and reference materials from the knowledge base.\",\n    vector_store=vector_store,\n    top_k=3,\n    score_threshold=0.7\n)\n\n# Create the RAG agent\nrag_agent = Agent(\n    agent_name=\"DocumentationAssistant\",\n    system_prompt=\"\"\"\n    You are a helpful assistant with access to documentation.\n    When asked questions, use the retrieval tool to find relevant information.\n    Always reference where your information came from.\n    \"\"\",\n    llm=get_model(\"gpt-4\"),\n    tools=[retrieval_tool]\n)\n\n# Use the agent\nresponse = rag_agent.run({\"input\": \"What information do we have about X?\"})\nprint(response[\"output\"])\n</code></pre>"},{"location":"examples/rag/#chromadb-example","title":"ChromaDB Example","text":"<p>This example shows how to use ChromaDB as the vector store:</p> <pre><code>import chromadb\nfrom lg_adk import Agent, get_model\nfrom lg_adk.tools.retrieval import ChromaDBRetrievalTool\nfrom langchain_community.embeddings import OpenAIEmbeddings\n\n# Set up ChromaDB\nchroma_client = chromadb.PersistentClient(path=\"path/to/chromadb\")\n\n# Create embedding function wrapper for ChromaDB\nclass OpenAIEmbeddingFunction:\n    def __call__(self, texts):\n        return embeddings.embed_documents(texts)\n\nembeddings = OpenAIEmbeddings()\nembedding_function = OpenAIEmbeddingFunction()\n\n# Get or create collection\ncollection = chroma_client.get_or_create_collection(\n    name=\"your_collection\",\n    embedding_function=embedding_function\n)\n\n# Add documents if needed\n# collection.add(\n#     documents=[\"doc1\", \"doc2\", \"doc3\"],\n#     metadatas=[{\"source\": \"source1\"}, {\"source\": \"source2\"}, {\"source\": \"source3\"}],\n#     ids=[\"id1\", \"id2\", \"id3\"]\n# )\n\n# Create a ChromaDB retrieval tool\nretrieval_tool = ChromaDBRetrievalTool(\n    name=\"chromadb_retrieval\",\n    description=\"Use this tool to retrieve information from the ChromaDB knowledge base.\",\n    collection_name=\"your_collection\",\n    chroma_client=chroma_client,\n    embedding_function=embedding_function,\n    top_k=3,\n    score_threshold=0.3\n)\n\n# Create a RAG agent\nchromadb_agent = Agent(\n    agent_name=\"ChromaDBAssistant\",\n    system_prompt=\"You are an assistant with access to a ChromaDB knowledge base. Use the retrieval tool to find information.\",\n    llm=get_model(\"gpt-4\"),\n    tools=[retrieval_tool]\n)\n\n# Use the agent\nresponse = chromadb_agent.run({\"input\": \"What information do we have about X?\"})\nprint(response[\"output\"])\n</code></pre>"},{"location":"examples/rag/#google-adk-style-rag","title":"Google ADK-Style RAG","text":"<p>This example shows how to create a RAG application in a style similar to Google's Agent Development Kit:</p> <pre><code>import os\nfrom dotenv import load_dotenv\n\n# Import LG-ADK components\nfrom lg_adk import Agent\nfrom lg_adk.tools.retrieval import SimpleVectorRetrievalTool\n\n# Load environment variables\nload_dotenv()\n\n# System prompt\ndef return_instructions_root():\n    return \"\"\"\n    You are a helpful assistant with access to a knowledge base.\n    Use the retrieval tool to find information when answering questions.\n\n    When responding:\n    1. First retrieve relevant information using the tool\n    2. Then synthesize a clear and helpful response\n    3. If the information isn't in the knowledge base, say so\n\n    Always cite where you found your information.\n    \"\"\"\n\ndef setup_rag_agent():\n    \"\"\"Set up a RAG agent similar to Google's ADK style.\"\"\"\n    # Set up vector store (code omitted for brevity)\n    from lg_adk.models import get_model\n\n    # Create the retrieval tool\n    lg_adk_retrieval = SimpleVectorRetrievalTool(\n        name='retrieve_documentation',\n        description=(\n            'Use this tool to retrieve documentation and reference materials'\n        ),\n        vector_store=your_vector_store,\n        top_k=5,\n        score_threshold=0.6,\n    )\n\n    # Create the agent\n    rag_agent = Agent(\n        agent_name='documentation_agent',\n        system_prompt=return_instructions_root(),\n        llm=get_model('gpt-4'),\n        tools=[\n            lg_adk_retrieval,\n        ]\n    )\n\n    return rag_agent\n\n# Create and use the agent\nagent = setup_rag_agent()\nresponse = agent.run({\"input\": \"What does the documentation say about X?\"})\nprint(response[\"output\"])\n</code></pre>"},{"location":"examples/rag/#advanced-rag-with-memory","title":"Advanced RAG with Memory","text":"<p>This example shows how to create a RAG application with memory to maintain context across interactions:</p> <pre><code>from lg_adk import Agent, GraphBuilder\nfrom lg_adk.memory import MemoryManager\nfrom lg_adk.sessions import SessionManager\nfrom lg_adk.tools.retrieval import SimpleVectorRetrievalTool\n\n# Set up components\nmemory_manager = MemoryManager()\nsession_manager = SessionManager()\nretrieval_tool = SimpleVectorRetrievalTool(...)\n\n# Define node functions\ndef get_or_create_session(state):\n    \"\"\"Get or create a session.\"\"\"\n    import uuid\n\n    session_id = state.get(\"session_id\")\n    if not session_id:\n        session_id = str(uuid.uuid4())\n        session_manager.create_session(session_id)\n\n    session_data = session_manager.get_session(session_id)\n    return {\"session_id\": session_id, \"session_data\": session_data}\n\ndef retrieve_history(state):\n    \"\"\"Retrieve conversation history.\"\"\"\n    session_id = state[\"session_id\"]\n    conversation_history = memory_manager.get_conversation_history(session_id)\n    return {\"conversation_history\": conversation_history}\n\ndef retrieve_context(state):\n    \"\"\"Retrieve relevant documents.\"\"\"\n    query = state[\"input\"]\n    context = retrieval_tool.run(query)\n    return {\"context\": context}\n\ndef generate_response(state):\n    \"\"\"Generate a response based on context and history.\"\"\"\n    rag_agent = Agent(\n        agent_name=\"RAGWithMemory\",\n        system_prompt=\"Answer based on the context and conversation history.\",\n        llm=get_model(\"gpt-4\")\n    )\n\n    result = rag_agent.run({\n        \"input\": state[\"input\"],\n        \"context\": state[\"context\"],\n        \"conversation_history\": state[\"conversation_history\"]\n    })\n\n    return {\"output\": result[\"output\"]}\n\ndef update_memory(state):\n    \"\"\"Add the interaction to memory.\"\"\"\n    session_id = state[\"session_id\"]\n    memory_manager.add_message(session_id, {\"role\": \"user\", \"content\": state[\"input\"]})\n    memory_manager.add_message(session_id, {\"role\": \"assistant\", \"content\": state[\"output\"]})\n    return state\n\n# Build the graph\nbuilder = GraphBuilder()\nbuilder.add_node(\"get_or_create_session\", get_or_create_session)\nbuilder.add_node(\"retrieve_history\", retrieve_history)\nbuilder.add_node(\"retrieve_context\", retrieve_context)\nbuilder.add_node(\"generate_response\", generate_response)\nbuilder.add_node(\"update_memory\", update_memory)\n\n# Define the flow\nflow = [\n    (None, \"get_or_create_session\"),\n    (\"get_or_create_session\", \"retrieve_history\"),\n    (\"retrieve_history\", \"retrieve_context\"),\n    (\"retrieve_context\", \"generate_response\"),\n    (\"generate_response\", \"update_memory\"),\n    (\"update_memory\", None)\n]\n\n# Build and use the graph\nrag_graph = builder.build(flow=flow)\nresult = rag_graph.invoke({\"input\": \"Tell me about X\"})\nprint(result[\"output\"])\n\n# Continue the conversation\nresult = rag_graph.invoke({\"input\": \"Tell me more about it\", \"session_id\": result[\"session_id\"]})\nprint(result[\"output\"])\n</code></pre>"},{"location":"examples/rag/#full-examples","title":"Full Examples","text":"<p>For more detailed examples, see the full code in the <code>docs/examples</code> directory:</p> <ul> <li>simple_rag.py: A complete example of creating RAG agents with FAISS and ChromaDB</li> <li>google_style_rag.py: An example showing the Google ADK-style approach</li> <li>rag_with_memory.py: An example demonstrating RAG with conversation memory</li> </ul>"},{"location":"examples/rag_example/","title":"\ud83d\udcda Basic RAG Example with LG-ADK","text":"<p>This example demonstrates how to build a simple Retrieval-Augmented Generation (RAG) system that processes a user query, retrieves relevant documents from a vector store, and generates a response based on the retrieved context.</p> <p>You can copy and run this example as a script.</p> <pre><code>import os\nfrom typing import Any, Dict, List\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_community.vectorstores import Chroma\n\nfrom lg_adk import Agent, GraphBuilder\nfrom lg_adk.memory import MemoryManager\n\n# --- 1. Create sample documents ---\n# Create a sample text file for our knowledge base\nsample_text = \"\"\"\n# Artificial Intelligence\n\nArtificial intelligence (AI) is intelligence demonstrated by machines, as opposed to intelligence displayed by humans or animals.\nAI research has been defined as the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals.\n\nThe term \"artificial intelligence\" was first used by John McCarthy in 1956. The field has gone through multiple cycles of optimism followed by disappointment and loss of funding, followed by new approaches and renewed optimism.\n\n## Machine Learning\n\nMachine learning (ML) is a subset of AI that focuses on the development of algorithms that can access data and use it to learn for themselves.\nThe primary goal is to allow computers to learn automatically without human intervention.\n\n### Types of Machine Learning:\n- Supervised Learning: The algorithm is trained on labeled data.\n- Unsupervised Learning: The algorithm finds patterns in unlabeled data.\n- Reinforcement Learning: The algorithm learns through trial and error.\n\n## Natural Language Processing\n\nNatural Language Processing (NLP) is a branch of AI that helps computers understand, interpret, and manipulate human language.\nNLP is used in many applications including:\n- Voice assistants like Siri and Alexa\n- Translation services like Google Translate\n- Customer service chatbots\n\n## Deep Learning\n\nDeep learning is a subset of machine learning that uses artificial neural networks with multiple layers.\nThese neural networks attempt to simulate the behavior of the human brain to solve complex problems.\n\"\"\"\n\n# Create a documents directory if it doesn't exist\nos.makedirs(\"documents\", exist_ok=True)\n\n# Write the sample text to a file\nwith open(\"documents/ai_overview.txt\", \"w\") as f:\n    f.write(sample_text)\n\n# --- 2. Create the vector store ---\n# Load the document\ndocuments = TextLoader(\"documents/ai_overview.txt\").load()\n\n# Split the text into chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\nsplits = text_splitter.split_documents(documents)\n\n# Create embeddings\nembedding_function = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n\n# Create a vector store\nvector_store = Chroma.from_documents(documents=splits, embedding=embedding_function)\n\n# --- 3. Define the RAG agents ---\n# Query processing agent\nquery_processor = Agent(\n    name=\"query_processor\",\n    llm=\"ollama/llama3\",\n    description=\"Processes and reformulates user queries for optimal retrieval\",\n    system_message=\"\"\"You are a query processing specialist. Your job is to:\n    1. Understand the user's query\n    2. Reformulate it to make it more effective for retrieval\n    3. Extract key terms and concepts\n\n    Output only the reformulated query without any explanations or additional text.\n    \"\"\",\n)\n\n# Response generation agent\nresponse_generator = Agent(\n    name=\"response_generator\",\n    llm=\"ollama/llama3\",\n    description=\"Generates responses based on retrieved context and user query\",\n    system_message=\"\"\"You are a response generator. Your job is to:\n    1. Read the retrieved context carefully\n    2. Understand the user's original question\n    3. Generate a comprehensive and accurate response based on the context\n    4. If the context doesn't contain relevant information, acknowledge the limitations\n\n    Always base your answers on the provided context only.\n    \"\"\",\n)\n\n\n# --- 4. Define the RAG workflow functions ---\ndef process_query(state: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Process the user query for retrieval.\"\"\"\n    user_input = state.get(\"input\", \"\")\n\n    # Use the query processor agent to reformulate the query\n    result = query_processor.run({\"input\": user_input})\n    processed_query = result.get(\"output\", user_input)\n\n    return {\n        **state,\n        \"original_query\": user_input,\n        \"processed_query\": processed_query,\n    }\n\n\ndef retrieve_context(state: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Retrieve relevant context from the vector store.\"\"\"\n    processed_query = state.get(\"processed_query\", \"\")\n\n    # Search the vector store\n    docs = vector_store.similarity_search(processed_query, k=3)\n    context = [doc.page_content for doc in docs]\n\n    return {\n        **state,\n        \"context\": context,\n    }\n\n\ndef generate_response(state: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Generate a response based on the query and retrieved context.\"\"\"\n    original_query = state.get(\"original_query\", \"\")\n    context = state.get(\"context\", [])\n\n    # Format the context\n    formatted_context = \"\\n\\n\".join([f\"Document chunk {i+1}:\\n{chunk}\" for i, chunk in enumerate(context)])\n\n    # Generate response with the response agent\n    prompt = f\"\"\"\n    Context information:\n    {formatted_context}\n\n    User question: {original_query}\n\n    Please answer the question based on the context provided.\n    \"\"\"\n\n    result = response_generator.run({\"input\": prompt})\n    response = result.get(\"output\", \"\")\n\n    return {\n        **state,\n        \"output\": response,\n    }\n\n\n# --- 5. Build the RAG graph ---\nbuilder = GraphBuilder()\n\n# Create a memory manager for session persistence\nmemory_manager = MemoryManager()\nbuilder.add_memory(memory_manager)\n\n# Build the graph with the defined workflow\nflow = [\n    (None, \"process_query\"),\n    (\"process_query\", \"retrieve_context\"),\n    (\"retrieve_context\", \"generate_response\"),\n    (\"generate_response\", None),\n]\n\n# Add the nodes\nbuilder.add_node(\"process_query\", process_query)\nbuilder.add_node(\"retrieve_context\", retrieve_context)\nbuilder.add_node(\"generate_response\", generate_response)\n\n# Build the graph with the flow\ngraph = builder.build(flow=flow)\n\n# --- 6. Run the RAG system ---\nif __name__ == \"__main__\":\n    print(\"RAG Example\")\n    print(\"===========\")\n    print(\"Type 'exit' to quit.\\n\")\n\n    while True:\n        user_input = input(\"You: \")\n        if user_input.lower() in [\"exit\", \"quit\", \"q\"]:\n            break\n\n        # Run the RAG graph\n        result = graph.invoke({\"input\": user_input})\n\n        # Print the response\n        print(f\"\\nRAG System: {result.get('output', '')}\\n\")\n</code></pre>"},{"location":"examples/session_management_example/","title":"\ud83d\uddc2\ufe0f Session Management Example with LG-ADK","text":"<p>This example demonstrates enhanced session management with LG-ADK, including user tracking, rich metadata, and analytics features.</p> <p>You can copy and run this example as a script.</p> <pre><code>import os\nimport uuid\nfrom datetime import datetime\nfrom typing import Any, Dict, List\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_openai import ChatOpenAI\n\nfrom lg_adk.agents.base import Agent\nfrom lg_adk.builders.graph_builder import GraphBuilder\nfrom lg_adk.sessions.session_manager import SynchronizedSessionManager\n\n# Set up a simple agent for this example\nclass SimpleAgent(Agent):\n    \"\"\"A simple agent that responds to user queries.\"\"\"\n\n    def __init__(self, name: str = \"simple_agent\"):\n        \"\"\"Initialize the agent.\"\"\"\n        super().__init__(name=name)\n\n        # Set up the model\n        model_name = os.environ.get(\"OPENAI_MODEL_NAME\", \"gpt-3.5-turbo\")\n        self.llm = ChatOpenAI(model_name=model_name, temperature=0.7)\n\n        # Set up the prompt\n        self.prompt = ChatPromptTemplate.from_template(\n            \"\"\"\n            You are a helpful assistant. Answer the user's question\n            based on the following context and current conversation.\n\n            Current conversation:\n            {messages}\n\n            User query: {input}\n\n            Your response:\n        \"\"\"\n        )\n\n    def __call__(self, state: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Process the state and generate a response.\"\"\"\n        # Extract input and context from state\n        user_input = state.get(\"input\", \"\")\n        messages = state.get(\"messages\", [])\n\n        # Format messages for context\n        formatted_messages = \"\\n\".join([f\"{msg.get('role', 'unknown')}: {msg.get('content', '')}\" for msg in messages])\n\n        # Generate response\n        chain = self.prompt | self.llm\n        response = chain.invoke({\"input\": user_input, \"messages\": formatted_messages})\n\n        # Update state with response\n        updated_state = state.copy()\n        updated_state[\"output\"] = response.content\n        updated_state[\"agent\"] = self.name\n\n        # Add the new message to the history\n        if \"messages\" not in updated_state:\n            updated_state[\"messages\"] = []\n\n        # Add user message if not already present\n        if not any(msg.get(\"role\") == \"user\" and msg.get(\"content\") == user_input for msg in updated_state[\"messages\"]):\n            updated_state[\"messages\"].append({\"role\": \"user\", \"content\": user_input})\n\n        # Add assistant response\n        updated_state[\"messages\"].append({\"role\": \"assistant\", \"content\": response.content})\n\n        return updated_state\n\n\ndef main():\n    \"\"\"Run the session management example.\"\"\"\n    # Create a synchronized session manager (thread-safe for production use)\n    session_manager = SynchronizedSessionManager()\n\n    # Create a simple agent\n    agent = SimpleAgent()\n\n    # Create a graph builder with the agent and session manager\n    builder = GraphBuilder(name=\"session_example\")\n    builder.add_agent(agent)\n    builder.configure_session_management(session_manager)\n\n    # Build the graph\n    graph = builder.build()\n\n    print(\"\\n\ud83d\udd04 Starting new session for user 'alice'...\\n\")\n\n    # Start a new conversation (simulate a user session)\n    alice_metadata = {\"user_id\": \"alice\", \"device\": \"iPhone\", \"locale\": \"en-US\", \"source\": \"mobile_app\"}\n\n    # First message in the conversation\n    response1 = builder.run(\n        message=\"Hello! Can you tell me about session management in LangGraph?\", metadata=alice_metadata\n    )\n\n    # Get the session ID that was created\n    alice_session_id = response1[\"session_id\"]\n    print(f\"Created session: {alice_session_id}\")\n    print(f\"Response: {response1['output']}\\n\")\n\n    # Continue the conversation using the same session ID\n    response2 = builder.run(message=\"How does it help with multi-user applications?\", session_id=alice_session_id)\n    print(f\"Response: {response2['output']}\\n\")\n\n    # Start a new conversation for a different user\n    print(\"\\n\ud83d\udd04 Starting new session for user 'bob'...\\n\")\n\n    bob_metadata = {\"user_id\": \"bob\", \"device\": \"Android\", \"locale\": \"en-GB\", \"source\": \"web_browser\"}\n\n    # First message in the second conversation\n    response3 = builder.run(\n        message=\"What's the difference between StateGraph and Graph in LangGraph?\", metadata=bob_metadata\n    )\n\n    # Get the session ID for Bob\n    bob_session_id = response3[\"session_id\"]\n    print(f\"Created session: {bob_session_id}\")\n    print(f\"Response: {response3['output']}\\n\")\n\n    # Now demonstrate the enhanced features\n    print(\"\\n\ud83d\udcca Enhanced Session Features:\\n\")\n\n    # 1. Get all sessions for a user\n    alice_sessions = session_manager.get_user_sessions(\"alice\")\n    print(f\"Alice's sessions: {alice_sessions}\")\n\n    # 2. Get session statistics and analytics\n    alice_session = session_manager.get_session(alice_session_id)\n    print(f\"Alice's session statistics:\")\n    print(f\"  - Interactions: {alice_session.interactions}\")\n    print(f\"  - Total tokens in: {alice_session.total_tokens_in}\")\n    print(f\"  - Total tokens out: {alice_session.total_tokens_out}\")\n    print(f\"  - Average response time: {alice_session.total_response_time / max(1, alice_session.interactions):.2f}s\")\n\n    # 3. Get session metadata\n    alice_metadata = session_manager.get_session_metadata(alice_session_id)\n    print(f\"Alice's metadata: {alice_metadata}\")\n\n    # 4. Update session metadata\n    session_manager.update_session_metadata(\n        alice_session_id, {\"subscription_tier\": \"premium\", \"last_topic\": \"session_management\"}, merge=True\n    )\n\n    # Get updated metadata\n    updated_metadata = session_manager.get_session_metadata(alice_session_id)\n    print(f\"Alice's updated metadata: {updated_metadata}\")\n\n    # Calculate session duration\n    metadata = session_manager.get_session_metadata(alice_session_id)\n    created_at = metadata.get(\"created_at\", datetime.now())\n    duration = (datetime.now() - created_at).total_seconds()\n    print(f\"Alice's session duration: {duration:.2f} seconds\")\n\n    # Cleanup\n    print(\"\\n\ud83e\uddf9 Cleaning up sessions...\")\n    session_manager.end_session(alice_session_id)\n    session_manager.end_session(bob_session_id)\n    print(\"Sessions ended.\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/simple_chat/","title":"\ud83d\udcac Simple Chat Example with LG-ADK","text":"<p>This example demonstrates how to build a simple chat agent using LG-ADK, with memory and session management.</p> <p>You can copy and run this example as a script.</p> <pre><code>from lg_adk import Agent, GraphBuilder, MemoryManager, SessionManager\nfrom lg_adk.config.settings import Settings\nfrom lg_adk.tools.web_search import WebSearchTool\n\n\ndef main():\n    \"\"\"Run the simple chat example.\"\"\"\n    # Load settings from environment variables\n    settings = Settings.from_env()\n\n    # Create an agent\n    assistant = Agent(\n        name=\"assistant\",\n        llm=settings.default_llm,\n        description=\"A helpful AI assistant that answers user questions\",\n    )\n\n    # Add a tool to the agent\n    assistant.add_tool(WebSearchTool())\n\n    # Create memory and session managers\n    memory_manager = MemoryManager()\n    session_manager = SessionManager(memory_manager=memory_manager)\n\n    # Create a new session\n    session = session_manager.create_session()\n\n    # Create a graph with the agent\n    builder = GraphBuilder()\n    builder.add_agent(assistant)\n\n    # Build the graph\n    graph = builder.build()\n\n    # Simple chat loop\n    print(\"Simple Chat Example (type 'exit' to quit)\")\n    print(\"----------------------------------------\")\n\n    while True:\n        # Get user input\n        user_input = input(\"\\nYou: \")\n\n        if user_input.lower() == \"exit\":\n            break\n\n        # Process the input with the graph\n        state = {\"input\": user_input}\n        state = session_manager.process_with_session(session.id, state)\n        result = graph.invoke(state)\n\n        # Display the result\n        print(f\"\\nAssistant: {result.get('output', 'No response')}\")\n\n    print(\"\\nChat session ended.\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/simple_rag/","title":"\ud83d\udcd6 Simple RAG Example with LG-ADK","text":"<p>This example demonstrates how to create a RAG (Retrieval-Augmented Generation) agent with LG-ADK, similar to Google's ADK. It covers both FAISS and ChromaDB vector store options.</p> <p>You can copy and run this example as a script.</p> <pre><code>import os\nfrom typing import Any, Dict, List\nfrom dotenv import load_dotenv\n\n# Import LG-ADK components\nfrom lg_adk import Agent, get_model\nfrom lg_adk.tools.retrieval import ChromaDBRetrievalTool, SimpleVectorRetrievalTool\n\n# Load environment variables\nload_dotenv()\n\ndef create_simple_rag() -&gt; None:\n    \"\"\"Create a simple RAG agent using a vector store.\"\"\"\n    try:\n        # Import LangChain components\n        from langchain.text_splitter import RecursiveCharacterTextSplitter\n        from langchain_community.document_loaders import TextLoader\n        from langchain_community.embeddings import OpenAIEmbeddings\n        from langchain_community.vectorstores import FAISS, Chroma\n    except ImportError:\n        print(\"LangChain packages are required for this example. Install with:\")\n        print(\"pip install langchain langchain-community faiss-cpu\")\n        return\n\n    print(\"\\n=== Creating a Simple RAG Agent ===\\n\")\n\n    # Load documents (replace with your own documents)\n    docs_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"data\")\n    os.makedirs(docs_dir, exist_ok=True)\n\n    # Create a sample document if it doesn't exist\n    sample_doc_path = os.path.join(docs_dir, \"sample_doc.txt\")\n    if not os.path.exists(sample_doc_path):\n        with open(sample_doc_path, \"w\") as f:\n            f.write(\"\"\"\n# LG-ADK Documentation\n\n## Overview\nLG-ADK (LangGraph Agent Development Kit) is a framework for building AI agents and multi-agent systems.\nIt provides tools for creating, orchestrating, and deploying AI agents.\n\n## Features\n- Agent-based architecture\n- Multi-agent collaboration\n- Memory management\n- Tool integration\n- Human-in-the-loop capabilities\n\n## Getting Started\nTo get started with LG-ADK, install the package and import the necessary components:\n\n```python\npip install lg-adk\n\nfrom lg_adk import Agent, GraphBuilder\n</code></pre>"},{"location":"examples/simple_rag/#agent-creation","title":"Agent Creation","text":"<p>Create an agent by specifying a model and system prompt:</p> <pre><code>agent = Agent(\n    agent_name=\"MyAgent\",\n    system_prompt=\"You are a helpful assistant.\",\n    llm=get_model(\"gpt-4\")\n)\n</code></pre>"},{"location":"examples/simple_rag/#building-graphs","title":"Building Graphs","text":"<p>Create a graph by connecting multiple agents:</p> <p><pre><code>builder = GraphBuilder()\nbuilder.add_agent(\"assistant\", assistant_agent)\nbuilder.add_agent(\"researcher\", research_agent)\nbuilder.enable_human_feedback()\ngraph = builder.build()\n</code></pre>             \"\"\")</p> <pre><code># Load and process documents\nloader = TextLoader(sample_doc_path)\ndocuments = loader.load()\n\n# Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\nchunks = text_splitter.split_documents(documents)\n\n# Create embeddings using OpenAI\nif not os.getenv(\"OPENAI_API_KEY\"):\n    print(\"OPENAI_API_KEY environment variable is required.\")\n    print(\"Please set it before running this example.\")\n    return\n\nembeddings = OpenAIEmbeddings()\n\n# Create a vector store\nvector_store = FAISS.from_documents(chunks, embeddings)\n\n# Create a retrieval tool\nretrieval_tool = SimpleVectorRetrievalTool(\n    name=\"retrieve_documentation\",\n    description=\"Use this tool to retrieve documentation and reference materials from the knowledge base.\",\n    vector_store=vector_store,\n    top_k=3,\n    score_threshold=0.7,\n)\n\n# System prompt for the RAG agent\nsystem_prompt = \"\"\"\nYou are a helpful assistant with access to documentation about LG-ADK.\nWhen asked questions, use the retrieval tool to find relevant information.\n\nFollow these steps when responding:\n1. Analyze the question to understand what information is needed\n2. Use the retrieval tool to find relevant documentation\n3. Synthesize a helpful response based on the retrieved information\n4. If the information is not available, acknowledge that and provide general help\n\nAlways reference where your information comes from in the retrieved documents.\n\"\"\"\n\n# Create the RAG agent\nmodel = get_model(\"gpt-4\")\nrag_agent = Agent(\n    agent_name=\"DocumentationAssistant\", system_prompt=system_prompt, llm=model, tools=[retrieval_tool]\n)\n\n# Test the agent with a sample question\nsample_questions = [\n    \"How do I create an agent with LG-ADK?\",\n    \"What are the main features of LG-ADK?\",\n    \"Can you explain how to build a graph with multiple agents?\",\n    \"What is the purpose of LG-ADK?\",\n]\n\n# Run the agent on each sample question\nfor question in sample_questions:\n    print(f\"\\nQuestion: {question}\")\n    response = rag_agent.run({\"input\": question})\n    print(f\"Answer: {response.get('output', 'No response')}\")\n</code></pre> <p>def create_chromadb_rag() -&gt; None:     \"\"\"Create a RAG agent using ChromaDB.\"\"\"     try:         import chromadb         from langchain.text_splitter import RecursiveCharacterTextSplitter         from langchain_community.document_loaders import TextLoader         from langchain_community.embeddings import OpenAIEmbeddings     except ImportError:         print(\"ChromaDB and LangChain packages are required for this example. Install with:\")         print(\"pip install chromadb langchain langchain-community\")         return</p> <pre><code>print(\"\\n=== Creating a ChromaDB RAG Agent ===\\n\")\n\n# Sample document path\ndocs_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), \"data\")\nsample_doc_path = os.path.join(docs_dir, \"sample_doc.txt\")\n\nif not os.path.exists(sample_doc_path):\n    print(f\"Sample document not found: {sample_doc_path}\")\n    print(\"Please run the create_simple_rag() function first.\")\n    return\n\n# Load and process documents\nloader = TextLoader(sample_doc_path)\ndocuments = loader.load()\n\n# Split documents into chunks\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\nchunks = text_splitter.split_documents(documents)\n\n# Create embeddings using OpenAI\nif not os.getenv(\"OPENAI_API_KEY\"):\n    print(\"OPENAI_API_KEY environment variable is required.\")\n    return\n\nembeddings = OpenAIEmbeddings()\n\n# Create a ChromaDB client\nchroma_dir = os.path.join(docs_dir, \"chroma_db\")\nos.makedirs(chroma_dir, exist_ok=True)\n\nclient = chromadb.PersistentClient(path=chroma_dir)\n\n# Create embedding function wrapper for ChromaDB\nclass OpenAIEmbeddingFunction:\n    def __call__(self, texts):\n        return embeddings.embed_documents(texts)\n\nembedding_function = OpenAIEmbeddingFunction()\n\n# Get or create collection\ncollection = client.get_or_create_collection(name=\"lg_adk_docs\", embedding_function=embedding_function)\n\n# Add documents if collection is empty\nif collection.count() == 0:\n    collection.add(\n        documents=[chunk.page_content for chunk in chunks],\n        metadatas=[chunk.metadata for chunk in chunks],\n        ids=[f\"doc_{i}\" for i in range(len(chunks))],\n    )\n\n# Create a ChromaDB retrieval tool\nretrieval_tool = ChromaDBRetrievalTool(\n    name=\"chromadb_retrieval\",\n    description=\"Use this tool to retrieve documentation about LG-ADK from the ChromaDB knowledge base.\",\n    collection_name=\"lg_adk_docs\",\n    chroma_client=client,\n    embedding_function=embedding_function,\n    top_k=3,\n    score_threshold=0.3,\n)\n\n# System prompt for the RAG agent\nsystem_prompt = \"\"\"\nYou are a helpful assistant with access to documentation about LG-ADK.\nWhen asked questions, use the ChromaDB retrieval tool to find relevant information.\n\nFollow these steps when responding:\n1. Analyze the question to understand what information is needed\n2. Use the retrieval tool to find relevant documentation\n3. Synthesize a helpful response based on the retrieved information\n4. If the information is not available, acknowledge that and provide general help\n\nAlways cite where your information comes from in the retrieved documents.\n\"\"\"\n\n# Create the RAG agent\nmodel = get_model(\"gpt-4\")\nchroma_rag_agent = Agent(\n    agent_name=\"ChromaDocumentationAssistant\", system_prompt=system_prompt, llm=model, tools=[retrieval_tool]\n)\n\n# Test the agent with a sample question\nquestion = \"How do I create a multi-agent system with LG-ADK?\"\nresponse = chroma_rag_agent.run({\"input\": question})\nprint(f\"\\nQuestion: {question}\")\nprint(f\"Answer: {response.get('output', 'No response')}\")\n</code></pre> <p>def main() -&gt; None:     create_simple_rag()     create_chromadb_rag()</p> <p>if name == \"main\":     main()</p>"},{"location":"examples/tool_usage/","title":"Tool Usage with LG-ADK","text":"<p>This guide demonstrates how to incorporate tools into your agents using LG-ADK. Tools extend your agent's capabilities, allowing them to perform actions like retrieving information, calculating values, or interacting with external systems.</p>"},{"location":"examples/tool_usage/#basic-tool-integration","title":"Basic Tool Integration","text":""},{"location":"examples/tool_usage/#step-1-define-a-simple-tool","title":"Step 1: Define a Simple Tool","text":"<p>First, let's define a simple calculator tool:</p> <pre><code>from typing import Dict, Any\nfrom lg_adk.tools.base import BaseTool\n\nclass CalculatorTool(BaseTool):\n    name = \"calculator\"\n    description = \"Performs basic arithmetic operations\"\n\n    def _run(self, operation: str, a: float, b: float) -&gt; Dict[str, Any]:\n        \"\"\"\n        Perform a basic arithmetic operation.\n\n        Args:\n            operation: The operation to perform (add, subtract, multiply, divide)\n            a: First number\n            b: Second number\n\n        Returns:\n            Dictionary containing the result\n        \"\"\"\n        result = None\n        if operation == \"add\":\n            result = a + b\n        elif operation == \"subtract\":\n            result = a - b\n        elif operation == \"multiply\":\n            result = a * b\n        elif operation == \"divide\":\n            if b == 0:\n                return {\"error\": \"Cannot divide by zero\"}\n            result = a / b\n        else:\n            return {\"error\": f\"Unknown operation: {operation}\"}\n\n        return {\"result\": result}\n</code></pre>"},{"location":"examples/tool_usage/#step-2-register-the-tool-with-your-agent","title":"Step 2: Register the Tool with Your Agent","text":"<p>Now let's create an agent that can use this tool:</p> <pre><code>from lg_adk.agents import Agent\nfrom lg_adk.models import get_model\n\n# Initialize your agent\nagent = Agent(\n    name=\"math_assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a helpful math assistant. Use the calculator tool when needed.\",\n    tools=[CalculatorTool()]\n)\n\n# Now the agent can use the calculator tool during conversations\nresponse = agent.run(\"What is 1234 \u00d7 5678?\")\nprint(response)\n</code></pre>"},{"location":"examples/tool_usage/#example-web-search-tool","title":"Example: Web Search Tool","text":"<p>Here's a more complex example using a web search tool:</p> <pre><code>from lg_adk.tools.web import WebSearchTool\nfrom lg_adk.agents import Agent\nfrom lg_adk.models import get_model\n\n# Initialize an agent with the web search tool\nagent = Agent(\n    name=\"research_assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a research assistant. Use the web search tool to find up-to-date information.\",\n    tools=[WebSearchTool(api_key=\"your_search_api_key\")]\n)\n\n# Ask the agent a question that requires searching for information\nresponse = agent.run(\"What were the major tech news headlines yesterday?\")\nprint(response)\n</code></pre>"},{"location":"examples/tool_usage/#complete-example-multi-tool-agent","title":"Complete Example: Multi-Tool Agent","text":"<p>This example shows how to create an agent with multiple tools:</p> <pre><code>import os\nfrom lg_adk.agents import Agent\nfrom lg_adk.models import get_model\nfrom lg_adk.tools.web import WebSearchTool\nfrom lg_adk.tools.file import FileReadTool, FileWriteTool\nfrom lg_adk.tools.memory import MemoryTool\n\n# Setup tools\nweb_search = WebSearchTool(api_key=os.environ.get(\"SEARCH_API_KEY\"))\nfile_read = FileReadTool()\nfile_write = FileWriteTool()\nmemory = MemoryTool()\n\n# Create an agent with multiple tools\nagent = Agent(\n    name=\"assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a helpful assistant with multiple capabilities:\n    - You can search the web for information\n    - You can read and write files\n    - You can store and retrieve information from your memory\n    Use these tools whenever appropriate to help the user.\"\"\",\n    tools=[web_search, file_read, file_write, memory]\n)\n\n# Example interaction\nconversation = agent.run(\"\"\"\nPlease help me with these tasks:\n1. Find the current price of Bitcoin\n2. Save that information to a file called 'crypto_prices.txt'\n3. Remember that I'm interested in cryptocurrency prices\n\"\"\")\n\nprint(conversation)\n</code></pre>"},{"location":"examples/tool_usage/#tool-output-processing","title":"Tool Output Processing","text":"<p>LG-ADK automatically handles parsing tool outputs and sending them back to the model. You can also customize how tool outputs are processed:</p> <pre><code>from lg_adk.agents import Agent\nfrom lg_adk.models import get_model\nfrom lg_adk.tools.base import BaseTool\n\nclass CustomTool(BaseTool):\n    name = \"custom_tool\"\n    description = \"A custom tool with output processing\"\n\n    def _run(self, query: str) -&gt; dict:\n        # Tool implementation\n        return {\"data\": f\"Processed: {query}\"}\n\n    def process_output(self, output: dict) -&gt; str:\n        \"\"\"Custom processing of tool output before sending to model\"\"\"\n        return f\"TOOL RESULT: {output['data'].upper()}\"\n\n# Create agent with custom tool\nagent = Agent(\n    name=\"custom_agent\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"Use the custom tool when appropriate.\",\n    tools=[CustomTool()]\n)\n</code></pre>"},{"location":"examples/tool_usage/#tool-error-handling","title":"Tool Error Handling","text":"<p>Tools in LG-ADK include built-in error handling mechanisms:</p> <pre><code>from lg_adk.tools.base import BaseTool\n\nclass RiskyTool(BaseTool):\n    name = \"risky_tool\"\n    description = \"A tool that might fail\"\n\n    def _run(self, input_param: str) -&gt; dict:\n        try:\n            # Some operation that might fail\n            if input_param == \"fail\":\n                raise ValueError(\"Demonstration error\")\n            return {\"result\": f\"Successfully processed {input_param}\"}\n        except Exception as e:\n            # Error handling\n            return {\n                \"error\": str(e),\n                \"status\": \"failed\"\n            }\n</code></pre> <p>With proper tool design, your agents can handle errors gracefully and provide helpful feedback to users.</p>"},{"location":"examples/enhanced_session_management/","title":"Enhanced Session Management Example","text":"<p>This example demonstrates advanced session management capabilities in LG-ADK with features such as:</p> <ul> <li>Custom session manager with analytics</li> <li>Session creation and tracking</li> <li>Usage statistics and metadata management</li> <li>Session expiration and cleanup</li> </ul>"},{"location":"examples/enhanced_session_management/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.11+</li> <li>Dependencies listed in <code>requirements.txt</code></li> </ul>"},{"location":"examples/enhanced_session_management/#installation","title":"Installation","text":"<pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"examples/enhanced_session_management/#running-the-example","title":"Running the Example","text":"<pre><code>python main.py\n</code></pre>"},{"location":"examples/enhanced_session_management/#key-features","title":"Key Features","text":"<ul> <li>Session Analytics: Track and analyze session metrics</li> <li>Extended Metadata: Store and update rich metadata for each session</li> <li>Usage Statistics: Monitor conversation counts and session duration</li> <li>Automated Cleanup: Manage expired sessions with configurable timeouts</li> </ul> <p>This example shows how to build enterprise-ready applications with robust session management using the LG-ADK framework.</p>"},{"location":"examples/morphik_example/","title":"Morphik Integration Examples","text":"<p>This directory contains examples demonstrating how to integrate Morphik with LG-ADK. Morphik is a powerful platform for AI applications that provides advanced document processing, knowledge graph capabilities, and structured context integration via Model Context Protocol (MCP).</p>"},{"location":"examples/morphik_example/#prerequisites","title":"Prerequisites","text":"<p>Before running these examples, you'll need:</p> <ol> <li>A running Morphik instance - follow Morphik's installation guide</li> <li>The Morphik Python package: <code>pip install morphik</code></li> <li>OpenAI API key (for MCP examples) - set as environment variable <code>OPENAI_API_KEY</code></li> </ol>"},{"location":"examples/morphik_example/#configuration","title":"Configuration","text":"<p>You can configure Morphik integration using environment variables:</p> <pre><code># Morphik connection settings\nexport MORPHIK_HOST=localhost  # Default is localhost\nexport MORPHIK_PORT=8000       # Default is 8000\nexport MORPHIK_API_KEY=your_api_key  # Optional API key if your instance requires it\n\n# Default user and folder for Morphik operations\nexport MORPHIK_DEFAULT_USER=default  # Default user for Morphik operations\nexport MORPHIK_DEFAULT_FOLDER=default  # Default folder for documents\n\n# Set to use Morphik as the default database in LG-ADK\nexport USE_MORPHIK_AS_DEFAULT=true  # Set to \"true\" to use Morphik as default\n</code></pre>"},{"location":"examples/morphik_example/#examples","title":"Examples","text":""},{"location":"examples/morphik_example/#basic-morphik-integration","title":"Basic Morphik Integration","text":"<p>Run the basic example to see how to connect to Morphik, create a folder, add documents, and perform a simple query:</p> <pre><code>python morphik_integration.py\n</code></pre> <p>This demonstrates: - Connecting to a Morphik instance - Creating folders and adding documents - Using a Morphik retrieval tool with an agent - Basic query processing</p>"},{"location":"examples/morphik_example/#advanced-morphik-features","title":"Advanced Morphik Features","text":"<p>Run the advanced example to see how to use Morphik's knowledge graph capabilities and MCP integration:</p> <pre><code>python advanced_morphik.py\n</code></pre> <p>This example showcases: - Creating knowledge graphs from documents - Managing knowledge graphs (creating, updating, listing, deleting) - Querying entity relationships from knowledge graphs - Using Model Context Protocol for structured information retrieval - Multiple agents working with the same Morphik knowledge base</p>"},{"location":"examples/morphik_example/#using-morphik-in-your-lg-adk-applications","title":"Using Morphik in Your LG-ADK Applications","text":""},{"location":"examples/morphik_example/#setting-up-the-morphik-database-manager","title":"Setting Up the Morphik Database Manager","text":"<pre><code>from lg_adk.database import MorphikDatabaseManager\nfrom lg_adk.config import get_settings\n\nsettings = get_settings()\ndb_manager = MorphikDatabaseManager(\n    host=settings.morphik_host,\n    port=settings.morphik_port,\n    api_key=settings.morphik_api_key,\n    default_user=settings.morphik_default_user,\n    default_folder=settings.morphik_default_folder\n)\n</code></pre>"},{"location":"examples/morphik_example/#creating-retrieval-tools","title":"Creating Retrieval Tools","text":"<pre><code>from lg_adk.tools import MorphikRetrievalTool\n\n# Basic semantic search tool\nretrieval_tool = MorphikRetrievalTool(\n    folder_path=\"my_folder\",\n    user=\"my_user\"\n)\n</code></pre>"},{"location":"examples/morphik_example/#creating-knowledge-graph-tools","title":"Creating Knowledge Graph Tools","text":"<pre><code>from lg_adk.tools import MorphikGraphTool, MorphikGraphCreationTool\n\n# Tool for querying knowledge graphs\ngraph_tool = MorphikGraphTool(\n    folder_path=\"my_folder\",\n    user=\"my_user\",\n    graph_name=\"my_knowledge_graph\",\n    hop_depth=2,\n    include_paths=True\n)\n\n# Tool for creating and managing knowledge graphs\ngraph_creation_tool = MorphikGraphCreationTool(\n    folder_path=\"my_folder\",\n    user=\"my_user\"\n)\n</code></pre>"},{"location":"examples/morphik_example/#using-morphik-with-agents","title":"Using Morphik with Agents","text":"<pre><code>from lg_adk.agents import Agent\nfrom lg_adk.models import OpenAIModelProvider\n\nagent = Agent(\n    name=\"MorphikAgent\",\n    model_provider=OpenAIModelProvider(model=\"gpt-4-turbo\"),\n    tools=[retrieval_tool, graph_tool],\n    system_prompt=\"You are an assistant with access to a Morphik knowledge base.\"\n)\n</code></pre>"},{"location":"examples/morphik_example/#using-mcp-for-advanced-context-integration","title":"Using MCP for Advanced Context Integration","text":"<pre><code>from lg_adk.tools import MorphikMCPTool\n\n# Create an MCP tool\nmcp_tool = MorphikMCPTool(\n    folder_path=\"my_folder\",\n    user=\"my_user\"\n)\n\n# Use with an MCP-compatible model\nmcp_agent = Agent(\n    name=\"MCPAgent\",\n    model_provider=OpenAIModelProvider(model=\"gpt-4-turbo\"),\n    tools=[mcp_tool],\n    system_prompt=\"You are an assistant with access to structured context via MCP.\"\n)\n</code></pre>"},{"location":"examples/morphik_example/#creating-and-managing-knowledge-graphs","title":"Creating and Managing Knowledge Graphs","text":"<pre><code># Create a knowledge graph from documents\ngraph_creation_tool._run(\n    action=\"create\",\n    graph_name=\"tech_knowledge\",\n    document_ids=[\"doc1\", \"doc2\", \"doc3\"],\n    entity_extraction_prompt=\"Extract technology entities and their relationships\"\n)\n\n# List available knowledge graphs\ngraphs = graph_creation_tool._run(action=\"list\", graph_name=\"\")\n\n# Update a knowledge graph with new documents\ngraph_creation_tool._run(\n    action=\"update\",\n    graph_name=\"tech_knowledge\",\n    document_ids=[\"doc4\", \"doc5\"]\n)\n\n# Delete a knowledge graph\ngraph_creation_tool._run(action=\"delete\", graph_name=\"tech_knowledge\")\n</code></pre>"},{"location":"examples/morphik_example/#using-morphik-as-the-default-database","title":"Using Morphik as the Default Database","text":"<p>To configure LG-ADK to use Morphik as the default database:</p> <ol> <li>Set the environment variable: <code>USE_MORPHIK_AS_DEFAULT=true</code></li> <li>In your code, confirm using <code>get_settings().use_morphik_as_default</code></li> </ol>"},{"location":"examples/morphik_example/#additional-resources","title":"Additional Resources","text":"<ul> <li>Morphik Documentation</li> <li>Model Context Protocol (MCP) Specification</li> </ul>"},{"location":"getting_started/installation/","title":"\ud83d\udee0\ufe0f Installation","text":"<p>LG-ADK can be installed using pip or Poetry.</p>"},{"location":"getting_started/installation/#prerequisites","title":"\u26a1 Prerequisites","text":"<ul> <li>\ud83d\udc0d Python 3.11 or higher</li> <li>\ud83e\udd99 (Optional) Ollama for local model support</li> <li>\ud83e\udd16 (Optional) Access to Google AI services for Gemini models</li> </ul>"},{"location":"getting_started/installation/#using-pip","title":"\ud83d\udce6 Using pip","text":"<p>Quick install with pip</p> <pre><code>pip install lg-adk\n</code></pre>"},{"location":"getting_started/installation/#using-poetry","title":"\ud83d\udce6 Using Poetry","text":"<p>Install with Poetry</p> <pre><code>poetry add lg-adk\n</code></pre>"},{"location":"getting_started/installation/#development-installation","title":"\ud83d\udc69\u200d\ud83d\udcbb Development Installation","text":"<p>If you want to contribute to LG-ADK, you can install it in development mode:</p> <pre><code># Clone the repository\ngit clone https://github.com/yourusername/lg-adk.git\ncd lg-adk\n\n# Install dependencies\npoetry install\n\n# Alternatively with pip\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting_started/installation/#setting-up-environment-variables","title":"\u2699\ufe0f Setting Up Environment Variables","text":"<p>LG-ADK uses environment variables for configuration. You can create a <code>.env</code> file in your project directory:</p> <pre><code># .env file\nOPENAI_API_KEY=your_openai_api_key  # If using OpenAI models\nGOOGLE_API_KEY=your_google_api_key  # If using Gemini models\nOLLAMA_BASE_URL=http://localhost:11434  # For local Ollama models\nDEFAULT_LLM=ollama/llama3  # Default model to use\n</code></pre>"},{"location":"getting_started/installation/#verifying-installation","title":"\u2705 Verifying Installation","text":"<p>You can verify your installation by running a simple example:</p> <pre><code>from lg_adk import Agent\n\n# This should print the version number\nprint(f\"LG-ADK version: {lg_adk.__version__}\")\n\n# Create a simple agent\nagent = Agent(\n    name=\"test_agent\",\n    llm=\"ollama/llama3\",\n    description=\"Test agent\"\n)\n</code></pre>"},{"location":"getting_started/installation/#troubleshooting","title":"\ud83d\uded1 Troubleshooting","text":"<p>Common Issues</p> <ul> <li>Import Errors: Make sure you have installed all the required dependencies.</li> <li>Model Connection Errors:</li> <li>For Ollama: Ensure Ollama is running locally (<code>ollama serve</code>).</li> <li>For Gemini/OpenAI: Check your API keys are set correctly.</li> </ul>"},{"location":"getting_started/installation/#getting-help","title":"\ud83d\udca1 Getting Help","text":"<p>If you encounter issues, please:</p> <ol> <li>Check the Troubleshooting Guide</li> <li>Search existing GitHub Issues</li> <li>Open a new issue if needed</li> </ol>"},{"location":"getting_started/quick_start/","title":"\ud83d\udea6 Quick Start Guide","text":"<p>This guide will help you get started with LG-ADK and build your first agent in minutes.</p>"},{"location":"getting_started/quick_start/#installation","title":"\ud83d\udce6 Installation","text":"<p>Install with pip or Poetry</p> <pre><code>pip install lg-adk\n# or\npoetry add lg-adk\n</code></pre>"},{"location":"getting_started/quick_start/#creating-a-simple-agent","title":"\ud83e\udd16 Creating a Simple Agent","text":"<pre><code>from lg_adk import Agent\n\n# Create a simple agent\nagent = Agent(\n    name=\"assistant\",\n    llm=\"ollama/llama3\",  # You can use \"gemini/gemini-pro\" or other models\n    description=\"A helpful assistant that answers questions\"\n)\n\n# Run the agent with a user query\nresult = agent.run({\"input\": \"What is artificial intelligence?\"})\nprint(result[\"output\"])\n</code></pre>"},{"location":"getting_started/quick_start/#building-an-agent-with-a-graph","title":"\ud83d\udd17 Building an Agent with a Graph","text":"<pre><code>from lg_adk import Agent, GraphBuilder\n\n# Create an agent\nagent = Agent(\n    name=\"assistant\",\n    llm=\"ollama/llama3\",\n    description=\"A helpful assistant that answers questions\"\n)\n\n# Create a graph builder\nbuilder = GraphBuilder()\nbuilder.add_agent(agent)\n\n# Build the graph\ngraph = builder.build()\n\n# Run the graph\nresult = graph.invoke({\"input\": \"What is machine learning?\"})\nprint(result[\"output\"])\n</code></pre>"},{"location":"getting_started/quick_start/#creating-a-multi-agent-system","title":"\ud83d\udc65 Creating a Multi-Agent System","text":"<pre><code>from lg_adk import Agent, MultiAgentSystem\n\n# Create a coordinator agent\ncoordinator = Agent(\n    name=\"coordinator\",\n    llm=\"ollama/llama3\",\n    description=\"Coordinates tasks between specialized agents\"\n)\n\n# Create specialized agents\nresearcher = Agent(\n    name=\"researcher\",\n    llm=\"ollama/llama3\",\n    description=\"Researches information and provides detailed answers\"\n)\n\nwriter = Agent(\n    name=\"writer\",\n    llm=\"ollama/llama3\",\n    description=\"Writes concise and clear content\"\n)\n\n# Create a multi-agent system\nsystem = MultiAgentSystem(\n    name=\"research_team\",\n    coordinator=coordinator,\n    agents=[researcher, writer],\n    description=\"A team that researches and writes about topics\"\n)\n\n# Run the system\nresult = system.run({\"input\": \"Explain quantum computing\"})\nprint(result[\"output\"])\n</code></pre>"},{"location":"getting_started/quick_start/#using-different-model-providers","title":"\ud83e\udde0 Using Different Model Providers","text":"<pre><code># Using Ollama (local models)\nlocal_agent = Agent(\n    name=\"local_assistant\",\n    llm=\"ollama/llama3\",\n    description=\"An assistant running on a local model\"\n)\n\n# Using Google's Gemini models\ngemini_agent = Agent(\n    name=\"gemini_assistant\",\n    llm=\"gemini/gemini-pro\",\n    description=\"An assistant powered by Gemini\"\n)\n\n# Using OpenAI models\nopenai_agent = Agent(\n    name=\"openai_assistant\",\n    llm=\"openai/gpt-4\",\n    description=\"An assistant powered by GPT-4\"\n)\n</code></pre>"},{"location":"getting_started/quick_start/#evaluating-your-agent","title":"\ud83d\udcca Evaluating Your Agent","text":"<pre><code>from lg_adk import Agent, EvalDataset, Evaluator\n\n# Create an agent\nagent = Agent(\n    name=\"assistant\",\n    llm=\"ollama/llama3\",\n    description=\"A helpful assistant\"\n)\n\n# Create or load an evaluation dataset\ndataset = EvalDataset(\n    name=\"Simple Questions\",\n    description=\"Basic knowledge questions\",\n    examples=[\n        {\n            \"id\": \"q1\",\n            \"input\": \"What is the capital of France?\",\n            \"expected_output\": \"The capital of France is Paris.\"\n        },\n        {\n            \"id\": \"q2\",\n            \"input\": \"Who wrote Romeo and Juliet?\",\n            \"expected_output\": \"William Shakespeare wrote Romeo and Juliet.\"\n        }\n    ]\n)\n\n# Create an evaluator and run evaluation\nevaluator = Evaluator()\nresults = evaluator.evaluate(agent, dataset)\n\n# Print evaluation results\nprint(f\"Accuracy: {results.metric_scores.get('AccuracyMetric', 0)}\")\nprint(f\"Latency: {results.metric_scores.get('LatencyMetric', 0)} seconds\")\n</code></pre>"},{"location":"getting_started/quick_start/#running-an-interactive-session","title":"\ud83d\udcac Running an Interactive Session","text":"<pre><code># Run an agent interactively\nlg-adk run path/to/your_agent.py\n\n# Evaluate an agent against a dataset\nlg-adk eval path/to/your_agent.py path/to/dataset.json\n\n# Debug an agent visually (requires langgraph-cli)\nlg-adk debug path/to/your_agent.py\n</code></pre>"},{"location":"getting_started/quick_start/#next-steps","title":"\ud83c\udf1f Next Steps","text":"<p>Now that you've seen the basics, check out these resources:</p> <ul> <li>Agent Guide \ud83e\udd16</li> <li>Multi-Agent Systems \ud83d\udc65</li> <li>Examples \ud83d\udca1</li> <li>API Reference \ud83d\udee0\ufe0f</li> </ul>"},{"location":"guides/building_graphs/","title":"\ud83c\udfd7\ufe0f Building Graphs with LG-ADK","text":"<p>Build, compose, and orchestrate powerful agent workflows using LangGraph!</p> <p>What you'll learn</p> <p>This guide walks you through building graphs in LG-ADK, from simple linear flows to advanced multi-agent orchestration. You'll learn how to: - Add agents and tools to a graph - Define node functions and state - Use memory and session management - Integrate with LangGraph CLI for debugging and deployment</p>"},{"location":"guides/building_graphs/#why-use-graphs","title":"\ud83e\udd14 Why Use Graphs?","text":"<p>Graphs let you design flexible, modular agent workflows! \ud83c\udf10</p>"},{"location":"guides/building_graphs/#key-concepts","title":"\ud83e\udde9 Key Concepts","text":"<ul> <li>Nodes: \ud83d\udfe2 Each node is a step in your workflow (an agent, a tool, or a function)</li> <li>Edges: \u27a1\ufe0f Define the flow between nodes</li> <li>State: \ud83d\udce6 Data passed between nodes</li> </ul>"},{"location":"guides/building_graphs/#quick-example","title":"\ud83d\udea6 Quick Example","text":"<p>Minimal graph setup</p> <pre><code>from lg_adk import Agent, GraphBuilder\nagent = Agent(name=\"assistant\", llm=\"ollama/llama3\")\nbuilder = GraphBuilder()\nbuilder.add_agent(agent)\ngraph = builder.build()\nresult = graph.invoke({\"input\": \"Hello!\"})\nprint(result[\"output\"])\n</code></pre>"},{"location":"guides/building_graphs/#adding-nodes","title":"\ud83d\udee0\ufe0f Adding Nodes","text":"<ul> <li>Use <code>add_agent</code>, <code>add_tool</code>, or <code>add_node</code> to add steps to your graph.</li> <li>Each node can be an agent, a function, or a tool.</li> </ul>"},{"location":"guides/building_graphs/#connecting-nodes","title":"\ud83d\udd17 Connecting Nodes","text":"<p>Edges define the flow</p> <p>Use the <code>flow</code> argument in <code>build()</code> to specify the order and branching of nodes.</p>"},{"location":"guides/building_graphs/#state-management","title":"\ud83e\udde0 State Management","text":"<ul> <li>State is a Python dict passed between nodes.</li> <li>You can enrich, modify, or branch on state at each step.</li> </ul>"},{"location":"guides/building_graphs/#common-pitfalls","title":"\ud83d\udea8 Common Pitfalls","text":"<p>Watch out for cycles!</p> <p>Cycles in your graph can cause infinite loops. Always check your flow.</p>"},{"location":"guides/building_graphs/#next-steps","title":"\ud83c\udf1f Next Steps","text":"<ul> <li>Tool Integration \ud83d\udee0\ufe0f</li> <li>Session Management \ud83d\uddc2\ufe0f</li> <li>Examples \ud83d\udca1</li> </ul>"},{"location":"guides/building_graphs/#understanding-graph-architecture","title":"Understanding Graph Architecture","text":"<p>In LG-ADK, a graph is a collection of connected agents and components that work together to accomplish complex tasks. Graphs allow you to:</p> <ol> <li>Connect multiple agents in a workflow</li> <li>Define how information flows between agents</li> <li>Create conditional logic between agent interactions</li> <li>Maintain state throughout the entire process</li> </ol>"},{"location":"guides/building_graphs/#getting-started-with-graphbuilder","title":"Getting Started with GraphBuilder","text":"<p>The <code>GraphBuilder</code> class is your main tool for constructing agent graphs:</p> <pre><code>from lg_adk.builders import GraphBuilder\nfrom lg_adk.agents import Agent\nfrom lg_adk.models import get_model\n\n# Create a new graph builder\nbuilder = GraphBuilder(name=\"simple_graph\")\n</code></pre>"},{"location":"guides/building_graphs/#adding-agents-to-a-graph","title":"Adding Agents to a Graph","text":"<p>You can add pre-configured agents to your graph:</p> <pre><code># Create some agents\nresearcher = Agent(\n    name=\"researcher\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a research agent who finds information on topics.\"\n)\n\nwriter = Agent(\n    name=\"writer\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a writer agent who creates content based on research.\"\n)\n\n# Add agents to the graph\nbuilder.add_agent(researcher)\nbuilder.add_agent(writer)\n</code></pre>"},{"location":"guides/building_graphs/#defining-node-connections","title":"Defining Node Connections","text":"<p>Connect nodes to establish the flow of information:</p> <pre><code># Connect the researcher to the writer\nbuilder.connect(\n    source=\"researcher\",\n    target=\"writer\"\n)\n\n# Connect the writer back to the user (end of the graph)\nbuilder.connect(\n    source=\"writer\",\n    target=\"__end__\"  # Special node that represents the end of the graph\n)\n</code></pre>"},{"location":"guides/building_graphs/#conditional-routing","title":"Conditional Routing","text":"<p>You can create more complex flows with conditional routing:</p> <pre><code># Define a routing function\ndef route_based_on_complexity(state):\n    \"\"\"Route to different agents based on query complexity\"\"\"\n    complexity = state.get(\"complexity\", \"simple\")\n    if complexity == \"complex\":\n        return \"deep_researcher\"\n    else:\n        return \"basic_researcher\"\n\n# Add a conditional branch\nbuilder.add_conditional_edge(\n    source=\"user_input\",\n    condition_function=route_based_on_complexity,\n    targets=[\"basic_researcher\", \"deep_researcher\"]\n)\n</code></pre>"},{"location":"guides/building_graphs/#adding-memory-to-a-graph","title":"Adding Memory to a Graph","text":"<p>Memory allows your graph to maintain context across interactions:</p> <pre><code>from lg_adk.memory import MemoryManager\nfrom lg_adk.database import DatabaseManager\n\n# Create memory manager\nmemory_manager = MemoryManager(\n    database_manager=DatabaseManager(connection_string=\"sqlite:///graph_memory.db\")\n)\n\n# Add memory to the graph\nbuilder.add_memory(memory_manager)\n</code></pre>"},{"location":"guides/building_graphs/#human-in-the-loop-integration","title":"Human-in-the-Loop Integration","text":"<p>For tasks that require human oversight:</p> <pre><code># Enable human-in-the-loop for the entire graph\nbuilder.enable_human_in_the_loop()\n\n# Or for specific transitions\nbuilder.enable_human_in_the_loop(\n    source=\"critical_decision\",\n    target=\"high_impact_action\"\n)\n</code></pre>"},{"location":"guides/building_graphs/#building-and-running-the-graph","title":"Building and Running the Graph","text":"<p>Once you've configured your graph, build and run it:</p> <pre><code># Build the graph\ngraph = builder.build()\n\n# Run the graph with an initial input\nresult = graph.run(\"Research the latest advancements in quantum computing and write a summary.\")\nprint(result)\n</code></pre>"},{"location":"guides/building_graphs/#streaming-graph-output","title":"Streaming Graph Output","text":"<p>For long-running processes, you might want to stream the results:</p> <pre><code># Enable streaming for the graph\ngraph = builder.build(stream=True)\n\n# Stream the results\nfor chunk in graph.stream(\"Tell me about artificial intelligence.\"):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"guides/building_graphs/#asynchronous-graph-execution","title":"Asynchronous Graph Execution","text":"<p>For non-blocking operation:</p> <pre><code>import asyncio\n\nasync def main():\n    # Build the graph with async support\n    graph = builder.build()\n\n    # Run the graph asynchronously\n    result = await graph.arun(\"Analyze the current trends in renewable energy.\")\n    print(result)\n\n    # Stream results asynchronously\n    async for chunk in graph.astream(\"Explain machine learning concepts.\"):\n        print(chunk, end=\"\", flush=True)\n\n# Run the async function\nasyncio.run(main())\n</code></pre>"},{"location":"guides/building_graphs/#advanced-graph-patterns","title":"Advanced Graph Patterns","text":""},{"location":"guides/building_graphs/#parallel-processing","title":"Parallel Processing","text":"<p>Execute multiple agents in parallel:</p> <pre><code># Create a parallel processing section\nbuilder.add_parallel_nodes(\n    [\"market_researcher\", \"technical_researcher\", \"social_researcher\"]\n)\n\n# Add a node to combine the parallel results\nbuilder.add_agent(combiner)\n\n# Connect the parallel nodes to the combiner\nfor node in [\"market_researcher\", \"technical_researcher\", \"social_researcher\"]:\n    builder.connect(source=node, target=\"combiner\")\n</code></pre>"},{"location":"guides/building_graphs/#iterative-refinement","title":"Iterative Refinement","text":"<p>Create loops for iterative improvement:</p> <pre><code># Create a drafting loop\nbuilder.connect(source=\"writer\", target=\"editor\")\nbuilder.connect(source=\"editor\", target=\"quality_check\")\n\n# Add a conditional to either continue refining or finish\ndef is_quality_sufficient(state):\n    quality_score = state.get(\"quality_score\", 0)\n    if quality_score &gt;= 8:\n        return \"complete\"\n    else:\n        return \"writer\"  # Back to writer for another draft\n\nbuilder.add_conditional_edge(\n    source=\"quality_check\",\n    condition_function=is_quality_sufficient,\n    targets=[\"writer\", \"complete\"]\n)\n</code></pre>"},{"location":"guides/building_graphs/#example-complete-research-assistant-graph","title":"Example: Complete Research Assistant Graph","text":"<p>Here's a complete example of a research assistant graph:</p> <pre><code>from lg_adk.builders import GraphBuilder\nfrom lg_adk.agents import Agent\nfrom lg_adk.models import get_model\nfrom lg_adk.tools import WebSearchTool\nfrom lg_adk.memory import MemoryManager\nfrom lg_adk.database import DatabaseManager\n\n# Setup agents\nresearcher = Agent(\n    name=\"researcher\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a research agent. Your job is to find information about a given topic.\n    Use your web search tool to gather relevant information.\"\"\",\n    tools=[WebSearchTool()]\n)\n\nanalyzer = Agent(\n    name=\"analyzer\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are an analysis agent. Your job is to analyze the research provided\n    and extract key insights. Focus on what's most important and relevant.\"\"\"\n)\n\nwriter = Agent(\n    name=\"writer\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a content writing agent. Your job is to take analyzed research\n    and create a well-structured, informative article about the topic.\"\"\"\n)\n\n# Setup memory\nmemory_manager = MemoryManager(\n    database_manager=DatabaseManager(connection_string=\"sqlite:///research_graph.db\")\n)\n\n# Create graph builder\nbuilder = GraphBuilder(name=\"research_assistant\")\n\n# Add agents\nbuilder.add_agent(researcher)\nbuilder.add_agent(analyzer)\nbuilder.add_agent(writer)\n\n# Add memory\nbuilder.add_memory(memory_manager)\n\n# Connect agents\nbuilder.connect(source=\"__start__\", target=\"researcher\")  # Start with researcher\nbuilder.connect(source=\"researcher\", target=\"analyzer\")   # Send research to analyzer\nbuilder.connect(source=\"analyzer\", target=\"writer\")       # Send analysis to writer\nbuilder.connect(source=\"writer\", target=\"__end__\")        # End with writer's output\n\n# Build the graph\ngraph = builder.build()\n\n# Run the graph\nresult = graph.run(\"What are the latest advancements in fusion energy research?\")\nprint(result)\n</code></pre>"},{"location":"guides/building_graphs/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Plan Your Graph: Sketch out the flow before implementation to visualize node relationships.</p> </li> <li> <p>Name Agents Clearly: Use descriptive names that indicate function.</p> </li> <li> <p>Keep Prompts Focused: Each agent should have a clear, specific role.</p> </li> <li> <p>Test Incremental Builds: Add and test a few nodes at a time rather than building the entire graph at once.</p> </li> <li> <p>Use Conditional Routing: Leverage conditional logic to create more adaptive workflows.</p> </li> <li> <p>Manage State Carefully: Be mindful of what information needs to be passed between nodes.</p> </li> <li> <p>Include Error Handling: Add error handling nodes or conditional paths for unexpected scenarios.</p> </li> <li> <p>Monitor Performance: Track execution time and success rates to optimize your graph.</p> </li> </ol> <p>With these techniques, you can build powerful, multi-agent systems that can tackle complex, multi-stage tasks with LG-ADK. For more details on specific components, refer to the Creating Agents, Tool Integration, and Memory Management guides.</p>"},{"location":"guides/building_graphs/#see-also","title":"\ud83d\udd17 See Also","text":"<ul> <li>\ud83e\udd16 Creating Agents</li> <li>\ud83d\udc65 Multi-Agent Guide</li> <li>\ud83d\udd0d RAG Guide</li> <li>\ud83d\uddc2\ufe0f Session Management Guide</li> <li>\ud83d\udca1 Examples Index</li> </ul>"},{"location":"guides/creating_agents/","title":"Creating Agents with LG-ADK","text":"<p>This guide covers how to create agents using the LangGraph Agent Development Kit (LG-ADK).</p>"},{"location":"guides/creating_agents/#agent-fundamentals","title":"Agent Fundamentals","text":"<p>An agent in LG-ADK is an entity that can: - Process and understand natural language - Make decisions based on provided information - Use tools to perform actions - Maintain state across interactions</p>"},{"location":"guides/creating_agents/#basic-agent-creation","title":"Basic Agent Creation","text":""},{"location":"guides/creating_agents/#step-1-import-the-required-classes","title":"Step 1: Import the Required Classes","text":"<pre><code>from lg_adk.agents import Agent\nfrom lg_adk.models import get_model\n</code></pre>"},{"location":"guides/creating_agents/#step-2-initialize-your-agent","title":"Step 2: Initialize Your Agent","text":"<pre><code># Create a simple agent\nagent = Agent(\n    name=\"assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a helpful AI assistant that answers user questions.\"\n)\n</code></pre> <p>The basic parameters for creating an agent are: - <code>name</code>: A unique identifier for your agent - <code>model</code>: The language model the agent will use (from the model registry) - <code>system_prompt</code>: Instructions that define the agent's behavior and capabilities</p>"},{"location":"guides/creating_agents/#step-3-using-your-agent","title":"Step 3: Using Your Agent","text":"<pre><code># Run the agent with a user input\nresponse = agent.run(\"What is the capital of France?\")\nprint(response)\n</code></pre>"},{"location":"guides/creating_agents/#advanced-agent-configuration","title":"Advanced Agent Configuration","text":"<p>You can configure more advanced settings for your agents:</p> <pre><code>from lg_adk.tools import WebSearchTool\nfrom lg_adk.memory import MemoryManager\nfrom lg_adk.database import DatabaseManager\n\n# Create a more advanced agent\nadvanced_agent = Agent(\n    name=\"research_assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a research assistant that helps users find and summarize information.\n    Use your tools when appropriate to provide accurate and up-to-date information.\"\"\",\n    tools=[WebSearchTool()],\n    memory_manager=MemoryManager(\n        database_manager=DatabaseManager(connection_string=\"sqlite:///memory.db\")\n    ),\n    max_tokens=1024,\n    temperature=0.7\n)\n</code></pre>"},{"location":"guides/creating_agents/#agent-with-custom-behavior","title":"Agent with Custom Behavior","text":"<p>You can customize how your agent processes inputs and outputs:</p> <pre><code>class CustomAgent(Agent):\n    def preprocess_input(self, user_input: str) -&gt; str:\n        \"\"\"Customize how user input is processed before being sent to the model\"\"\"\n        # Add a timestamp to each input\n        import datetime\n        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        return f\"[{timestamp}] User said: {user_input}\"\n\n    def postprocess_output(self, model_output: str) -&gt; str:\n        \"\"\"Customize how model output is processed before being returned to the user\"\"\"\n        # Add a signature to each response\n        return f\"{model_output}\\n\\n- CustomAgent\"\n\n# Initialize the custom agent\ncustom_agent = CustomAgent(\n    name=\"custom_assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a helpful assistant with a custom personality.\"\n)\n</code></pre>"},{"location":"guides/creating_agents/#streaming-responses","title":"Streaming Responses","text":"<p>LG-ADK supports streaming responses from your agent:</p> <pre><code># Initialize an agent with streaming enabled\nstreaming_agent = Agent(\n    name=\"streaming_assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a helpful AI assistant.\",\n    stream=True\n)\n\n# Process a streaming response\nfor chunk in streaming_agent.stream(\"Tell me a short story about a robot.\"):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"guides/creating_agents/#asynchronous-agents","title":"Asynchronous Agents","text":"<p>For applications requiring non-blocking operation, use async versions:</p> <pre><code>import asyncio\n\nasync def main():\n    # Initialize an async agent\n    agent = Agent(\n        name=\"async_assistant\",\n        model=get_model(\"openai/gpt-4\"),\n        system_prompt=\"You are a helpful AI assistant.\"\n    )\n\n    # Run the agent asynchronously\n    response = await agent.arun(\"What is the meaning of life?\")\n    print(response)\n\n    # Stream responses asynchronously\n    async for chunk in agent.astream(\"Tell me about quantum computing.\"):\n        print(chunk, end=\"\", flush=True)\n\n# Run the async function\nasyncio.run(main())\n</code></pre>"},{"location":"guides/creating_agents/#agent-with-multiple-models","title":"Agent with Multiple Models","text":"<p>You can create agents that can switch between different models:</p> <pre><code>from lg_adk.models import ModelRegistry\n\n# Register multiple models\nregistry = ModelRegistry()\nregistry.register(\"default\", \"openai/gpt-4\")\nregistry.register(\"fast\", \"openai/gpt-3.5-turbo\")\nregistry.register(\"local\", \"ollama/llama2\")\n\n# Create an agent that can switch models\nmulti_model_agent = Agent(\n    name=\"adaptive_assistant\",\n    model=registry.get(\"default\"),\n    system_prompt=\"You are a helpful assistant that adapts to user needs.\"\n)\n\n# Later, switch to a different model\nmulti_model_agent.model = registry.get(\"fast\")\nresponse = multi_model_agent.run(\"Give me a quick answer!\")\n</code></pre>"},{"location":"guides/creating_agents/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Clear System Prompts: Define precisely what your agent should and shouldn't do.</p> </li> <li> <p>Appropriate Tools: Only give your agent the tools it needs for its specific tasks.</p> </li> <li> <p>Memory Management: Configure memory appropriately for your use case.</p> </li> <li> <p>Error Handling: Implement proper error handling, especially for tool usage.</p> </li> <li> <p>Testing: Test your agents thoroughly with different inputs to ensure they behave as expected.</p> </li> </ol>"},{"location":"guides/creating_agents/#example-complete-agent-setup","title":"Example: Complete Agent Setup","text":"<p>Here's a complete example that brings together various concepts:</p> <pre><code>from lg_adk.agents import Agent\nfrom lg_adk.models import get_model\nfrom lg_adk.tools import WebSearchTool, CalculatorTool\nfrom lg_adk.memory import MemoryManager\nfrom lg_adk.database import DatabaseManager\n\n# Setup tools\ntools = [\n    WebSearchTool(),\n    CalculatorTool()\n]\n\n# Setup memory\nmemory_manager = MemoryManager(\n    database_manager=DatabaseManager(connection_string=\"sqlite:///agent_memory.db\")\n)\n\n# Create the agent\nresearch_agent = Agent(\n    name=\"research_assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a research assistant with these capabilities:\n    1. You can search the web for current information\n    2. You can perform calculations\n    3. You remember previous conversations with the user\n\n    Always be helpful, accurate, and concise in your responses.\n    When you don't know something, use your tools rather than guessing.\n    \"\"\",\n    tools=tools,\n    memory_manager=memory_manager,\n    temperature=0.2,\n    max_tokens=1024\n)\n\n# Use the agent\nresponse = research_agent.run(\"What was the population of Tokyo in 2022, and what percentage of Japan's total population does that represent?\")\nprint(response)\n</code></pre> <p>With this guide, you should now have a good understanding of how to create and configure agents using LG-ADK. For more advanced use cases, see the related guides on Building Graphs, Tool Integration, and Memory Management.</p>"},{"location":"guides/human_in_the_loop/","title":"\ud83e\uddd1\u200d\ud83d\udcbb Human-in-the-Loop in LG-ADK","text":""},{"location":"guides/human_in_the_loop/#why-add-human-in-the-loop","title":"\ud83e\udd14 Why Add Human-in-the-Loop?","text":"<p>Sometimes, only a human can make the right call! Add review, approval, or intervention steps to your agent workflows. \ud83d\udc40</p>"},{"location":"guides/human_in_the_loop/#where-to-use-human-in-the-loop","title":"\ud83e\udde9 Where to Use Human-in-the-Loop","text":"<ul> <li>\u2705 Critical Decisions: Approve or reject important actions</li> <li>\ud83d\udcdd Content Review: Check generated text before sending</li> <li>\ud83e\uddd1\u200d\u2696\ufe0f Escalation: Route to a human when the agent is unsure</li> <li>\ud83d\uded1 Safety: Prevent unsafe or unwanted outputs</li> </ul>"},{"location":"guides/human_in_the_loop/#quick-example","title":"\ud83d\udea6 Quick Example","text":"<p>Enable human-in-the-loop for a node</p> <pre><code>builder.enable_human_in_the_loop(source=\"review\", target=\"publish\")\n</code></pre>"},{"location":"guides/human_in_the_loop/#how-it-works","title":"\ud83d\udee0\ufe0f How It Works","text":"<ul> <li>Insert human-in-the-loop nodes anywhere in your graph</li> <li>The workflow pauses and waits for human input at these nodes</li> <li>You can customize prompts and review logic</li> </ul>"},{"location":"guides/human_in_the_loop/#common-pitfalls","title":"\ud83d\udea8 Common Pitfalls","text":"<p>Workflow stalls</p> <p>Make sure you have a way to notify or alert humans when their input is needed, or the workflow may pause indefinitely.</p>"},{"location":"guides/human_in_the_loop/#next-steps","title":"\ud83c\udf1f Next Steps","text":"<ul> <li>Building Graphs \ud83c\udfd7\ufe0f</li> <li>Session Management \ud83d\uddc2\ufe0f</li> <li>Examples \ud83d\udca1</li> </ul>"},{"location":"guides/human_in_the_loop/#human-in-the-loop-with-lg-adk","title":"Human-in-the-Loop with LG-ADK","text":"<p>This guide explains how to implement human-in-the-loop interactions in the LangGraph Agent Development Kit, allowing agents to request human input during their processing.</p>"},{"location":"guides/human_in_the_loop/#understanding-human-in-the-loop","title":"Understanding Human-in-the-Loop","text":"<p>Human-in-the-loop (HITL) enables:</p> <ol> <li>Agents to request human input or clarification</li> <li>Humans to review and approve agent actions</li> <li>Collaborative problem-solving between humans and agents</li> <li>Building systems with appropriate human oversight</li> </ol>"},{"location":"guides/human_in_the_loop/#core-components","title":"Core Components","text":"<p>The HITL system in LG-ADK consists of:</p> <ul> <li>HumanInputNode: A specialized graph node that pauses for human input</li> <li>HumanManager: Manages the interaction between agents and humans</li> <li>GraphBuilder HITL methods: Helper methods to enable HITL in your graphs</li> </ul>"},{"location":"guides/human_in_the_loop/#basic-implementation","title":"Basic Implementation","text":"<p>To enable human-in-the-loop in your graph:</p> <pre><code>from lg_adk.builders import GraphBuilder\nfrom lg_adk.agents import Agent\nfrom lg_adk.models import get_model\nfrom lg_adk.human import HumanManager\n\n# Create an agent\nagent = Agent(\n    name=\"assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a helpful assistant that knows when to ask humans for help.\"\n)\n\n# Create a human manager\nhuman_manager = HumanManager()\n\n# Create a graph with human-in-the-loop\nbuilder = GraphBuilder()\nbuilder.add_agent(agent)\nbuilder.enable_human_in_the_loop(human_manager)\n\n# Build the graph\ngraph = builder.build()\n</code></pre>"},{"location":"guides/human_in_the_loop/#running-a-graph-with-human-input","title":"Running a Graph with Human Input","text":"<p>When running a graph with HITL enabled:</p> <pre><code># Start a conversation that might require human input\nresult = graph.run(\"What's the best approach for our new marketing campaign?\")\n\n# Check if human input is required\nif result.state.get(\"requires_human_input\", False):\n    # Provide human input\n    human_response = input(\"Human response: \")\n\n    # Continue the conversation with human input\n    updated_result = graph.continue_run(\n        state=result.state,\n        human_input=human_response\n    )\n</code></pre>"},{"location":"guides/human_in_the_loop/#agent-initiated-human-input","title":"Agent-Initiated Human Input","text":"<p>You can configure your agent to recognize when it should request human input:</p> <pre><code>agent = Agent(\n    name=\"customer_service\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a customer service agent.\n\n    If the customer request involves:\n    - High-value refunds (over $100)\n    - Account closures\n    - Complaints requiring escalation\n\n    Ask for human supervisor input by saying \"I need to consult with a supervisor on this.\"\n    Otherwise, handle the request yourself.\"\"\"\n)\n</code></pre> <p>The HumanManager will automatically detect this pattern and pause for human input.</p>"},{"location":"guides/human_in_the_loop/#configuring-human-intervention-triggers","title":"Configuring Human Intervention Triggers","text":"<p>You can customize when the system should pause for human input:</p> <pre><code>from lg_adk.human import HumanManager\n\n# Create a human manager with custom triggers\nhuman_manager = HumanManager(\n    trigger_phrases=[\n        \"I need human input\",\n        \"Please ask the user\",\n        \"Human assistance required\"\n    ],\n    # Also trigger on specific actions or keywords\n    trigger_keywords=[\"refund\", \"escalate\", \"override\"],\n    # Require approval for high-risk actions\n    require_approval_for=[\"database_update\", \"payment_processing\"]\n)\n</code></pre>"},{"location":"guides/human_in_the_loop/#approval-workflows","title":"Approval Workflows","text":"<p>You can implement approval workflows for critical agent actions:</p> <pre><code># Create an agent that proposes actions requiring approval\nagent = Agent(\n    name=\"finance_assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a finance assistant.\n\n    For any transaction over $1000, create a PROPOSED_ACTION that\n    requires human approval before proceeding.\"\"\"\n)\n\n# Configure human manager for approvals\nhuman_manager = HumanManager(\n    approval_required=True,\n    approval_prompt=\"A financial action requires your approval. Do you approve? (yes/no)\"\n)\n\n# In the graph runner\nresult = graph.run(\"Please transfer $5000 to account #12345\")\n\nif result.state.get(\"requires_approval\", False):\n    # Display the proposed action to the human\n    print(f\"Proposed action: {result.state['proposed_action']}\")\n\n    # Get approval decision\n    approval = input(\"Do you approve? (yes/no): \")\n\n    # Continue with the approval decision\n    updated_result = graph.continue_run(\n        state=result.state,\n        human_input={\"approval\": approval == \"yes\"}\n    )\n</code></pre>"},{"location":"guides/human_in_the_loop/#multiple-human-input-points","title":"Multiple Human Input Points","text":"<p>You can configure a graph to pause for human input at multiple points:</p> <pre><code>from lg_adk.builders import GraphBuilder\nfrom lg_adk.agents import Agent\nfrom lg_adk.human import HumanManager, HumanInputNode\n\n# Create agents\nplanner = Agent(\n    name=\"planner\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You create plans based on user requests.\"\n)\n\nexecutor = Agent(\n    name=\"executor\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You execute plans after they've been approved.\"\n)\n\n# Create human input nodes\nplan_approval = HumanInputNode(\n    name=\"plan_approval\",\n    prompt=\"Please review the proposed plan. Do you approve it or want changes?\"\n)\n\nexecution_confirmation = HumanInputNode(\n    name=\"execution_confirmation\",\n    prompt=\"Execution is about to begin. Final confirmation?\"\n)\n\n# Build graph with multiple human input points\nbuilder = GraphBuilder()\nbuilder.add_agent(planner)\nbuilder.add_agent(executor)\nbuilder.add_human_node(plan_approval)\nbuilder.add_human_node(execution_confirmation)\n\n# Connect the components\nbuilder.add_edge(planner, plan_approval)\nbuilder.add_edge(plan_approval, executor)\nbuilder.add_edge(executor, execution_confirmation)\n\n# Build the graph\ngraph = builder.build()\n</code></pre>"},{"location":"guides/human_in_the_loop/#human-in-the-loop-with-streaming","title":"Human-in-the-Loop with Streaming","text":"<p>For a more interactive experience, you can use streaming with HITL:</p> <pre><code># Start a streaming run that might require human input\nfor chunk in graph.stream(\"What's your recommendation for our product roadmap?\"):\n    if chunk.get(\"requires_human_input\", False):\n        # Get human input\n        human_response = input(\"Human input required: \")\n\n        # Continue the stream with human input\n        for updated_chunk in graph.continue_stream(\n            state=chunk,\n            human_input=human_response\n        ):\n            print(updated_chunk.get(\"agent_response\", \"\"))\n    else:\n        print(chunk.get(\"agent_response\", \"\"), end=\"\", flush=True)\n</code></pre>"},{"location":"guides/human_in_the_loop/#configuring-human-input-timeouts","title":"Configuring Human Input Timeouts","text":"<p>You can configure timeouts for human input:</p> <pre><code>from lg_adk.human import HumanManager\n\n# Configure timeouts\nhuman_manager = HumanManager(\n    input_timeout=300,  # 5 minutes in seconds\n    fallback_response=\"No human input received within the timeout period. I'll proceed with the best course of action.\"\n)\n</code></pre> <p>If no human input is received within the timeout, the system will use the fallback response.</p>"},{"location":"guides/human_in_the_loop/#custom-input-and-output-interfaces","title":"Custom Input and Output Interfaces","text":"<p>You can customize how human input is collected and presented:</p> <pre><code>from lg_adk.human import HumanManager, InputInterface, OutputInterface\n\n# Custom input interface (e.g., from a web form)\nclass WebFormInput(InputInterface):\n    def get_input(self, prompt, state):\n        # Code to display prompt in web UI and collect input\n        # This is a placeholder implementation\n        return web_form_submit_value\n\n# Custom output interface (e.g., to a chat UI)\nclass ChatUIOutput(OutputInterface):\n    def display_output(self, output, state):\n        # Code to display output in chat UI\n        # This is a placeholder implementation\n        chat_ui.add_message(output)\n\n# Use custom interfaces\nhuman_manager = HumanManager(\n    input_interface=WebFormInput(),\n    output_interface=ChatUIOutput()\n)\n</code></pre>"},{"location":"guides/human_in_the_loop/#asynchronous-human-input","title":"Asynchronous Human Input","text":"<p>For web applications or services, you can use asynchronous human input:</p> <pre><code>import asyncio\nfrom lg_adk.human import AsyncHumanManager\n\n# Create an async human manager\nasync_human_manager = AsyncHumanManager()\n\n# In an async context\nasync def process_request(user_input, user_id):\n    # Start a graph run\n    result = await graph.arun(user_input, session_id=user_id)\n\n    if result.state.get(\"requires_human_input\", False):\n        # Store the state waiting for human input\n        await async_human_manager.store_pending_state(user_id, result.state)\n        # Return indication that human input is needed\n        return {\"status\": \"waiting_for_human_input\"}\n\n    return {\"status\": \"complete\", \"response\": result.response}\n\n# When human input arrives later\nasync def process_human_input(user_id, human_input):\n    # Retrieve the pending state\n    pending_state = await async_human_manager.get_pending_state(user_id)\n\n    if pending_state:\n        # Continue the run with the human input\n        updated_result = await graph.acontinue_run(\n            state=pending_state,\n            human_input=human_input\n        )\n\n        # Clear the pending state\n        await async_human_manager.clear_pending_state(user_id)\n\n        return {\"status\": \"complete\", \"response\": updated_result.response}\n\n    return {\"status\": \"error\", \"message\": \"No pending state found\"}\n</code></pre>"},{"location":"guides/human_in_the_loop/#human-feedback-collection","title":"Human Feedback Collection","text":"<p>You can use HITL to collect and incorporate human feedback:</p> <pre><code>from lg_adk.human import HumanFeedbackNode\n\n# Create a feedback node\nfeedback_node = HumanFeedbackNode(\n    name=\"result_feedback\",\n    prompt=\"How satisfied are you with this response? (1-5)\",\n    feedback_options=[\"1\", \"2\", \"3\", \"4\", \"5\"]\n)\n\n# Add to graph\nbuilder = GraphBuilder()\nbuilder.add_agent(agent)\nbuilder.add_human_node(feedback_node)\nbuilder.add_edge(agent, feedback_node)\n\n# Process feedback\ndef handle_feedback(feedback_value, session_id):\n    # Store feedback for later analysis\n    feedback_store.add({\n        \"session_id\": session_id,\n        \"rating\": feedback_value,\n        \"timestamp\": datetime.now()\n    })\n\n    # For low ratings, trigger review\n    if int(feedback_value) &lt;= 2:\n        trigger_human_review(session_id)\n</code></pre>"},{"location":"guides/human_in_the_loop/#complete-example-customer-support-with-human-escalation","title":"Complete Example: Customer Support with Human Escalation","text":"<p>Here's a complete example of a customer support system with HITL:</p> <pre><code>from lg_adk.agents import Agent\nfrom lg_adk.builders import GraphBuilder\nfrom lg_adk.models import get_model\nfrom lg_adk.human import HumanManager, HumanInputNode\nfrom lg_adk.tools import MemoryTool\nfrom lg_adk.memory import MemoryManager\nfrom lg_adk.database import DatabaseManager\n\n# Set up memory\ndb_manager = DatabaseManager(connection_string=\"sqlite:///customer_support.db\")\nmemory_manager = MemoryManager(database_manager=db_manager)\nmemory_tool = MemoryTool(memory_manager=memory_manager)\n\n# Create tier-1 support agent\ntier1_agent = Agent(\n    name=\"tier1_support\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a Tier 1 customer support agent.\n\n    Handle basic customer inquiries about:\n    - Account information\n    - Basic troubleshooting\n    - Product information\n\n    If the customer issue involves:\n    - Technical problems you can't solve\n    - Billing disputes\n    - Complaints about service\n\n    Say \"This requires escalation to a specialist.\" to trigger escalation.\"\"\",\n    tools=[memory_tool]\n)\n\n# Create tier-2 support agent\ntier2_agent = Agent(\n    name=\"tier2_support\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a Tier 2 specialist with advanced knowledge.\n\n    Review the conversation history and provide expert assistance.\n    If you need additional customer information, ask for it.\n\n    If the issue is beyond your scope, say \"This requires human manager review.\".\"\"\",\n    tools=[memory_tool]\n)\n\n# Create human input nodes\nhuman_specialist = HumanInputNode(\n    name=\"human_specialist\",\n    prompt=\"This customer issue requires human specialist review. Please provide guidance:\"\n)\n\n# Create human manager\nhuman_manager = HumanManager(\n    trigger_phrases=[\"This requires escalation to a specialist.\",\n                     \"This requires human manager review.\"]\n)\n\n# Build the graph\nbuilder = GraphBuilder()\nbuilder.add_agent(tier1_agent)\nbuilder.add_agent(tier2_agent)\nbuilder.add_human_node(human_specialist)\nbuilder.enable_human_in_the_loop(human_manager)\n\n# Connect the components\nbuilder.add_edge(tier1_agent, tier2_agent,\n                 condition=lambda state: \"requires escalation\" in state.get(\"agent_response\", \"\").lower())\nbuilder.add_edge(tier2_agent, human_specialist,\n                 condition=lambda state: \"requires human manager review\" in state.get(\"agent_response\", \"\").lower())\n\n# Build the graph\nsupport_graph = builder.build()\n\n# Example usage\nsession_id = \"customer_123\"\n\n# Initial customer inquiry\nresult = support_graph.run(\"I'm having trouble with my billing. My last invoice shows charges for services I didn't use.\",\n                        session_id=session_id)\n\n# If human input is needed at any point\nif result.state.get(\"requires_human_input\", False):\n    # Get input from human specialist\n    specialist_input = input(\"Specialist input: \")\n\n    # Continue with the human input\n    updated_result = support_graph.continue_run(\n        state=result.state,\n        human_input=specialist_input\n    )\n\n    print(\"Final response:\", updated_result.response)\nelse:\n    print(\"Response:\", result.response)\n</code></pre>"},{"location":"guides/human_in_the_loop/#best-practices-for-human-in-the-loop","title":"Best Practices for Human-in-the-Loop","text":"<ol> <li>Clear Prompting: Ensure agents have clear instructions about when to request human input</li> <li>Meaningful Context: Provide humans with sufficient context to make informed decisions</li> <li>Appropriate Timeouts: Set reasonable timeouts based on the urgency of the task</li> <li>Fallback Mechanisms: Implement fallback behaviors when human input isn't available</li> <li>User Experience: Design the human interaction to be intuitive and unobtrusive</li> <li>Feedback Loop: Collect data on which interactions require human intervention to improve the system</li> <li>Progressive Autonomy: Start with more human oversight and gradually reduce it as the system proves reliable</li> </ol> <p>By implementing human-in-the-loop functionality with LG-ADK, you can build AI systems that combine the strengths of automated agents with human judgment and expertise. For more information on related topics, see the Building Graphs, Creating Agents, and Tool Integration guides.</p>"},{"location":"guides/langgraph_cli_integration/","title":"\ud83d\uddbc\ufe0f LangGraph CLI Integration","text":""},{"location":"guides/langgraph_cli_integration/#why-use-langgraph-cli","title":"\ud83e\udd14 Why Use langgraph-cli?","text":"<p>The CLI lets you develop, debug, and visualize agent graphs interactively! \ud83d\udda5\ufe0f</p>"},{"location":"guides/langgraph_cli_integration/#quick-start","title":"\ud83d\udea6 Quick Start","text":"<p>Install langgraph-cli</p> <pre><code>pip install langgraph-cli\n</code></pre>"},{"location":"guides/langgraph_cli_integration/#key-features","title":"\ud83d\udee0\ufe0f Key Features","text":"<ul> <li>\ud83e\uddd1\u200d\ud83d\udcbb Interactive Debugging: Step through agent workflows</li> <li>\ud83d\uddbc\ufe0f Graph Visualization: See your agent graph structure</li> <li>\ud83d\udcdd Session Inspection: View and manage session state</li> <li>\ud83d\ude80 Deployment: Run and test agents locally or in the cloud</li> </ul>"},{"location":"guides/langgraph_cli_integration/#common-commands","title":"\ud83d\udd17 Common Commands","text":"<ul> <li><code>langgraph dev</code> \u2014 Start development mode</li> <li><code>langgraph debug</code> \u2014 Visualize and debug your graph</li> <li><code>langgraph run</code> \u2014 Run your agent or graph</li> <li><code>langgraph deploy</code> \u2014 Deploy to self-hosted or cloud</li> </ul>"},{"location":"guides/langgraph_cli_integration/#common-pitfalls","title":"\ud83d\udea8 Common Pitfalls","text":"<p>Missing langgraph.json</p> <p>Make sure your example or app directory contains a valid <code>langgraph.json</code> for CLI features.</p>"},{"location":"guides/langgraph_cli_integration/#next-steps","title":"\ud83c\udf1f Next Steps","text":"<ul> <li>Building Graphs \ud83c\udfd7\ufe0f</li> <li>Examples \ud83d\udca1</li> </ul>"},{"location":"guides/langgraph_cli_integration/#understanding-langgraph-cli","title":"Understanding LangGraph CLI","text":"<p>The LangGraph CLI provides several useful commands for working with LangGraph applications:</p> <ul> <li><code>langgraph dev</code>: Start a development server for testing your graphs</li> <li><code>langgraph serve</code>: Serve your graphs in production</li> <li><code>langgraph deploy</code>: Deploy your graphs to the cloud</li> <li><code>langgraph list</code>: List available graphs in your project</li> </ul>"},{"location":"guides/langgraph_cli_integration/#project-configuration","title":"Project Configuration","text":"<p>To use the LangGraph CLI with your LG-ADK project, you need a proper configuration file:</p>"},{"location":"guides/langgraph_cli_integration/#the-langgraphjson-file","title":"The <code>langgraph.json</code> File","text":"<p>Create a <code>langgraph.json</code> file in your project root:</p> <pre><code>{\n  \"graphs\": {\n    \"chat\": \"lg_adk.graphs.chat:graph\",\n    \"rag\": \"lg_adk.graphs.rag:graph\",\n    \"multi_agent\": \"lg_adk.graphs.multi_agent:graph\"\n  }\n}\n</code></pre> <p>Each entry specifies: - A name for the graph (e.g., \"chat\") - The import path in the format: <code>module.path:graph_variable_name</code></p>"},{"location":"guides/langgraph_cli_integration/#structuring-your-graphs-for-discovery","title":"Structuring Your Graphs for Discovery","text":"<p>To make your graphs discoverable by the LangGraph CLI, follow these conventions:</p>"},{"location":"guides/langgraph_cli_integration/#1-export-graph-variables","title":"1. Export Graph Variables","text":"<p>When defining graphs, export them as top-level variables:</p> <pre><code># In lg_adk/graphs/chat.py\nfrom langgraph.graph import Graph\nfrom lg_adk.agents import Agent\nfrom lg_adk.builders import GraphBuilder\n\n# Create and build the graph\nbuilder = GraphBuilder(name=\"chat\")\nbuilder.add_agent(Agent(name=\"assistant\", model=\"openai/gpt-4\"))\ngraph = builder.build()  # This is the variable the CLI will look for\n</code></pre>"},{"location":"guides/langgraph_cli_integration/#2-use-type-annotations","title":"2. Use Type Annotations","text":"<p>Properly type your graph for better tooling support:</p> <pre><code>from langgraph.graph import Graph\nfrom typing import Dict, Any, TypedDict\n\nclass ChatState(TypedDict):\n    messages: list\n    session_id: str\n\n# Create typed graph\ngraph: Graph[ChatState] = builder.build()\n</code></pre>"},{"location":"guides/langgraph_cli_integration/#session-management-with-langgraph-cli","title":"Session Management with LangGraph CLI","text":"<p>Proper session handling is crucial for multi-turn conversations with LangGraph CLI:</p>"},{"location":"guides/langgraph_cli_integration/#implementing-session-aware-graphs","title":"Implementing Session-Aware Graphs","text":"<p>Here's how to create graphs that correctly handle session state:</p> <pre><code>from langgraph.graph import Graph\nfrom typing import Dict, Any, TypedDict\nfrom lg_adk.agents import Agent\nfrom lg_adk.builders import GraphBuilder\n\n# Define state type\nclass GraphState(TypedDict):\n    messages: list\n    session_id: str\n    metadata: Dict[str, Any]\n\ndef build_graph():\n    # Create an agent\n    agent = Agent(\n        name=\"assistant\",\n        model=\"openai/gpt-4\",\n        system_prompt=\"You are a helpful assistant.\"\n    )\n\n    # Create graph builder\n    builder = GraphBuilder(name=\"chat\")\n    builder.add_agent(agent)\n\n    # Configure to track session in state\n    builder.configure_state_tracking(\n        include_session_id=True,\n        include_metadata=True\n    )\n\n    # Build the graph\n    return builder.build()\n\n# Export the graph for LangGraph CLI\ngraph = build_graph()\n</code></pre>"},{"location":"guides/langgraph_cli_integration/#correct-context-handling","title":"Correct Context Handling","text":"<p>Ensure your agent functions properly handle the checkpointer state:</p> <pre><code>from langgraph.checkpoint.base import Checkpointer\nfrom lg_adk.agents import Agent\nfrom typing import Dict, Any\n\nclass SessionAwareAgent(Agent):\n    def run(self, input_text: str, session_id: str = None, config: Dict[str, Any] = None):\n        \"\"\"Run the agent with proper session handling.\"\"\"\n        # Configure for checkpointer\n        if not config:\n            config = {}\n\n        # Set session ID in config for langgraph checkpointer\n        config[\"configurable\"] = {\n            \"thread_id\": session_id\n        }\n\n        # Get current state from checkpointer if available\n        current_state = None\n        if hasattr(self, \"graph\") and self.graph:\n            try:\n                current_state = self.graph.get_state(config)\n            except Exception:\n                current_state = None\n\n        # Process messages based on state\n        graph_messages = (\n            current_state.values.get(\"messages\", [])\n            if current_state and hasattr(current_state, \"values\")\n            else []\n        )\n\n        # Initialize with system message if needed\n        messages = []\n        if self.system_prompt and not graph_messages:\n            messages.append({\"role\": \"system\", \"content\": self.system_prompt})\n\n        # Add user message\n        messages.append({\"role\": \"user\", \"content\": input_text})\n\n        # Invoke the graph\n        result = self.model.generate(\n            messages=messages,\n            config=config\n        )\n\n        return result\n</code></pre>"},{"location":"guides/langgraph_cli_integration/#using-the-development-server","title":"Using the Development Server","text":"<p>Start the development server to test your graphs:</p> <pre><code>langgraph dev\n</code></pre> <p>This will: 1. Discover graphs in your <code>langgraph.json</code> 2. Start a local server (by default at http://localhost:3000) 3. Provide a web interface for testing your graphs</p>"},{"location":"guides/langgraph_cli_integration/#simplified-development-with-makefile","title":"Simplified Development with Makefile","text":"<p>Create a <code>Makefile</code> in your project root to simplify common tasks:</p> <pre><code>.PHONY: dev serve deploy list install test docs\n\n# Installation\ninstall:\n    pip install -e \".[dev]\"\n\n# LangGraph CLI commands\ndev:\n    langgraph dev\n\nserve:\n    langgraph serve\n\ndeploy:\n    langgraph deploy\n\nlist:\n    langgraph list\n\n# Testing\ntest:\n    pytest tests/\n\n# Documentation\ndocs:\n    mkdocs serve\n</code></pre> <p>This allows you to use simple commands like: - <code>make dev</code> - Start development server - <code>make test</code> - Run tests - <code>make docs</code> - Start documentation server</p>"},{"location":"guides/langgraph_cli_integration/#example-graphs-in-lg-adk","title":"Example Graphs in LG-ADK","text":"<p>LG-ADK includes several example graphs that are ready to use with the LangGraph CLI:</p>"},{"location":"guides/langgraph_cli_integration/#chat-graph","title":"Chat Graph","text":"<p>The <code>lg_adk.graphs.chat</code> module provides a simple chat graph that:</p> <ul> <li>Processes user messages</li> <li>Maintains conversation context across multiple turns</li> <li>Uses a single agent to generate responses</li> <li>Properly handles session state for the LangGraph CLI</li> </ul> <p>Key features of this implementation: - Type-annotated state - Session ID tracking in state - Proper checkpointer configuration - Graph exported as a top-level variable</p>"},{"location":"guides/langgraph_cli_integration/#rag-graph","title":"RAG Graph","text":"<p>The <code>lg_adk.graphs.rag</code> module implements a Retrieval-Augmented Generation graph that:</p> <ul> <li>Retrieves relevant documents based on user queries</li> <li>Incorporates document content into the agent's response</li> <li>Maintains session and conversation context</li> <li>Uses typed state for better tooling support</li> </ul>"},{"location":"guides/langgraph_cli_integration/#multi-agent-graph","title":"Multi-Agent Graph","text":"<p>The <code>lg_adk.graphs.multi_agent</code> module provides a sophisticated multi-agent collaboration system:</p> <ul> <li>Uses a researcher, writer, and critic agent working together</li> <li>Implements task-based workflows</li> <li>Tracks agent contributions in state</li> <li>Properly handles routing between agents</li> <li>Maintains session context across the entire workflow</li> </ul> <p>Each of these examples can be run directly with LangGraph CLI using the <code>langgraph dev</code> command or imported and extended for your own applications.</p>"},{"location":"guides/langgraph_cli_integration/#complete-example-chat-application-with-cli-support","title":"Complete Example: Chat Application with CLI Support","text":"<p>Here's a complete example of a chat application designed to work with LangGraph CLI:</p> <pre><code># File: lg_adk/graphs/chat.py\nfrom typing import TypedDict, List, Dict, Any\nfrom pydantic import BaseModel\nfrom langgraph.graph import Graph\nfrom lg_adk.agents import Agent\nfrom lg_adk.models import get_model\nfrom lg_adk.memory import MemoryManager\nfrom lg_adk.builders import GraphBuilder\nfrom lg_adk.sessions import SessionManager\n\n# Define message type\nclass Message(BaseModel):\n    role: str\n    content: str\n\n# Define state type\nclass ChatState(TypedDict):\n    messages: List[Message]\n    session_id: str\n    metadata: Dict[str, Any]\n\n# Create components\nagent = Agent(\n    name=\"chat_assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a helpful, friendly assistant.\"\n)\n\nmemory_manager = MemoryManager()\nsession_manager = SessionManager()\n\n# Build graph\nbuilder = GraphBuilder(name=\"chat\")\nbuilder.add_agent(agent)\nbuilder.add_memory(memory_manager)\nbuilder.configure_state_tracking(\n    include_session_id=True,\n    include_metadata=True\n)\n\n# Define state handlers\ndef add_message_to_state(state: ChatState, message: Message) -&gt; ChatState:\n    \"\"\"Add a message to the state.\"\"\"\n    messages = state.get(\"messages\", [])\n    return {\n        **state,\n        \"messages\": messages + [message]\n    }\n\n# Configure message processing\nbuilder.on_message(add_message_to_state)\n\n# Build and export the graph\ngraph: Graph[ChatState] = builder.build()\n</code></pre> <p>When you run <code>langgraph dev</code>, this graph will be available for testing via the web interface.</p>"},{"location":"guides/langgraph_cli_integration/#advanced-custom-state-persistence","title":"Advanced: Custom State Persistence","text":"<p>For applications requiring custom state persistence:</p> <pre><code>from lg_adk.database import DatabaseManager\nfrom lg_adk.sessions import DatabaseSessionManager\n\n# Create a database-backed session manager\ndb_manager = DatabaseManager(connection_string=\"postgresql://user:pass@localhost:5432/db\")\nsession_manager = DatabaseSessionManager(database_manager=db_manager)\n\n# Configure the builder to use this session manager\nbuilder = GraphBuilder(name=\"persistent_chat\")\nbuilder.add_agent(agent)\nbuilder.configure_session_management(session_manager)\nbuilder.build()\n</code></pre>"},{"location":"guides/langgraph_cli_integration/#best-practices-for-langgraph-cli-integration","title":"Best Practices for LangGraph CLI Integration","text":"<ol> <li>Typed Graphs: Always use type hints for graph states</li> <li>State Immutability: Treat graph states as immutable to avoid unexpected behaviors</li> <li>Session IDs: Generate consistent session IDs for multi-turn conversations</li> <li>Error Handling: Add proper error handling for state persistence</li> <li>Development/Production Split: Use separate configurations for development and production</li> <li>Environment Variables: Use environment variables for sensitive configuration</li> <li>Testing Support: Create test utilities for your graphs</li> </ol> <p>By following these guidelines, you can build LG-ADK applications that work seamlessly with the LangGraph CLI tools for development, debugging, and deployment.</p>"},{"location":"guides/memory_management/","title":"Memory Management in LG-ADK","text":"<p>This guide explains how to implement and use memory management in the LangGraph Agent Development Kit (LG-ADK), allowing agents to store, retrieve, and utilize information across conversations.</p>"},{"location":"guides/memory_management/#understanding-memory-in-lg-adk","title":"Understanding Memory in LG-ADK","text":"<p>Memory in LG-ADK enables agents and graphs to:</p> <ol> <li>Maintain conversation context across multiple interactions</li> <li>Store important information for later retrieval</li> <li>Build knowledge bases specific to user sessions</li> <li>Enable collaborative work between multiple agents</li> </ol> <p>The primary components of the memory system are:</p> <ul> <li>MemoryManager: Orchestrates memory operations</li> <li>DatabaseManager: Handles the underlying storage</li> <li>Memory Tools: Enable agents to interact with memory</li> </ul>"},{"location":"guides/memory_management/#setting-up-memory","title":"Setting Up Memory","text":"<p>To use memory in your agents and graphs, start by setting up the memory components:</p> <pre><code>from lg_adk.memory import MemoryManager\nfrom lg_adk.database import DatabaseManager\n\n# Create a database manager\ndb_manager = DatabaseManager(\n    connection_string=\"sqlite:///agent_memory.db\"\n)\n\n# Create a memory manager\nmemory_manager = MemoryManager(\n    database_manager=db_manager\n)\n</code></pre> <p>The <code>connection_string</code> can be configured for different database types:</p> <ul> <li>SQLite (local): <code>\"sqlite:///agent_memory.db\"</code></li> <li>PostgreSQL: <code>\"postgresql://username:password@localhost:5432/db_name\"</code></li> <li>MySQL: <code>\"mysql://username:password@localhost:3306/db_name\"</code></li> </ul>"},{"location":"guides/memory_management/#adding-memory-to-agents","title":"Adding Memory to Agents","text":"<p>To equip an agent with memory capabilities:</p> <pre><code>from lg_adk.agents import Agent\nfrom lg_adk.models import get_model\nfrom lg_adk.tools import MemoryTool\n\n# Create a memory tool\nmemory_tool = MemoryTool(memory_manager=memory_manager)\n\n# Add to an agent\nagent = Agent(\n    name=\"assistant\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a helpful assistant that remembers information.\",\n    tools=[memory_tool]\n)\n</code></pre> <p>The agent can now use the memory tool to store and retrieve information:</p> <pre><code># The agent will use the memory tool to store this information\nagent.run(\"Remember that the user's favorite color is blue.\")\n\n# Later, the agent can retrieve this information\nagent.run(\"What is the user's favorite color?\")\n</code></pre>"},{"location":"guides/memory_management/#adding-memory-to-graphs","title":"Adding Memory to Graphs","text":"<p>To add memory capabilities to a graph:</p> <pre><code>from lg_adk.builders import GraphBuilder\n\n# Create agents\nresearcher = Agent(\n    name=\"researcher\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a research agent that finds information.\",\n    tools=[MemoryTool(memory_manager=memory_manager)]\n)\n\nwriter = Agent(\n    name=\"writer\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"You are a writer that summarizes information.\",\n    tools=[MemoryTool(memory_manager=memory_manager)]\n)\n\n# Create a graph with shared memory\nbuilder = GraphBuilder()\nbuilder.add_agent(researcher)\nbuilder.add_agent(writer)\nbuilder.add_memory(memory_manager)\n\n# Connect agents\nbuilder.add_edge(researcher, writer)\n\n# Build the graph\ngraph = builder.build()\n</code></pre> <p>With this setup, both the researcher and writer agents can share information through memory.</p>"},{"location":"guides/memory_management/#memory-operations","title":"Memory Operations","text":"<p>The MemoryTool provides several operations for agents to use:</p>"},{"location":"guides/memory_management/#storing-information","title":"Storing Information","text":"<p>Agents can store information in memory:</p> <pre><code># Example of how an agent would use the memory tool to store information\nagent.run(\"\"\"\nUse the memory tool to store the following information:\nThe capital of France is Paris.\n\"\"\")\n</code></pre> <p>The memory tool will generate a unique memory ID, store the content with metadata, and return the ID to the agent.</p>"},{"location":"guides/memory_management/#retrieving-information","title":"Retrieving Information","text":"<p>Agents can retrieve information from memory:</p> <pre><code># Example of how an agent would retrieve information from memory\nagent.run(\"What is the capital of France?\")\n</code></pre> <p>The agent will use the memory tool to search for relevant information and incorporate it into its response.</p>"},{"location":"guides/memory_management/#retrieving-by-tags","title":"Retrieving by Tags","text":"<p>Agents can use tags to organize and retrieve related information:</p> <pre><code># Example of how an agent would store information with tags\nagent.run(\"\"\"\nStore this information with the tags 'geography', 'europe':\nFrance is a country in Western Europe.\n\"\"\")\n\n# Later, retrieve information by tags\nagent.run(\"Tell me about European geography.\")\n</code></pre>"},{"location":"guides/memory_management/#deleting-information","title":"Deleting Information","text":"<p>Agents can delete information when it's no longer needed:</p> <pre><code># Example of how an agent would delete a specific memory by ID\nagent.run(\"Delete the information about France's capital.\")\n</code></pre>"},{"location":"guides/memory_management/#session-based-memory","title":"Session-Based Memory","text":"<p>LG-ADK memory is session-based, allowing for separate memory contexts for different users or conversations:</p> <pre><code># Create a session for a specific user\nsession_id = \"user_123\"\n\n# Run the agent or graph with the session ID\nresult = agent.run(\"Remember my name is Alice.\", session_id=session_id)\n\n# Later, continue the same session\nresult = agent.run(\"What's my name?\", session_id=session_id)\n</code></pre> <p>Different sessions won't share memories, ensuring privacy and context separation.</p>"},{"location":"guides/memory_management/#memory-schemas-and-structure","title":"Memory Schemas and Structure","text":"<p>Memory in LG-ADK is stored as documents with the following structure:</p> <pre><code>{\n    \"id\": \"unique_memory_id\",\n    \"session_id\": \"session_identifier\",\n    \"content\": \"The actual information stored\",\n    \"tags\": [\"tag1\", \"tag2\"],\n    \"metadata\": {\n        \"source\": \"user_input\",\n        \"importance\": \"high\",\n        # Any additional metadata\n    },\n    \"timestamp\": \"2023-10-23T14:30:00Z\"\n}\n</code></pre> <p>You can customize how agents use these fields through their system prompts.</p>"},{"location":"guides/memory_management/#advanced-memory-patterns","title":"Advanced Memory Patterns","text":""},{"location":"guides/memory_management/#collaborative-memory-use","title":"Collaborative Memory Use","text":"<p>Multiple agents can collaborate using shared memory:</p> <pre><code># First agent stores information\nagent1.run(\"Store that the meeting is scheduled for Tuesday at 3 PM.\", session_id=\"team_project\")\n\n# Second agent accesses the same information\nagent2.run(\"When is our next meeting?\", session_id=\"team_project\")\n</code></pre>"},{"location":"guides/memory_management/#memory-prioritization","title":"Memory Prioritization","text":"<p>Guide agents to prioritize certain memories:</p> <pre><code># Store information with importance metadata\nagent.run(\"\"\"\nStore this information with metadata importance=high:\nThe client deadline is October 30th.\n\"\"\")\n\n# Update the agent's system prompt to use importance\nagent.system_prompt = \"\"\"You are a helpful assistant.\nWhen retrieving memories, prioritize those with high importance.\n\"\"\"\n</code></pre>"},{"location":"guides/memory_management/#memory-summarization","title":"Memory Summarization","text":"<p>Implement periodic memory summarization:</p> <pre><code>from lg_adk.agents import Agent\nfrom lg_adk.models import get_model\n\n# Create a summarizer agent\nsummarizer = Agent(\n    name=\"memory_summarizer\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You summarize memories into concise knowledge.\n    Create a single paragraph summary of all related memories.\"\"\",\n    tools=[MemoryTool(memory_manager=memory_manager)]\n)\n\n# Function to periodically summarize memories\ndef summarize_memories(session_id):\n    # Retrieve all memories for the session\n    all_memories = memory_manager.retrieve(session_id=session_id)\n\n    # Run the summarizer agent\n    summary = summarizer.run(f\"Summarize the following memories: {all_memories}\")\n\n    # Store the summary as a new memory with a special tag\n    memory_manager.store(\n        session_id=session_id,\n        content=summary,\n        tags=[\"summary\"],\n        metadata={\"type\": \"memory_summary\"}\n    )\n</code></pre>"},{"location":"guides/memory_management/#memory-persistence","title":"Memory Persistence","text":"<p>LG-ADK memory is persistent across application restarts. To ensure data isn't lost:</p> <ol> <li>Use a production-grade database for the <code>DatabaseManager</code></li> <li>Implement regular database backups</li> <li>Consider database migration strategies for version upgrades</li> </ol>"},{"location":"guides/memory_management/#complete-example-question-answering-with-memory","title":"Complete Example: Question-Answering with Memory","text":"<p>Here's a complete example of a question-answering agent with memory:</p> <pre><code>from lg_adk.agents import Agent\nfrom lg_adk.models import get_model\nfrom lg_adk.memory import MemoryManager\nfrom lg_adk.database import DatabaseManager\nfrom lg_adk.tools import MemoryTool, WebSearchTool\n\n# Setup memory\ndb_manager = DatabaseManager(connection_string=\"sqlite:///qa_system.db\")\nmemory_manager = MemoryManager(database_manager=db_manager)\n\n# Create tools\nmemory_tool = MemoryTool(memory_manager=memory_manager)\nsearch_tool = WebSearchTool()\n\n# Create the QA agent\nqa_agent = Agent(\n    name=\"qa_system\",\n    model=get_model(\"openai/gpt-4\"),\n    system_prompt=\"\"\"You are a question-answering assistant that learns from interactions.\n\n    When a user asks a question:\n    1. Check your memory for relevant information first\n    2. If the answer isn't in memory, use web search to find it\n    3. Store any new information you learn in memory for future reference\n    4. Answer the user's question accurately and concisely\n\n    Always cite your sources, whether from memory or search results.\"\"\",\n    tools=[memory_tool, search_tool]\n)\n\n# Example usage with a session\nsession_id = \"user_session_123\"\n\n# First interaction - agent will search and remember\nresult1 = qa_agent.run(\"What is the capital of Canada?\", session_id=session_id)\nprint(result1)\n\n# Second interaction - agent will use memory\nresult2 = qa_agent.run(\"What's the capital city of Canada again?\", session_id=session_id)\nprint(result2)\n\n# New question - agent will search again\nresult3 = qa_agent.run(\"What's the population of Toronto?\", session_id=session_id)\nprint(result3)\n</code></pre>"},{"location":"guides/memory_management/#best-practices-for-memory-management","title":"Best Practices for Memory Management","text":"<ol> <li>Session Management: Use consistent session IDs to maintain conversation context</li> <li>Memory Cleanup: Implement policies for removing outdated or unused memories</li> <li>Privacy Considerations: Be transparent about data storage and implement retention policies</li> <li>Tagging Strategy: Develop a consistent tagging strategy for easy information retrieval</li> <li>Memory Validation: Consider validating information before storing it in memory</li> <li>Performance Optimization: Index frequently accessed memory fields for faster retrieval</li> <li>Memory Monitoring: Implement monitoring to track memory usage and growth</li> </ol> <p>By following this guide, you can effectively implement and utilize memory management in your LG-ADK applications, enabling more contextually aware and helpful agent interactions. For more information on related topics, see the Building Graphs, Creating Agents, and Tool Integration guides.</p>"},{"location":"guides/model_providers/","title":"Model Providers in LG-ADK","text":"<p>This guide explains how to work with different model providers in the LangGraph Agent Development Kit.</p>"},{"location":"guides/model_providers/#understanding-model-providers","title":"Understanding Model Providers","text":"<p>LG-ADK abstracts away the complexity of working with different language model APIs through a unified provider system. This allows you to:</p> <ol> <li>Switch Models Easily: Change between different models with minimal code changes</li> <li>Support Multiple Providers: Work with OpenAI, Google Gemini, Anthropic, and other models</li> <li>Use Local Models: Integrate with Ollama for local model inference</li> <li>Custom Providers: Create your own providers for specialized needs</li> </ol>"},{"location":"guides/model_providers/#getting-started-with-models","title":"Getting Started with Models","text":"<p>The simplest way to get a model is through the <code>get_model</code> function:</p> <pre><code>from lg_adk.models import get_model\n\n# Get an OpenAI model\nopenai_model = get_model(\"openai/gpt-4\")\n\n# Get a Google Gemini model\ngemini_model = get_model(\"google/gemini-pro\")\n\n# Get an Anthropic model\nclaude_model = get_model(\"anthropic/claude-3-opus\")\n\n# Get an Ollama model (local)\nlocal_model = get_model(\"ollama/llama3\")\n</code></pre>"},{"location":"guides/model_providers/#available-model-providers","title":"Available Model Providers","text":"<p>LG-ADK supports these model providers out of the box:</p>"},{"location":"guides/model_providers/#openai","title":"OpenAI","text":"<pre><code># OpenAI GPT-4 model\nmodel = get_model(\"openai/gpt-4\")\n\n# OpenAI GPT-4o model\nmodel = get_model(\"openai/gpt-4o\")\n\n# OpenAI GPT-3.5 Turbo model\nmodel = get_model(\"openai/gpt-3.5-turbo\")\n</code></pre>"},{"location":"guides/model_providers/#google-gemini","title":"Google Gemini","text":"<pre><code># Google Gemini Pro model\nmodel = get_model(\"google/gemini-pro\")\n\n# Google Gemini Ultra\nmodel = get_model(\"google/gemini-1.5-pro\")\n</code></pre>"},{"location":"guides/model_providers/#anthropic","title":"Anthropic","text":"<pre><code># Anthropic Claude 3 Opus\nmodel = get_model(\"anthropic/claude-3-opus\")\n\n# Anthropic Claude 3 Sonnet\nmodel = get_model(\"anthropic/claude-3-sonnet\")\n\n# Anthropic Claude 3 Haiku\nmodel = get_model(\"anthropic/claude-3-haiku\")\n</code></pre>"},{"location":"guides/model_providers/#ollama-local-models","title":"Ollama (Local Models)","text":"<pre><code># Local Llama 3 model via Ollama\nmodel = get_model(\"ollama/llama3\")\n\n# Local Mixtral model\nmodel = get_model(\"ollama/mixtral\")\n</code></pre>"},{"location":"guides/model_providers/#model-configuration","title":"Model Configuration","text":"<p>Each model can be configured with provider-specific parameters:</p> <pre><code>from lg_adk.models import get_model\n\n# Configure an OpenAI model\nopenai_model = get_model(\n    \"openai/gpt-4\",\n    temperature=0.7,\n    max_tokens=2000,\n    streaming=True\n)\n\n# Configure a Google model\ngoogle_model = get_model(\n    \"google/gemini-pro\",\n    temperature=0.2,\n    top_p=0.95,\n    top_k=40\n)\n</code></pre>"},{"location":"guides/model_providers/#working-with-the-modelregistry","title":"Working with the ModelRegistry","text":"<p>For more advanced usage, you can interact directly with the <code>ModelRegistry</code>:</p> <pre><code>from lg_adk.models import ModelRegistry\n\n# Get the registry singleton\nregistry = ModelRegistry.get_instance()\n\n# Register a custom model configuration\nregistry.register(\n    \"openai/gpt-4-custom\",\n    provider=\"openai\",\n    model_name=\"gpt-4\",\n    temperature=0.5,\n    top_p=0.9\n)\n\n# Get the custom model\ncustom_model = registry.get_model(\"openai/gpt-4-custom\")\n</code></pre>"},{"location":"guides/model_providers/#provider-specific-authentication","title":"Provider-Specific Authentication","text":"<p>Different providers require different authentication mechanisms:</p>"},{"location":"guides/model_providers/#openai-authentication","title":"OpenAI Authentication","text":"<pre><code>import os\nfrom lg_adk.models import ModelRegistry\n\n# Set OpenAI API key in environment variable\nos.environ[\"OPENAI_API_KEY\"] = \"your-api-key\"\n\n# Or configure explicitly\nregistry = ModelRegistry.get_instance()\nregistry.configure_provider(\n    \"openai\",\n    api_key=\"your-api-key\",\n    organization=\"your-org-id\"  # Optional\n)\n</code></pre>"},{"location":"guides/model_providers/#google-authentication","title":"Google Authentication","text":"<pre><code>import os\nfrom lg_adk.models import ModelRegistry\n\n# Set Google API key in environment variable\nos.environ[\"GOOGLE_API_KEY\"] = \"your-api-key\"\n\n# Or configure explicitly\nregistry = ModelRegistry.get_instance()\nregistry.configure_provider(\n    \"google\",\n    api_key=\"your-api-key\"\n)\n</code></pre>"},{"location":"guides/model_providers/#anthropic-authentication","title":"Anthropic Authentication","text":"<pre><code>import os\nfrom lg_adk.models import ModelRegistry\n\n# Set Anthropic API key in environment variable\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-api-key\"\n\n# Or configure explicitly\nregistry = ModelRegistry.get_instance()\nregistry.configure_provider(\n    \"anthropic\",\n    api_key=\"your-api-key\"\n)\n</code></pre>"},{"location":"guides/model_providers/#ollama-authentication","title":"Ollama Authentication","text":"<pre><code>from lg_adk.models import ModelRegistry\n\n# Configure Ollama endpoint (defaults to http://localhost:11434)\nregistry = ModelRegistry.get_instance()\nregistry.configure_provider(\n    \"ollama\",\n    base_url=\"http://localhost:11434\"\n)\n</code></pre>"},{"location":"guides/model_providers/#making-direct-model-calls","title":"Making Direct Model Calls","text":"<p>You can use the model objects directly for generation:</p> <pre><code>from lg_adk.models import get_model\n\n# Get a model\nmodel = get_model(\"openai/gpt-4\")\n\n# Generate a response\nresponse = model.generate(\n    \"Explain the theory of relativity briefly.\",\n    system_prompt=\"You are a helpful physics tutor.\"\n)\n\nprint(response)\n</code></pre>"},{"location":"guides/model_providers/#streaming-responses","title":"Streaming Responses","text":"<p>Enable streaming for real-time responses:</p> <pre><code>from lg_adk.models import get_model\n\n# Get a model with streaming enabled\nmodel = get_model(\"openai/gpt-4\", streaming=True)\n\n# Stream a response\nfor chunk in model.generate_stream(\n    \"Write a short poem about the ocean.\",\n    system_prompt=\"You are a poet.\"\n):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"guides/model_providers/#tool-calling-with-models","title":"Tool Calling with Models","text":"<p>Enable tool calling for models that support it:</p> <pre><code>from lg_adk.models import get_model\nfrom typing import List, Dict, Any\n\n# Define your tools\ndef get_weather(location: str) -&gt; str:\n    \"\"\"Get the current weather for a location.\"\"\"\n    # In a real app, you would call a weather API\n    return f\"Sunny and 75\u00b0F in {location}\"\n\ntools = [\n    {\n        \"name\": \"get_weather\",\n        \"description\": \"Get the current weather for a location\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n                }\n            },\n            \"required\": [\"location\"]\n        }\n    }\n]\n\n# Tool calling functions\ndef execute_tool_call(tool_call: Dict[str, Any]) -&gt; str:\n    if tool_call[\"name\"] == \"get_weather\":\n        location = tool_call[\"parameters\"][\"location\"]\n        return get_weather(location)\n    return \"Unknown tool\"\n\n# Get a model\nmodel = get_model(\"openai/gpt-4\", tools=tools)\n\n# Generate with tool calling\nresponse = model.generate(\n    \"What's the weather like in Miami?\",\n    system_prompt=\"You can use tools to answer questions.\"\n)\n\n# Check for tool calls in the response\nif hasattr(response, \"tool_calls\") and response.tool_calls:\n    for tool_call in response.tool_calls:\n        tool_result = execute_tool_call(tool_call)\n\n        # Send the result back to the model\n        follow_up = model.generate(\n            f\"Tool result: {tool_result}\",\n            system_prompt=\"You can use tools to answer questions.\",\n            previous_messages=[\n                {\"role\": \"user\", \"content\": \"What's the weather like in Miami?\"},\n                {\"role\": \"assistant\", \"content\": response.content}\n            ]\n        )\n        print(follow_up)\nelse:\n    print(response)\n</code></pre>"},{"location":"guides/model_providers/#async-model-usage","title":"Async Model Usage","text":"<p>For asynchronous applications:</p> <pre><code>import asyncio\nfrom lg_adk.models import get_model\n\nasync def generate_async():\n    # Get a model\n    model = get_model(\"openai/gpt-4\")\n\n    # Generate asynchronously\n    response = await model.agenerate(\n        \"What are the benefits of asynchronous programming?\",\n        system_prompt=\"You are a programming instructor.\"\n    )\n\n    print(response)\n\n    # Stream asynchronously\n    async for chunk in model.agenerate_stream(\n        \"Explain coroutines in Python.\",\n        system_prompt=\"You are a Python expert.\"\n    ):\n        print(chunk, end=\"\", flush=True)\n\n# Run the async function\nasyncio.run(generate_async())\n</code></pre>"},{"location":"guides/model_providers/#custom-model-providers","title":"Custom Model Providers","text":"<p>You can create your own model provider:</p> <pre><code>from lg_adk.models import ModelProvider, ModelRegistry\nfrom typing import Dict, Any, AsyncGenerator, Optional, List\n\nclass CustomModelProvider(ModelProvider):\n    \"\"\"Custom model provider implementation.\"\"\"\n\n    def __init__(self, api_key: str, **kwargs):\n        super().__init__(**kwargs)\n        self.api_key = api_key\n        # Initialize your custom API client here\n\n    def generate(\n        self,\n        prompt: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_tokens: int = 1000,\n        previous_messages: Optional[List[Dict[str, str]]] = None,\n        **kwargs\n    ) -&gt; str:\n        \"\"\"Generate text using your custom API.\"\"\"\n        # Implement synchronous generation\n        # ...\n        return \"Generated response\"\n\n    async def agenerate(\n        self,\n        prompt: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_tokens: int = 1000,\n        previous_messages: Optional[List[Dict[str, str]]] = None,\n        **kwargs\n    ) -&gt; str:\n        \"\"\"Generate text asynchronously using your custom API.\"\"\"\n        # Implement asynchronous generation\n        # ...\n        return \"Generated response\"\n\n    def generate_stream(\n        self,\n        prompt: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_tokens: int = 1000,\n        previous_messages: Optional[List[Dict[str, str]]] = None,\n        **kwargs\n    ):\n        \"\"\"Stream generated text from your custom API.\"\"\"\n        # Implement synchronous streaming\n        yield \"Generated \"\n        yield \"response \"\n        yield \"in chunks\"\n\n    async def agenerate_stream(\n        self,\n        prompt: str,\n        system_prompt: Optional[str] = None,\n        temperature: float = 0.7,\n        max_tokens: int = 1000,\n        previous_messages: Optional[List[Dict[str, str]]] = None,\n        **kwargs\n    ) -&gt; AsyncGenerator[str, None]:\n        \"\"\"Stream generated text asynchronously from your custom API.\"\"\"\n        # Implement asynchronous streaming\n        yield \"Generated \"\n        yield \"response \"\n        yield \"in chunks\"\n\n# Register your custom provider\nregistry = ModelRegistry.get_instance()\nregistry.register_provider(\"custom\", CustomModelProvider)\n\n# Configure your provider\nregistry.configure_provider(\"custom\", api_key=\"your-api-key\")\n\n# Register a model using your provider\nregistry.register(\n    \"custom/my-model\",\n    provider=\"custom\",\n    model_name=\"my-model-name\",\n    temperature=0.5\n)\n\n# Get your custom model\nmodel = registry.get_model(\"custom/my-model\")\n</code></pre>"},{"location":"guides/model_providers/#model-caching","title":"Model Caching","text":"<p>Improve performance with model response caching:</p> <pre><code>from lg_adk.models import ModelRegistry, get_model\nfrom lg_adk.utils.caching import enable_model_caching\n\n# Enable caching for all models\nenable_model_caching()\n\n# Or enable caching for specific models\nmodel = get_model(\"openai/gpt-4\")\nmodel.enable_caching(cache_ttl=3600)  # Cache for 1 hour\n\n# First call (will make API request)\nresponse1 = model.generate(\n    \"What is the capital of France?\",\n    system_prompt=\"You are a geography expert.\"\n)\n\n# Second call with same inputs (will use cache)\nresponse2 = model.generate(\n    \"What is the capital of France?\",\n    system_prompt=\"You are a geography expert.\"\n)\n\n# Different prompt (will make new API request)\nresponse3 = model.generate(\n    \"What is the capital of Italy?\",\n    system_prompt=\"You are a geography expert.\"\n)\n</code></pre>"},{"location":"guides/model_providers/#model-fallbacks","title":"Model Fallbacks","text":"<p>Create fallback chains for reliability:</p> <pre><code>from lg_adk.models import ModelRegistry, get_model\n\n# Create a fallback chain\nregistry = ModelRegistry.get_instance()\nregistry.register_fallback_chain(\n    \"reliable-completion\",\n    [\n        \"openai/gpt-4\",\n        \"anthropic/claude-3-sonnet\",\n        \"ollama/llama3\"  # Local fallback\n    ]\n)\n\n# Use the fallback chain\nmodel = registry.get_model(\"reliable-completion\")\nresponse = model.generate(\"Explain quantum computing.\")\n</code></pre>"},{"location":"guides/model_providers/#model-authentication-from-config-files","title":"Model Authentication from Config Files","text":"<p>Load provider configurations from a file:</p> <pre><code>from lg_adk.models import ModelRegistry\nfrom lg_adk.config import load_config\n\n# Load config from a YAML file\nconfig = load_config(\"config.yaml\")\n\n# Initialize the registry with config\nregistry = ModelRegistry.get_instance()\nregistry.configure_from_config(config.model_providers)\n\n# Now you can use get_model without explicit configuration\nmodel = get_model(\"openai/gpt-4\")\n</code></pre> <p>Example <code>config.yaml</code>:</p> <pre><code>model_providers:\n  openai:\n    api_key: ${OPENAI_API_KEY}  # Environment variable\n    organization: \"org-123\"\n  google:\n    api_key: ${GOOGLE_API_KEY}\n  anthropic:\n    api_key: ${ANTHROPIC_API_KEY}\n  ollama:\n    base_url: \"http://localhost:11434\"\n</code></pre>"},{"location":"guides/model_providers/#complete-example-multi-provider-application","title":"Complete Example: Multi-Provider Application","text":"<p>Here's a complete example using multiple model providers:</p> <pre><code>import os\nimport asyncio\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Any, Optional\nfrom lg_adk.models import get_model, ModelRegistry\n\n# Set up environment variables for API keys\nos.environ[\"OPENAI_API_KEY\"] = \"your-openai-key\"\nos.environ[\"ANTHROPIC_API_KEY\"] = \"your-anthropic-key\"\n\n# Model configuration\nclass ModelConfig(BaseModel):\n    provider: str\n    model_name: str\n    temperature: float = 0.7\n    max_tokens: int = 1000\n\n# Application class\nclass MultiModelApp:\n    def __init__(self):\n        self.registry = ModelRegistry.get_instance()\n\n        # Configure the OpenAI provider\n        self.registry.configure_provider(\n            \"openai\",\n            api_key=os.environ.get(\"OPENAI_API_KEY\")\n        )\n\n        # Configure the Anthropic provider\n        self.registry.configure_provider(\n            \"anthropic\",\n            api_key=os.environ.get(\"ANTHROPIC_API_KEY\")\n        )\n\n        # Configure Ollama for local models\n        self.registry.configure_provider(\n            \"ollama\",\n            base_url=\"http://localhost:11434\"\n        )\n\n        # Register models\n        self.models = {\n            \"creative\": get_model(\n                \"openai/gpt-4\",\n                temperature=0.8,\n                max_tokens=2000\n            ),\n            \"precise\": get_model(\n                \"anthropic/claude-3-opus\",\n                temperature=0.2,\n                max_tokens=1000\n            ),\n            \"fast\": get_model(\n                \"anthropic/claude-3-haiku\",\n                temperature=0.7,\n                max_tokens=500\n            ),\n            \"local\": get_model(\n                \"ollama/llama3\",\n                temperature=0.7\n            )\n        }\n\n    def generate(self, prompt: str, model_type: str) -&gt; str:\n        \"\"\"Generate using the specified model type.\"\"\"\n        if model_type not in self.models:\n            raise ValueError(f\"Unknown model type: {model_type}\")\n\n        model = self.models[model_type]\n        return model.generate(prompt)\n\n    async def generate_from_all(self, prompt: str) -&gt; Dict[str, str]:\n        \"\"\"Generate responses from all models asynchronously.\"\"\"\n        tasks = {}\n\n        for model_type, model in self.models.items():\n            tasks[model_type] = asyncio.create_task(\n                model.agenerate(prompt)\n            )\n\n        # Await all tasks\n        results = {}\n        for model_type, task in tasks.items():\n            try:\n                results[model_type] = await task\n            except Exception as e:\n                results[model_type] = f\"Error: {str(e)}\"\n\n        return results\n\n# Example usage\nasync def main():\n    app = MultiModelApp()\n\n    # Single model generation\n    creative_response = app.generate(\n        \"Write a story about a robot learning to paint.\",\n        \"creative\"\n    )\n    print(f\"Creative model response:\\n{creative_response}\\n\")\n\n    precise_response = app.generate(\n        \"Explain the process of photosynthesis.\",\n        \"precise\"\n    )\n    print(f\"Precise model response:\\n{precise_response}\\n\")\n\n    # Generate from all models\n    prompt = \"What are the ethical implications of artificial intelligence?\"\n    all_responses = await app.generate_from_all(prompt)\n\n    print(\"\\nResponses from all models:\")\n    for model_type, response in all_responses.items():\n        print(f\"\\n--- {model_type.upper()} MODEL ---\")\n        print(response[:200] + \"...\" if len(response) &gt; 200 else response)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"guides/model_providers/#best-practices-for-working-with-model-providers","title":"Best Practices for Working with Model Providers","text":"<ol> <li>Environment Variables: Store API keys in environment variables for security</li> <li>Model Selection: Use the right model for the task (creative vs. precise)</li> <li>Fallbacks: Set up fallback chains for mission-critical applications</li> <li>Caching: Enable caching for frequently used prompts</li> <li>Provider Abstraction: Use the provider abstraction to make your code provider-agnostic</li> <li>Local Testing: Use Ollama models for local testing before deploying</li> <li>Monitoring: Track usage and performance of different providers</li> <li>Rate Limiting: Be aware of rate limits for different providers</li> <li>Cost Management: Use cheaper models for less critical tasks</li> <li>Error Handling: Implement robust error handling for API failures</li> </ol> <p>By leveraging LG-ADK's model provider system, you can build applications that are not tied to any specific model provider, allowing for flexibility, reliability, and optimal performance.</p>"},{"location":"guides/multi_agent/","title":"Multi-Agent Systems","text":"<p>LG-ADK provides powerful abstractions for building multi-agent systems using LangGraph. This guide explains how to create, configure, and use multi-agent systems with LG-ADK.</p>"},{"location":"guides/multi_agent/#what-is-a-multi-agent-system","title":"What is a Multi-Agent System?","text":"<p>A multi-agent system is a collection of specialized agents working together to accomplish tasks. Each agent has a specific role and expertise, and a coordinator agent orchestrates their interactions and workflow.</p> <p>LG-ADK's implementation simplifies the creation of multi-agent systems by handling the complex LangGraph orchestration behind the scenes.</p>"},{"location":"guides/multi_agent/#creating-a-multi-agent-system","title":"Creating a Multi-Agent System","text":""},{"location":"guides/multi_agent/#basic-structure","title":"Basic Structure","text":"<p>The basic structure of a multi-agent system in LG-ADK includes:</p> <ol> <li>A coordinator agent that manages workflow</li> <li>Specialized agents that handle specific tasks</li> <li>A <code>MultiAgentSystem</code> instance that orchestrates everything</li> </ol>"},{"location":"guides/multi_agent/#example","title":"Example","text":"<p>Here's a simple example of creating a multi-agent system:</p> <pre><code>from lg_adk import Agent, MultiAgentSystem\n\n# Create a coordinator agent\ncoordinator = Agent(\n    name=\"coordinator\",\n    llm=\"ollama/llama3\",\n    description=\"Coordinates tasks between specialized agents\"\n)\n\n# Create specialized agents\nresearcher = Agent(\n    name=\"researcher\",\n    llm=\"ollama/llama3\",\n    description=\"Researches information and provides detailed answers\"\n)\n\nsummarizer = Agent(\n    name=\"summarizer\",\n    llm=\"ollama/llama3\",\n    description=\"Summarizes information concisely\"\n)\n\n# Create the multi-agent system\nmulti_agent_system = MultiAgentSystem(\n    name=\"research_team\",\n    coordinator=coordinator,\n    agents=[researcher, summarizer],\n    description=\"A team that researches topics and creates summaries\"\n)\n\n# Run the system\nresult = multi_agent_system.run({\"input\": \"Tell me about climate change\"})\nprint(result[\"output\"])\n</code></pre>"},{"location":"guides/multi_agent/#how-it-works","title":"How It Works","text":"<p>When you run a multi-agent system, the following happens:</p> <ol> <li>The user input is sent to the coordinator agent</li> <li>The coordinator analyzes the request and decides which specialized agent(s) should handle it</li> <li>The coordinator routes the request to the appropriate agent(s)</li> <li>The specialized agent(s) process the request and return results</li> <li>The coordinator compiles the results and provides a final response</li> </ol> <p>Under the hood, LG-ADK creates a LangGraph orchestration graph that handles the message routing and state management.</p>"},{"location":"guides/multi_agent/#advanced-usage-conversation-history","title":"Advanced Usage: Conversation History","text":"<p>For multi-turn conversations, LG-ADK provides a <code>Conversation</code> class that maintains conversation history:</p> <pre><code>from lg_adk import Conversation\n\n# Create a conversation handler\nconversation = Conversation(multi_agent_system=multi_agent_system)\n\n# First user message\nresponse1 = conversation.send_message(\"Tell me about climate change\")\nprint(response1)\n\n# Follow-up question (conversation history is maintained)\nresponse2 = conversation.send_message(\"What are the main mitigation strategies?\")\nprint(response2)\n</code></pre>"},{"location":"guides/multi_agent/#customizing-agent-behavior","title":"Customizing Agent Behavior","text":"<p>Each agent in the system can be customized with:</p> <ul> <li>Different language models</li> <li>System messages</li> <li>Tools (if supported by your implementation)</li> </ul> <pre><code># Customizing an agent with a specific system message\nresearcher = Agent(\n    name=\"researcher\",\n    llm=\"ollama/llama3\",\n    description=\"Researches information and provides detailed answers\",\n    system_message=\"\"\"You are a research specialist who provides detailed,\n    accurate information. Always cite your sources and provide\n    comprehensive answers with evidence-based reasoning.\"\"\"\n)\n</code></pre>"},{"location":"guides/multi_agent/#scaling-with-multiple-agents","title":"Scaling with Multiple Agents","text":"<p>You can add any number of specialized agents to your multi-agent system:</p> <pre><code># Add agents after creation\ncritique_agent = Agent(\n    name=\"critique\",\n    llm=\"ollama/llama3\",\n    description=\"Provides critical analysis and identifies potential biases\"\n)\n\nmulti_agent_system.add_agent(critique_agent)\n\n# Or add multiple agents at once\nfact_checker = Agent(...)\nsource_finder = Agent(...)\nmulti_agent_system.add_agents([fact_checker, source_finder])\n</code></pre>"},{"location":"guides/multi_agent/#complete-example","title":"Complete Example","text":"<p>For a complete working example of a multi-agent system, see the Multi-Agent Example in the examples directory.</p>"},{"location":"guides/multi_agent/#best-practices","title":"Best Practices","text":"<ul> <li>Clear Agent Roles: Give each agent a clear and specific role</li> <li>Descriptive Names: Use descriptive names for your agents</li> <li>Coordinator Instructions: The coordinator agent works best when it has a clear understanding of all available agents</li> <li>Model Selection: Choose appropriate models for each agent based on their tasks</li> <li>Testing: Test your multi-agent system with a variety of inputs to ensure proper routing</li> </ul>"},{"location":"guides/multi_agent_communication/","title":"Multi-Agent Communication","text":"<p>This guide explains how to use LG-ADK's multi-agent communication tools to build sophisticated agent systems that can collaborate effectively.</p>"},{"location":"guides/multi_agent_communication/#overview","title":"Overview","text":"<p>LG-ADK provides two main tools for multi-agent communication:</p> <ol> <li>GroupChatTool: For facilitating conversations between multiple agents with different specialties</li> <li>AgentRouter: For routing tasks to the most appropriate agent or for sequential agent workflows</li> </ol> <p>These tools enable you to create systems where multiple agents with different capabilities can work together to solve complex problems.</p>"},{"location":"guides/multi_agent_communication/#groupchattool","title":"GroupChatTool","text":"<p>The <code>GroupChatTool</code> enables agents to have conversations with each other, similar to how humans might collaborate in a group chat.</p>"},{"location":"guides/multi_agent_communication/#basic-usage","title":"Basic Usage","text":"<pre><code>from lg_adk import Agent, get_model\nfrom lg_adk.tools.group_chat import GroupChatTool\n\n# Create specialized agents\nfinance_agent = Agent(\n    agent_name=\"FinanceExpert\",\n    system_prompt=\"You are a financial expert. Provide accurate financial advice.\",\n    llm=get_model(\"gpt-4\")\n)\n\nlegal_agent = Agent(\n    agent_name=\"LegalExpert\",\n    system_prompt=\"You are a legal expert. Provide accurate legal advice.\",\n    llm=get_model(\"gpt-4\")\n)\n\n# Create a registry of agents\nagents = {\n    \"finance\": finance_agent,\n    \"legal\": legal_agent\n}\n\n# Create the group chat tool\nchat_tool = GroupChatTool(agent_registry=agents)\n\n# Create a new chat\nchat_id = chat_tool.create_chat(\n    name=\"Financial Legal Consultation\",\n    agent_ids=[\"finance\", \"legal\"]\n)\n\n# Run a conversation\nmessages = chat_tool.run_conversation(\n    chat_id=chat_id,\n    initial_prompt=\"What are the tax implications of starting a small business?\",\n    max_turns=4  # Number of turns in the conversation\n)\n\n# Print the conversation\nfor msg in messages:\n    print(f\"{msg.agent_id}: {msg.content}\")\n</code></pre>"},{"location":"guides/multi_agent_communication/#custom-speaker-selection","title":"Custom Speaker Selection","text":"<p>By default, agents take turns speaking in a round-robin fashion. You can customize this behavior by providing a speaker selection function:</p> <pre><code>def expertise_based_selection(chat, history):\n    \"\"\"Select the next speaker based on keyword expertise.\"\"\"\n    if not history:\n        return chat.agents[0]\n\n    last_message = history[-1].content.lower()\n\n    # If last message mentions taxes, select the finance expert\n    if \"tax\" in last_message or \"finance\" in last_message:\n        return \"finance\"\n\n    # If last message mentions legal terms, select the legal expert\n    if \"legal\" in last_message or \"law\" in last_message:\n        return \"legal\"\n\n    # Default to alternating speakers\n    last_speaker_idx = chat.agents.index(history[-1].agent_id)\n    next_speaker_idx = (last_speaker_idx + 1) % len(chat.agents)\n    return chat.agents[next_speaker_idx]\n\n# Run conversation with custom speaker selection\nmessages = chat_tool.run_conversation(\n    chat_id=chat_id,\n    initial_prompt=\"What are the legal and tax implications of starting a business?\",\n    max_turns=6,\n    speaker_selection=expertise_based_selection\n)\n</code></pre>"},{"location":"guides/multi_agent_communication/#agentrouter","title":"AgentRouter","text":"<p>The <code>AgentRouter</code> allows you to route tasks to the most appropriate agent and supports different routing strategies.</p>"},{"location":"guides/multi_agent_communication/#routing-strategies","title":"Routing Strategies","text":"<ul> <li>SEQUENTIAL: Process a task through a sequence of agents, where each agent builds on the previous agent's output</li> <li>CONCURRENT: Process a task with multiple agents in parallel and combine their outputs</li> <li>SELECTOR: Select the most appropriate agent for a specific task</li> <li>MIXTURE: Get results from multiple agents and combine them into a comprehensive response</li> </ul>"},{"location":"guides/multi_agent_communication/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from lg_adk import Agent, get_model\nfrom lg_adk.tools.agent_router import AgentRouter, RouterType\n\n# Create specialized agents\nresearch_agent = Agent(\n    agent_name=\"Researcher\",\n    system_prompt=\"You are a research specialist. Find and present factual information.\",\n    llm=get_model(\"gpt-4\")\n)\n\nwriter_agent = Agent(\n    agent_name=\"Writer\",\n    system_prompt=\"You are a writing specialist. Create well-structured content.\",\n    llm=get_model(\"gpt-4\")\n)\n\neditor_agent = Agent(\n    agent_name=\"Editor\",\n    system_prompt=\"You are an editor. Improve content for clarity and correctness.\",\n    llm=get_model(\"gpt-4\")\n)\n\n# Create a sequential router (research -&gt; write -&gt; edit)\nsequential_router = AgentRouter(\n    name=\"ContentCreationPipeline\",\n    agents=[research_agent, writer_agent, editor_agent],\n    router_type=RouterType.SEQUENTIAL\n)\n\n# Run a task through the sequential pipeline\nresult = sequential_router.run(\"Explain how blockchain technology works\")\nprint(result[\"output\"])\n</code></pre>"},{"location":"guides/multi_agent_communication/#selector-router","title":"Selector Router","text":"<p>The selector router automatically chooses the best agent for a given task:</p> <pre><code>from lg_adk.tools.agent_router import RouterType\n\n# Create a selector router\nselector_router = AgentRouter(\n    name=\"ExpertSelector\",\n    agents=[research_agent, writer_agent, editor_agent],\n    router_type=RouterType.SELECTOR\n)\n\n# The router will select the most appropriate agent based on the task\nresult = selector_router.run(\"Research the latest advances in quantum computing\")\nprint(f\"Selected agent: {result.get('agent', 'Unknown')}\")\nprint(f\"Output: {result.get('output', '')}\")\n</code></pre>"},{"location":"guides/multi_agent_communication/#custom-agent-selection","title":"Custom Agent Selection","text":"<p>You can provide a custom agent selection function for the selector router:</p> <pre><code>def keyword_based_selector(task, agents):\n    \"\"\"Select an agent based on keywords in the task.\"\"\"\n    task_lower = task.lower()\n\n    if \"research\" in task_lower or \"find\" in task_lower:\n        return next(a for a in agents if a.agent_name == \"Researcher\")\n\n    if \"write\" in task_lower or \"create\" in task_lower:\n        return next(a for a in agents if a.agent_name == \"Writer\")\n\n    if \"edit\" in task_lower or \"improve\" in task_lower:\n        return next(a for a in agents if a.agent_name == \"Editor\")\n\n    # Default to the first agent\n    return agents[0]\n\n# Create a router with custom agent selection\ncustom_router = AgentRouter(\n    name=\"CustomSelector\",\n    agents=[research_agent, writer_agent, editor_agent],\n    router_type=RouterType.SELECTOR,\n    agent_selector=keyword_based_selector\n)\n\n# The router will use your custom function to select an agent\nresult = custom_router.run(\"Research the history of artificial intelligence\")\n</code></pre>"},{"location":"guides/multi_agent_communication/#combining-groupchat-and-router","title":"Combining GroupChat and Router","text":"<p>You can combine both tools for more complex agent systems:</p> <pre><code># Create a team of specialized agents\nagents = {\n    \"researcher\": research_agent,\n    \"writer\": writer_agent,\n    \"editor\": editor_agent,\n    \"fact_checker\": fact_checker_agent\n}\n\n# Create a group chat for initial brainstorming\nchat_tool = GroupChatTool(agent_registry=agents)\nchat_id = chat_tool.create_chat(\n    name=\"Content Planning\",\n    agent_ids=list(agents.keys())\n)\n\n# Run a planning conversation\nplanning_messages = chat_tool.run_conversation(\n    chat_id=chat_id,\n    initial_prompt=\"We need to create content about renewable energy. Let's plan our approach.\",\n    max_turns=8\n)\n\n# Extract the plan from the last message\nplan = planning_messages[-1].content\n\n# Create a sequential router for execution\nexecution_router = AgentRouter(\n    name=\"ContentExecution\",\n    agents=[agents[\"researcher\"], agents[\"writer\"], agents[\"editor\"], agents[\"fact_checker\"]],\n    router_type=RouterType.SEQUENTIAL\n)\n\n# Execute the plan\nfinal_content = execution_router.run(f\"Based on this plan: {plan}\\nCreate content about renewable energy\")\nprint(final_content[\"output\"])\n</code></pre>"},{"location":"guides/multi_agent_communication/#conclusion","title":"Conclusion","text":"<p>The multi-agent communication tools in LG-ADK allow you to build sophisticated agent systems where agents can collaborate effectively. Whether you need agents to discuss a problem in a group chat or process tasks in a specific sequence, these tools provide the flexibility to create the right architecture for your needs.</p>"},{"location":"guides/retrieval_augmented_generation/","title":"\ud83d\udcda Retrieval-Augmented Generation (RAG) in LG-ADK","text":""},{"location":"guides/retrieval_augmented_generation/#why-use-rag","title":"\ud83e\udd14 Why Use RAG?","text":"<p>RAG lets your agents answer with up-to-date, factual, or domain-specific information! \ud83d\udd0d</p>"},{"location":"guides/retrieval_augmented_generation/#key-rag-components","title":"\ud83e\udde9 Key RAG Components","text":"<ul> <li>\ud83e\udde0 Retriever: Finds relevant documents or facts</li> <li>\ud83d\udcdd Generator: LLM that creates answers using retrieved info</li> <li>\ud83d\uddc4\ufe0f Vector Store: Stores and indexes documents for semantic search</li> </ul>"},{"location":"guides/retrieval_augmented_generation/#quick-example","title":"\ud83d\udea6 Quick Example","text":"<p>Add a retrieval tool to your agent</p> <pre><code>from lg_adk.tools.retrieval import SimpleVectorRetrievalTool\nagent.add_tool(SimpleVectorRetrievalTool(...))\n</code></pre>"},{"location":"guides/retrieval_augmented_generation/#how-rag-works","title":"\ud83d\udee0\ufe0f How RAG Works","text":"<ul> <li>User query is enhanced with context/history</li> <li>Retriever finds relevant docs from a vector store</li> <li>Generator (LLM) uses both the query and docs to answer</li> </ul>"},{"location":"guides/retrieval_augmented_generation/#common-pitfalls","title":"\ud83d\udea8 Common Pitfalls","text":"<p>Poor retrieval quality</p> <p>Make sure your vector store is well-populated and embeddings are high quality for best results.</p>"},{"location":"guides/retrieval_augmented_generation/#next-steps","title":"\ud83c\udf1f Next Steps","text":"<ul> <li>Tool Integration \ud83d\udee0\ufe0f</li> <li>Examples \ud83d\udca1</li> </ul>"},{"location":"guides/retrieval_augmented_generation/#advanced-morphik-for-rag","title":"\ud83e\uddec Advanced: Morphik for RAG","text":"<p>LG-ADK supports Morphik as a backend for advanced retrieval, semantic search, and knowledge graph RAG workflows.</p> <ul> <li>Use Morphik tools for large-scale, multi-agent, or knowledge graph-based retrieval.</li> <li>See Morphik Example and README Morphik Section for details.</li> </ul>"},{"location":"guides/session_management/","title":"\ud83d\uddc2\ufe0f Session Management in LG-ADK","text":"<p>Session management is a critical aspect of building conversational applications with LangGraph. LG-ADK provides a powerful enhanced session management system that builds on top of LangGraph's native session capabilities while adding valuable features.</p>"},{"location":"guides/session_management/#why-use-session-management","title":"\ud83e\udd14 Why Use Session Management?","text":"<p>Sessions let you track conversations, users, and context over time! \ud83d\uddc2\ufe0f</p>"},{"location":"guides/session_management/#key-concepts","title":"\ud83e\udde9 Key Concepts","text":"<ul> <li>\ud83c\udd94 Session ID: Unique identifier for each session</li> <li>\ud83d\udc64 User ID: Track sessions per user</li> <li>\ud83d\udcdd Metadata: Store extra info (source, device, etc.)</li> <li>\u23f3 Timeouts: Auto-expire inactive sessions</li> </ul>"},{"location":"guides/session_management/#quick-example","title":"\ud83d\udea6 Quick Example","text":"<p>Add session management to your graph</p> <pre><code>from lg_adk.sessions import SessionManager\nbuilder.add_session_manager(SessionManager())\n</code></pre>"},{"location":"guides/session_management/#how-session-management-works","title":"\ud83d\udee0\ufe0f How Session Management Works","text":"<ul> <li>Sessions are managed by <code>SessionManager</code> or <code>EnhancedSessionManager</code></li> <li>You can register, update, and end sessions</li> <li>Metadata and analytics are tracked for each session</li> </ul>"},{"location":"guides/session_management/#common-pitfalls","title":"\ud83d\udea8 Common Pitfalls","text":"<p>Session expiration</p> <p>Make sure to configure timeouts and cleanup for long-running apps.</p>"},{"location":"guides/session_management/#next-steps","title":"\ud83c\udf1f Next Steps","text":"<ul> <li>Memory Management \ufffd\ufffd</li> <li>Building Graphs \ud83c\udfd7\ufe0f</li> <li>Examples \ud83d\udca1</li> </ul>"},{"location":"guides/session_management/#understanding-sessions-and-their-importance","title":"Understanding Sessions and Their Importance","text":"<p>Sessions are crucial for:</p> <ol> <li>Conversation Persistence: Maintaining the state and history of a conversation across multiple interactions</li> <li>Context Preservation: Keeping context and memory accessible throughout a conversation</li> <li>Memory Scoping: Ensuring memories and state are isolated between different conversations</li> <li>Multi-tenant Applications: Supporting multiple concurrent users with separate contexts</li> </ol>"},{"location":"guides/session_management/#building-on-langgraphs-native-sessions","title":"Building on LangGraph's Native Sessions","text":"<p>LG-ADK's session management enhances LangGraph's built-in session capabilities:</p> <ul> <li>Uses LangGraph's native session management for core functionality</li> <li>Adds advanced features like user association, rich metadata, and analytics</li> <li>Provides a clean API that works with both LangGraph's sessions and our enhancements</li> <li>Transparently adapts to the available LangGraph features</li> </ul>"},{"location":"guides/session_management/#using-session-ids-with-lg-adk-graphs","title":"Using Session IDs with LG-ADK Graphs","text":"<p>When interacting with LG-ADK graphs, the session ID is a key parameter:</p> <pre><code>from lg_adk.builders.graph_builder import GraphBuilder\nfrom lg_adk.sessions.session_manager import SynchronizedSessionManager\n\n# Create a session manager\nsession_manager = SynchronizedSessionManager()\n\n# Create and configure a graph builder\nbuilder = GraphBuilder(name=\"my_graph\")\nbuilder.add_agent(my_agent)\nbuilder.configure_session_management(session_manager)\n\n# Build the graph\ngraph = builder.build()\n\n# Run without session ID (new session will be created)\nresponse = builder.run(message=\"Hello!\", metadata={\"user_id\": \"alice\"})\nsession_id = response[\"session_id\"]\n\n# Continue the conversation with the same session\nfollow_up = builder.run(message=\"Tell me more\", session_id=session_id)\n</code></pre>"},{"location":"guides/session_management/#enhanced-session-manager-types","title":"Enhanced Session Manager Types","text":"<p>LG-ADK provides several session manager implementations:</p> <ul> <li><code>SessionManager</code>: Base implementation with user tracking, metadata, and analytics</li> <li><code>SynchronizedSessionManager</code>: Thread-safe implementation for production use</li> <li><code>DatabaseSessionManager</code>: Persistent implementation that stores sessions in a database</li> <li><code>AsyncSessionManager</code>: Asynchronous implementation for async/await code</li> </ul>"},{"location":"guides/session_management/#tracking-users-and-sessions","title":"Tracking Users and Sessions","text":"<p>A key feature of LG-ADK's session management is user tracking:</p> <pre><code># Create a session with user association\nsession_id = session_manager.create_session(user_id=\"alice\")\n\n# Or add user information in metadata\nresponse = builder.run(\n    message=\"Hello!\",\n    metadata={\"user_id\": \"alice\", \"device\": \"mobile\"}\n)\n\n# Get all sessions for a user\nuser_sessions = session_manager.get_user_sessions(\"alice\")\n</code></pre>"},{"location":"guides/session_management/#session-metadata-management","title":"Session Metadata Management","text":"<p>LG-ADK provides rich metadata management for sessions:</p> <pre><code># Add metadata when creating a session\nsession_id = session_manager.create_session(metadata={\"source\": \"web\", \"locale\": \"en-US\"})\n\n# Update session metadata\nsession_manager.update_session_metadata(\n    session_id,\n    {\"last_page\": \"checkout\"},\n    merge=True  # Merge with existing metadata (default)\n)\n\n# Get session metadata\nmetadata = session_manager.get_session_metadata(session_id)\n</code></pre>"},{"location":"guides/session_management/#session-analytics-and-tracking","title":"Session Analytics and Tracking","text":"<p>Track and analyze session usage with built-in analytics:</p> <pre><code># Get session object with all tracking information\nsession = session_manager.get_session(session_id)\n\n# Access session statistics\ninteraction_count = session.interactions\ntotal_tokens_in = session.total_tokens_in\ntotal_tokens_out = session.total_tokens_out\nresponse_time = session.total_response_time\nlast_active = session.last_active\n\n# Track an interaction manually (usually done automatically by GraphBuilder)\nsession_manager.track_interaction(\n    session_id,\n    tokens_in=15,     # Input token count\n    tokens_out=25,    # Output token count\n    response_time=1.2 # Response time in seconds\n)\n</code></pre>"},{"location":"guides/session_management/#session-lifecycle-management","title":"Session Lifecycle Management","text":"<p>Manage the complete lifecycle of sessions:</p> <pre><code># Create a session\nsession_id = session_manager.create_session()\n\n# Check if a session exists\nif session_manager.session_exists(session_id):\n    # Use the session\n    pass\n\n# End a session when it's no longer needed\nsession_manager.end_session(session_id)\n\n# Clear expired sessions\nexpired_count = session_manager.cleanup_expired_sessions()\n</code></pre>"},{"location":"guides/session_management/#asynchronous-session-management","title":"Asynchronous Session Management","text":"<p>LG-ADK provides full async support for session management:</p> <pre><code>from lg_adk.sessions.session_manager import AsyncSessionManager\n\n# Create an async session manager\nasync_manager = AsyncSessionManager()\n\n# Register a session asynchronously\nsession_id = await async_manager.acreate_session(user_id=\"alice\")\n\n# Track interaction asynchronously\nawait async_manager.atrack_interaction(\n    session_id,\n    tokens_in=10,\n    tokens_out=20,\n    response_time=0.5\n)\n\n# Use with graph builder's async run method\nresult = await builder.arun(message=\"Hello!\", session_id=session_id)\n</code></pre>"},{"location":"guides/session_management/#thread-safety-with-synchronizedsessionmanager","title":"Thread Safety with SynchronizedSessionManager","text":"<p>For production applications, use the thread-safe session manager:</p> <pre><code>from lg_adk.sessions.session_manager import SynchronizedSessionManager\n\n# Create a thread-safe session manager\nsession_manager = SynchronizedSessionManager()\n\n# All operations are now thread-safe and can be called from multiple threads\nsession_id = session_manager.create_session()\n</code></pre>"},{"location":"guides/session_management/#persistent-sessions-with-databasesessionmanager","title":"Persistent Sessions with DatabaseSessionManager","text":"<p>For long-running applications, use database-backed sessions:</p> <pre><code>from lg_adk.sessions.session_manager import DatabaseSessionManager\nfrom lg_adk.database.database_manager import DatabaseManager\n\n# Create a database manager\ndb_manager = DatabaseManager()\n\n# Create a database-backed session manager\nsession_manager = DatabaseSessionManager(db_manager=db_manager)\n\n# Sessions will now persist across application restarts\nsession_id = session_manager.create_session()\n</code></pre>"},{"location":"guides/session_management/#session-timeout-and-expiration","title":"Session Timeout and Expiration","text":"<p>Configure session timeouts to automatically clean up inactive sessions:</p> <pre><code># Create a session with a 30-minute timeout\nsession_id = session_manager.create_session(timeout=1800)  # 30 minutes in seconds\n\n# Or with a timedelta\nfrom datetime import timedelta\nsession_id = session_manager.create_session(timeout=timedelta(hours=1))\n\n# Clean up expired sessions periodically\nexpired_count = session_manager.cleanup_expired_sessions()\n</code></pre>"},{"location":"guides/session_management/#complete-example","title":"Complete Example","text":"<p>Here's a complete example demonstrating session management with LG-ADK:</p> <pre><code>from lg_adk.builders.graph_builder import GraphBuilder\nfrom lg_adk.agents.base import Agent\nfrom lg_adk.sessions.session_manager import SynchronizedSessionManager\nfrom lg_adk.models import get_model\n\n# Create thread-safe session manager for production use\nsession_manager = SynchronizedSessionManager()\n\n# Create an agent\nagent = Agent(\n    name=\"assistant\",\n    model=get_model(\"openai/gpt-3.5-turbo\")\n)\n\n# Create a graph builder\nbuilder = GraphBuilder(name=\"my_app\")\nbuilder.add_agent(agent)\nbuilder.configure_session_management(session_manager)\ngraph = builder.build()\n\n# User 1: Start a new conversation\nalice_metadata = {\"user_id\": \"alice\", \"device\": \"mobile\"}\nalice_response = builder.run(\n    message=\"Hello! Can you help me with my project?\",\n    metadata=alice_metadata\n)\nalice_session_id = alice_response[\"session_id\"]\n\n# User 2: Start a different conversation\nbob_metadata = {\"user_id\": \"bob\", \"device\": \"web\"}\nbob_response = builder.run(\n    message=\"What's the weather like today?\",\n    metadata=bob_metadata\n)\nbob_session_id = bob_response[\"session_id\"]\n\n# Continue Alice's conversation\nalice_followup = builder.run(\n    message=\"I need help with Python.\",\n    session_id=alice_session_id\n)\n\n# Get analytics for both users\nalice_session = session_manager.get_session(alice_session_id)\nbob_session = session_manager.get_session(bob_session_id)\n\nprint(f\"Alice's interactions: {alice_session.interactions}\")\nprint(f\"Bob's interactions: {bob_session.interactions}\")\n\n# End sessions when done\nsession_manager.end_session(alice_session_id)\nsession_manager.end_session(bob_session_id)\n</code></pre>"},{"location":"guides/session_management/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Use SynchronizedSessionManager for Production: The thread-safe implementation prevents race conditions in multi-threaded environments.</p> </li> <li> <p>Associate Users with Sessions: Always associate sessions with users when possible for better tracking and analytics.</p> </li> <li> <p>Clean Up Sessions: Call <code>end_session()</code> when a conversation is complete to free resources.</p> </li> <li> <p>Use Timeouts: Configure appropriate timeouts to automatically clean up inactive sessions.</p> </li> <li> <p>Track Interactions: Use the <code>track_interaction()</code> method to record token usage and response times.</p> </li> <li> <p>Store Minimal Metadata: Only store necessary information in session metadata to avoid bloat.</p> </li> <li> <p>Leverage Analytics: Use the built-in analytics to understand user behavior and optimize your application.</p> </li> <li> <p>Use Async When Appropriate: For async applications, use the AsyncSessionManager for better performance.</p> </li> </ol> <p>By leveraging LG-ADK's enhanced session management, you can build sophisticated conversational applications that maintain context, track users, and provide rich analytics while seamlessly integrating with LangGraph's native capabilities.</p>"},{"location":"guides/tool_integration/","title":"\ud83d\udee0\ufe0f Tool Integration with LG-ADK","text":""},{"location":"guides/tool_integration/#why-integrate-tools","title":"\ud83e\udd14 Why Integrate Tools?","text":"<p>Tools let your agents access external data, APIs, and perform actions beyond LLMs! \ud83c\udf0d</p>"},{"location":"guides/tool_integration/#types-of-tools","title":"\ud83e\udde9 Types of Tools","text":"<ul> <li>\ud83d\udd0d Retrieval Tools: Search databases, vector stores, or the web</li> <li>\ud83d\uddc3\ufe0f Database Tools: Read/write to SQL, NoSQL, or custom stores</li> <li>\ud83c\udf10 API Tools: Call external APIs (weather, news, etc.)</li> <li>\ud83d\udee0\ufe0f Custom Tools: Any Python function or class</li> </ul>"},{"location":"guides/tool_integration/#quick-example","title":"\ud83d\udea6 Quick Example","text":"<p>Add a web search tool to your agent</p> <pre><code>from lg_adk.tools import WebSearchTool\nagent.add_tool(WebSearchTool())\n</code></pre>"},{"location":"guides/tool_integration/#how-tools-work","title":"\ud83e\udde0 How Tools Work","text":"<ul> <li>Tools are added to agents via <code>add_tool()</code> or at initialization</li> <li>Each tool exposes a name, description, and a callable interface</li> <li>Agents can decide when to use tools based on prompts or logic</li> </ul>"},{"location":"guides/tool_integration/#creating-a-custom-tool","title":"\ud83d\udee0\ufe0f Creating a Custom Tool","text":"<p>Minimal custom tool</p> <pre><code>from lg_adk.tools import Tool\n\nclass MyTool(Tool):\n    name = \"my_tool\"\n    description = \"Returns a greeting.\"\n\n    def run(self, input):\n        return f\"Hello, {input}!\"\n\nagent.add_tool(MyTool())\n</code></pre>"},{"location":"guides/tool_integration/#tool-chaining","title":"\ud83d\udd17 Tool Chaining","text":"<ul> <li>You can chain tools together in a graph for complex workflows</li> <li>Tools can pass state to each other or to agents</li> </ul>"},{"location":"guides/tool_integration/#common-pitfalls","title":"\ud83d\udea8 Common Pitfalls","text":"<p>Tool name conflicts</p> <p>Make sure each tool has a unique name in your agent or graph.</p>"},{"location":"guides/tool_integration/#next-steps","title":"\ud83c\udf1f Next Steps","text":"<ul> <li>Building Graphs \ud83c\udfd7\ufe0f</li> <li>Memory Management \ud83e\udde0</li> <li>Examples \ud83d\udca1</li> </ul>"},{"location":"guides/tool_integration/#morphik-integration-advanced","title":"\ud83e\uddec Morphik Integration (Advanced)","text":"<p>LG-ADK supports Morphik, a platform for advanced document retrieval, knowledge graph creation, and structured context (MCP) for LLMs.</p> <ul> <li>Use <code>MorphikRetrievalTool</code>, <code>MorphikGraphTool</code>, <code>MorphikGraphCreationTool</code>, and <code>MorphikMCPTool</code> for seamless integration.</li> <li>Supports semantic search, graph queries, and multi-agent collaboration on Morphik knowledge.</li> </ul> <p>See: - Morphik Example - Main README Morphik Section - Morphik Documentation</p>"}]}